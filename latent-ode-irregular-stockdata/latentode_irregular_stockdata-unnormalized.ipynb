{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "latentode_irregular_stockdata-unnormalized.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UmYrQ38hy_g"
      },
      "source": [
        "# Latent ODE for Stock Data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8BMb4s2hy_h",
        "outputId": "67097766-b2d0-4829-d639-82bb88738f03"
      },
      "source": [
        "# Install the latest version of author's repo neural ode implementation\n",
        "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "!cd torchdiffeq && pip install -e .\n",
        "!pip install yfinance\n",
        "!ls torchdiffeq/torchdiffeq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdiffeq'...\n",
            "remote: Enumerating objects: 1138, done.\u001b[K\n",
            "remote: Counting objects: 100% (434/434), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 1138 (delta 256), reused 401 (delta 240), pack-reused 704\u001b[K\n",
            "Receiving objects: 100% (1138/1138), 8.29 MiB | 21.28 MiB/s, done.\n",
            "Resolving deltas: 100% (682/682), done.\n",
            "Obtaining file:///content/torchdiffeq\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu111)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "  Running setup.py develop for torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.64.tar.gz (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.64-py2.py3-none-any.whl size=24109 sha256=420a4836fbdb9b181348e0a52162dfe4dda9afe55576a1ab4dc92ad1d26b1050\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/fe/9b/a4d3d78796b699e37065e5b6c27b75cff448ddb8b24943c288\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.3 yfinance-0.1.64\n",
            "_impl  __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cSqPUFvhy_j"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnauJ9FBhy_k"
      },
      "source": [
        "# run_models.py\n",
        "import os\n",
        "import sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import SystemRandom\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "import torch.optim as optim\n",
        "\n",
        "import utils as utils\n",
        "from data import *\n",
        "# from lib.plotting import *\n",
        "\n",
        "# from lib.rnn_baselines import *\n",
        "# from lib.ode_rnn import *\n",
        "# from lib.create_latent_ode_model import create_LatentODE_model\n",
        "# from lib.parse_datasets import parse_datasets\n",
        "# from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
        "from diffeq_solver import DiffeqSolver\n",
        "# from mujoco_physics import HopperPhysics\n",
        "\n",
        "from utils import compute_loss_all_batches\n",
        "\n",
        "import sys\n",
        "# print(sys.argv[1:])\n",
        "\n",
        "# Libraries for downloading data\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Libraries for parsing data\n",
        "from torch.distributions import uniform\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn import model_selection\n",
        "import random\n",
        "from utils import get_dict_template\n",
        "\n",
        "# Libraries for encoder_decoder.py\n",
        "from torch.distributions import Categorical, Normal\n",
        "from torch.nn.modules.rnn import LSTM, GRU\n",
        "from utils import get_device\n",
        "\n",
        "# Libraries for likelihood_eval.py\n",
        "import gc\n",
        "import sklearn as sk\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions import kl_divergence, Independent\n",
        "\n",
        "# Libraries for base_models.py\n",
        "from torch.nn.modules.rnn import GRUCell, LSTMCell, RNNCellBase\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "# Libraries for ode_func.py\n",
        "from torch.nn.utils.spectral_norm import spectral_norm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMVN3oEthy_l"
      },
      "source": [
        "# Parameters, Manual Seed, ExperimentID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJGcORZ0hy_l"
      },
      "source": [
        "# Generative model for noisy data based on ODE\n",
        "parser = argparse.ArgumentParser('Latent ODE')\n",
        "# n = size of the dataset\n",
        "parser.add_argument('-n',  type=int, default=2000, help=\"Size of the dataset\")\n",
        "# n_iters = 50\n",
        "parser.add_argument('--niters', type=int, default=50)\n",
        "parser.add_argument('--lr',  type=float, default=1e-3, help=\"Starting learning rate.\")\n",
        "# batch_size = 50\n",
        "parser.add_argument('-b', '--batch-size', type=int, default=50)\n",
        "parser.add_argument('--viz', action='store_true', help=\"Show plots while training\")\n",
        "\n",
        "parser.add_argument('--save', type=str, default='experiments/', help=\"Path for save checkpoints\")\n",
        "parser.add_argument('--load', type=str, default=None, help=\"ID of the experiment to load for evaluation. If None, run a new experiment.\")\n",
        "parser.add_argument('-r', '--random-seed', type=int, default=1991, help=\"Random_seed\")\n",
        "# dataset = stock_lag5_forecast5\n",
        "parser.add_argument('--dataset', type=str, default='stock_lag5_forecast5', help=\"Dataset to load. Available: stock_lag5_forecast5\")\n",
        "parser.add_argument('-s', '--sample-tp', type=float, default=None, help=\"Number of time points to sub-sample.\"\n",
        "\t\"If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\")\n",
        "\n",
        "parser.add_argument('-c', '--cut-tp', type=int, default=None, help=\"Cut out the section of the timeline of the specified length (in number of points).\"\n",
        "\t\"Used for periodic function demo.\")\n",
        "\n",
        "parser.add_argument('--quantization', type=float, default=0.1, help=\"Quantization on the physionet dataset.\"\n",
        "\t\"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\")\n",
        "\n",
        "parser.add_argument('--latent-ode', default = True, action='store_true', help=\"Run Latent ODE seq2seq model\")\n",
        "parser.add_argument('--z0-encoder', type=str, default='odernn', help=\"Type of encoder for Latent ODE model: odernn or rnn\")\n",
        "\n",
        "parser.add_argument('--classic-rnn', action='store_true', help=\"Run RNN baseline: classic RNN that sees true points at every point. Used for interpolation only.\")\n",
        "parser.add_argument('--rnn-cell', default=\"gru\", help=\"RNN Cell type. Available: gru (default), expdecay\")\n",
        "parser.add_argument('--input-decay', action='store_true', help=\"For RNN: use the input that is the weighted average of impirical mean and previous value (like in GRU-D)\")\n",
        "\n",
        "parser.add_argument('--ode-rnn', action='store_true', help=\"Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\")\n",
        "\n",
        "parser.add_argument('--rnn-vae', action='store_true', help=\"Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\")\n",
        "# latents = input_dim\n",
        "parser.add_argument('-l', '--latents', type=int, default=10, help=\"Size of the latent state\")\n",
        "# rec_dims = more than 2*input_dim\n",
        "parser.add_argument('--rec-dims', type=int, default=25, help=\"Dimensionality of the recognition model (ODE or RNN).\")\n",
        "\n",
        "parser.add_argument('--rec-layers', type=int, default=3, help=\"Number of layers in ODE func in recognition ODE\")\n",
        "parser.add_argument('--gen-layers', type=int, default=3, help=\"Number of layers in ODE func in generative ODE\")\n",
        "# units for ODE func\n",
        "parser.add_argument('-u', '--units', type=int, default=300, help=\"Number of units per layer in ODE func\")\n",
        "# units for GRU\n",
        "parser.add_argument('-g', '--gru-units', type=int, default=100, help=\"Number of units per layer in each of GRU update networks\")\n",
        "\n",
        "parser.add_argument('--poisson', action='store_true', help=\"Model poisson-process likelihood for the density of events in addition to reconstruction.\")\n",
        "parser.add_argument('--classif', action='store_true', help=\"Include binary classification loss -- used for Physionet dataset for hospiral mortality\")\n",
        "\n",
        "parser.add_argument('--linear-classif', action='store_true', help=\"If using a classifier, use a linear classifier instead of 1-layer NN\")\n",
        "# extrap = True\n",
        "parser.add_argument('--extrap', default = True, action='store_true', help=\"Set extrapolation mode. If this flag is not set, run interpolation mode.\")\n",
        "# timesteps = lag+forecast\n",
        "parser.add_argument('-t', '--timepoints', type=int, default=10, help=\"Total number of time-points\")\n",
        "parser.add_argument('--max-t',  type=float, default=5., help=\"We subsample points in the interval [0, args.max_tp]\")\n",
        "parser.add_argument('--noise-weight', type=float, default=0.01, help=\"Noise amplitude for generated traejctories\")\n",
        "\n",
        "sys.argv = ['-f']\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# file_name = os.path.basename(__file__)[:-3]\n",
        "utils.makedirs(args.save)  # saves in 'experiments/' folder\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbFeze76hy_n",
        "outputId": "e46f560c-5226-4e72-c939-a51f47e6001e"
      },
      "source": [
        "torch.manual_seed(args.random_seed)\n",
        "np.random.seed(args.random_seed)\n",
        "\n",
        "experimentID = args.load  # None\n",
        "# print(f\"experimentID is {experimentID}\")\n",
        "\n",
        "if experimentID is None:\n",
        "    # Make a new experiment ID\n",
        "    experimentID = int(SystemRandom().random()*100000) # from random import SystemRandom\n",
        "print(f\"experimentID is {experimentID}\")\n",
        "ckpt_path = os.path.join(args.save, \"experiment_\" + str(experimentID) + '.ckpt') \n",
        "# print(f\"ckpt_path is {ckpt_path}\")\n",
        "\n",
        "start = time.time()\n",
        "# print(\"Sampling dataset of {} training examples\".format(args.n))  # n is size of the dataset\n",
        "\n",
        "# print(f\"args is {str(args)}\")\n",
        "\n",
        "input_command = sys.argv\n",
        "# print(f\"input_command is {input_command}\")\n",
        "\n",
        "ind = [i for i in range(len(input_command)) if input_command[i] == \"--load\"]\n",
        "# print(f\"ind is {ind}\")\n",
        "# print(f\"len(ind) is {len(ind)}\")\n",
        "\n",
        "if len(ind) == 1:\n",
        "    ind = ind[0]\n",
        "    input_command = input_command[:ind] + input_command[(ind+2):]\n",
        "input_command = \" \".join(input_command)\n",
        "# print(f\"input_command is {input_command}\")\n",
        "utils.makedirs(\"results/\")\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "experimentID is 41822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Ld41k7hy_o"
      },
      "source": [
        "# Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC70xR81hy_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0235a2-d648-4a7d-f8b0-d22cba9da939"
      },
      "source": [
        "# Uncomment to get the training data of stock data with lag and forecast\n",
        "# 1. Get Open and Close Price of asset (o, c) for each trading day.\n",
        "# libraries\n",
        "\n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "period = '1d'\n",
        "start='2000-1-1'\n",
        "end='2021-8-31'\n",
        "\n",
        "# Creating a path to save the data\n",
        "path = f\"raw-stock-data/data-{start.split('-')[0]}-{end.split('-')[0]}\"\n",
        "if not os.path.exists(path):\n",
        "    # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "    # Create a new directory\n",
        "    os.makedirs(path)\n",
        "    print(f\"{path} directory is created\")\n",
        "\n",
        "# Downloading the data\n",
        "for tickerName, ticker in dict_tickers.items():\n",
        "    tickerName = tickerName\n",
        "    ticker = ticker\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    download_raw_stock_data(filepath, ticker, start, end, period)\n",
        "\n",
        "# print('\\n')\n",
        "\n",
        "# print(f\"The size of each asset\")\n",
        "for tickerName in dict_tickers.keys():\n",
        "    df = pd.read_csv(f\"{path}/{tickerName}.csv\")\n",
        "    print(f\"{tickerName} size: {len(df)}\")\n",
        "\n",
        "# 2. Get weekly data.\n",
        "# 3. Transform $d_{i}$ to sequences of lag * len($d_{i}$) length.\n",
        "\n",
        "\n",
        "week_sequence = {}\n",
        "lag = 5\n",
        "forecast = 5\n",
        "for tickerName in dict_tickers.keys():\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    # Get the data in the required format\n",
        "    data = stockDataTransformer(filepath)\n",
        "    # # Total Data Size\n",
        "    data_size = data.shape[0]\n",
        "    # print(f\"{tickerName} data.shape {data.shape}\")\n",
        "    data_orig = series_to_supervised(data, lag, forecast).values\n",
        "    # print(f'{tickerName} Data Original after series to supervised on data {data_orig.shape}')\n",
        "    week_sequence[tickerName] = data_orig\n",
        "    # print('\\n')\n",
        "\n",
        "# 4. Bundle all sequences together\n",
        "data = week_sequence['Apple']\n",
        "for tickerName in week_sequence.keys():\n",
        "    if tickerName != 'Apple':\n",
        "        data1 = week_sequence[tickerName]\n",
        "        data = np.concatenate((data, data1))\n",
        "        print(f\"data.shape {data.shape}\")\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw-stock-data/data-2000-2021 directory is created\n",
            "Apple size: 5450\n",
            "Microsoft size: 5450\n",
            "Google size: 4288\n",
            "Bitcoin size: 2537\n",
            "Facebook size: 2336\n",
            "Walmart size: 5450\n",
            "Amazon size: 5450\n",
            "CVS size: 5450\n",
            "Berkshire size: 5450\n",
            "ExxonMobil size: 5450\n",
            "AtandT size: 5450\n",
            "Costco size: 5450\n",
            "Walgreens size: 5450\n",
            "Kroger size: 5450\n",
            "JPMorgan size: 5450\n",
            "Verizon size: 5450\n",
            "FordMotor size: 5450\n",
            "GeneralMotors size: 2713\n",
            "Dell size: 1268\n",
            "BankOfAmerica size: 5450\n",
            "Target size: 5450\n",
            "GeneralElectric size: 5450\n",
            "JohnsonandJohnson size: 5450\n",
            "Nvidia size: 5451\n",
            "Intel size: 5450\n",
            "data.shape (2162, 100)\n",
            "data.shape (3009, 100)\n",
            "data.shape (3506, 100)\n",
            "data.shape (3963, 100)\n",
            "data.shape (5044, 100)\n",
            "data.shape (6125, 100)\n",
            "data.shape (7206, 100)\n",
            "data.shape (8287, 100)\n",
            "data.shape (9368, 100)\n",
            "data.shape (10449, 100)\n",
            "data.shape (11530, 100)\n",
            "data.shape (12611, 100)\n",
            "data.shape (13692, 100)\n",
            "data.shape (14773, 100)\n",
            "data.shape (15854, 100)\n",
            "data.shape (16935, 100)\n",
            "data.shape (17468, 100)\n",
            "data.shape (17711, 100)\n",
            "data.shape (18792, 100)\n",
            "data.shape (19873, 100)\n",
            "data.shape (20954, 100)\n",
            "data.shape (22035, 100)\n",
            "data.shape (23106, 100)\n",
            "data.shape (24187, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8N3A7Z3iTyy"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imjYAxRYi2Xc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85205cb-35ab-43c1-a9aa-977c4b9419eb"
      },
      "source": [
        "training_path = 'data'\n",
        "if not os.path.exists(training_path):\n",
        "    # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "    # Create a new directory\n",
        "    os.makedirs(training_path)\n",
        "    print(f\"{path} directory is created\")\n",
        "data_df = pd.DataFrame(data)\n",
        "data_df.to_csv(f\"{training_path}/data-lag{lag}-forecast{forecast}.csv\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw-stock-data/data-2000-2021 directory is created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4Tlm4XYhy_p",
        "outputId": "9946f5b7-8580-476a-88c8-bfdfdb7f53ca"
      },
      "source": [
        "# Importing the data with index as the first column\n",
        "training_path = 'data'\n",
        "lag = 5\n",
        "forecast = 5\n",
        "data = pd.read_csv(f\"{training_path}/data-lag{lag}-forecast{forecast}.csv\", index_col=0).values\n",
        "print(f\"data.shape {data.shape}\")\n",
        "# Reshape data to (data.shape[0], lag+forecast, data.shape[1])\n",
        "data = data.reshape(data.shape[0], lag+forecast, 10)\n",
        "# print(f\"data.shape {data.shape}\")\n",
        "\n",
        "# Convert data to tensor\n",
        "data = torch.from_numpy(data).float().to(device)\n",
        "print(f\"data.shape {data.shape}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.shape (24187, 100)\n",
            "data.shape torch.Size([24187, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkMnH9Gbhy_q"
      },
      "source": [
        "# Parse Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Rkuecghy_q"
      },
      "source": [
        "###########################\n",
        "# Latent ODEs for Stock Data \n",
        "# Authors: Omkar Mehta \n",
        "###########################\n",
        "\n",
        "class StockData(object):\n",
        "\n",
        "\tdef __init__(self, root, download = True, generate=False, device = torch.device(\"cpu\")):\n",
        "\t\tself.root = root\n",
        "\t\tif download:\n",
        "\t\t\tdata = self._download()\n",
        "\n",
        "\t\tif generate:\n",
        "\t\t\tself._generate_dataset()\n",
        "\n",
        "\t\tif not self._check_exists():\n",
        "\t\t\traise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
        "\n",
        "\t\tdata_file = os.path.join(self.data_folder, training_file)\n",
        "\t\tdata = pd.read_csv(os.path.join(self.root, training_file), index_col=0).values\n",
        "\t\t# Reshape data to (data.shape[0], lag+forecast, data.shape[1])\n",
        "\t\tdata = data.reshape(data.shape[0], lag+forecast, D)\n",
        "\t\t# Convert data to tensor\n",
        "\t\tself.data = torch.from_numpy(data).float().to(device)\n",
        "\t\t# self.data, self.data_min, self.data_max = utils.normalize_data(self.data)\n",
        "\n",
        "\t\tself.device =device\n",
        "\n",
        "\tdef _download(self):\n",
        "\t\tif self._check_exists():\n",
        "\t\t\treturn\n",
        "\t\tif not os.path.exists(self.data_folder):\n",
        "\t\t\tos.makedirs(self.data_folder, exist_ok=True)\n",
        "\t\tdata = pd.read_csv(os.path.join(self.root, training_file), index_col=0).values\n",
        "\t\t# Reshape data to (data.shape[0], lag+forecast, data.shape[1])\n",
        "\t\tdata = data.reshape(data.shape[0], lag+forecast, data.shape[1])\n",
        "\t\treturn data\n",
        "\tdef _check_exists(self):\n",
        "\t\treturn os.path.exists(os.path.join(self.data_folder, training_file))\n",
        "\n",
        "\t@property\n",
        "\tdef data_folder(self):\n",
        "\t\treturn os.path.join(self.root)\n",
        "\n",
        "\t# def __getitem__(self, index):\n",
        "\t#     return self.data[index]\n",
        "\n",
        "\tdef get_dataset(self):\n",
        "\t\treturn self.data\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef size(self, ind = None):\n",
        "\t\tif ind is not None:\n",
        "\t\t\treturn self.data.shape[ind]\n",
        "\t\treturn self.data.shape\n",
        "\t\t\t\n",
        "\tdef __repr__(self):\n",
        "\t\tfmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "\t\tfmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "\t\tfmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "\t\treturn fmt_str\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEI5y8U2hy_r"
      },
      "source": [
        "def parse_datasets(args, device):\n",
        "\t\n",
        "\t# Parse datasets\n",
        "\tdef basic_collate_fn(batch, time_steps, args = args, device = device, data_type = \"train\"):\n",
        "\t\tbatch = torch.stack(batch)\n",
        "\t\tdata_dict = {\n",
        "\t\t\t\"data\": batch, \n",
        "\t\t\t\"time_steps\": time_steps}\n",
        "\n",
        "\t\tdata_dict = utils.split_and_subsample_batch(data_dict, args, data_type = data_type)\n",
        "\t\treturn data_dict\n",
        "\n",
        "\n",
        "\tdataset_name = args.dataset\n",
        "\n",
        "\tn_total_tp = args.timepoints + args.extrap\n",
        "\tmax_t_extrap = args.max_t / args.timepoints * n_total_tp\n",
        "\n",
        "\t##################################################################\n",
        "\t# Stock Data with lag and forecast dataset\n",
        "\tif dataset_name == \"stock_lag5_forecast5\":\n",
        "\t\tdataset_obj = StockData(root='data', download=True, generate=False, device = device)\n",
        "\t\tdataset = dataset_obj.get_dataset()\n",
        "\t\tdataset = dataset.to(device)\n",
        "\n",
        "\n",
        "\t\tn_tp_data = dataset[:].shape[1]\n",
        "\n",
        "\t\t# Time steps that are used later on for exrapolation\n",
        "\t\ttime_steps = torch.arange(start=0, end = n_tp_data, step=1).float().to(device)\n",
        "\t\ttime_steps = time_steps / len(time_steps)\n",
        "\n",
        "\t\tdataset = dataset.to(device)\n",
        "\t\ttime_steps = time_steps.to(device)\n",
        "\n",
        "\t\tif not args.extrap:\n",
        "\t\t\t# Creating dataset for interpolation\n",
        "\t\t\t# sample time points from different parts of the timeline, \n",
        "\t\t\t# so that the model learns from different parts of hopper trajectory\n",
        "\t\t\tn_traj = len(dataset)\n",
        "\t\t\tn_tp_data = dataset.shape[1]\n",
        "\t\t\tn_reduced_tp = args.timepoints\n",
        "\n",
        "\t\t\t# sample time points from different parts of the timeline, \n",
        "\t\t\t# so that the model learns from different parts of hopper trajectory\n",
        "\t\t\tstart_ind = np.random.randint(0, high=n_tp_data - n_reduced_tp +1, size=n_traj)\n",
        "\t\t\tend_ind = start_ind + n_reduced_tp\n",
        "\t\t\tsliced = []\n",
        "\t\t\tfor i in range(n_traj):\n",
        "\t\t\t\t  sliced.append(dataset[i, start_ind[i] : end_ind[i], :])\n",
        "\t\t\tdataset = torch.stack(sliced).to(device)\n",
        "\t\t\ttime_steps = time_steps[:n_reduced_tp]\n",
        "\n",
        "\t\t# Split into train and test by the time sequences\n",
        "\t\ttrain_y, test_y = utils.split_train_test(dataset, train_fraq = 0.8)\n",
        "\n",
        "\t\tn_samples = len(dataset)\n",
        "\t\tinput_dim = dataset.size(-1)\n",
        "\n",
        "\t\tbatch_size = min(args.batch_size, args.n)\n",
        "\t\ttrain_dataloader = DataLoader(train_y, batch_size = batch_size, shuffle=False,\n",
        "\t\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps, data_type = \"train\"))\n",
        "\t\ttest_dataloader = DataLoader(test_y, batch_size = n_samples, shuffle=False,\n",
        "\t\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps, data_type = \"test\"))\n",
        "\t\t\n",
        "\t\tdata_objects = {\"dataset_obj\": dataset_obj, \n",
        "\t\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\t\"n_test_batches\": len(test_dataloader)}\n",
        "\t\treturn data_objects\n",
        "\n",
        "\t##################################################################\n",
        "\t# Physionet dataset\n",
        "\n",
        "\tif dataset_name == \"physionet\":\n",
        "\t\ttrain_dataset_obj = PhysioNet('data/physionet', train=True, \n",
        "\t\t\t\t\t\t\t\t\t\tquantization = args.quantization,\n",
        "\t\t\t\t\t\t\t\t\t\tdownload=True, n_samples = min(10000, args.n), \n",
        "\t\t\t\t\t\t\t\t\t\tdevice = device)\n",
        "\t\t# Use custom collate_fn to combine samples with arbitrary time observations.\n",
        "\t\t# Returns the dataset along with mask and time steps\n",
        "\t\ttest_dataset_obj = PhysioNet('data/physionet', train=False, \n",
        "\t\t\t\t\t\t\t\t\t\tquantization = args.quantization,\n",
        "\t\t\t\t\t\t\t\t\t\tdownload=True, n_samples = min(10000, args.n), \n",
        "\t\t\t\t\t\t\t\t\t\tdevice = device)\n",
        "\n",
        "\t\t# Combine and shuffle samples from physionet Train and physionet Test\n",
        "\t\ttotal_dataset = train_dataset_obj[:len(train_dataset_obj)]\n",
        "\n",
        "\t\tif not args.classif:\n",
        "\t\t\t# Concatenate samples from original Train and Test sets\n",
        "\t\t\t# Only 'training' physionet samples are have labels. Therefore, if we do classifiction task, we don't need physionet 'test' samples.\n",
        "\t\t\ttotal_dataset = total_dataset + test_dataset_obj[:len(test_dataset_obj)]\n",
        "\n",
        "\t\t# Shuffle and split\n",
        "\t\ttrain_data, test_data = model_selection.train_test_split(total_dataset, train_size= 0.8, \n",
        "\t\t\trandom_state = 42, shuffle = True)\n",
        "\n",
        "\t\trecord_id, tt, vals, mask, labels = train_data[0]\n",
        "\n",
        "\t\tn_samples = len(total_dataset)\n",
        "\t\tinput_dim = vals.size(-1)\n",
        "\n",
        "\t\tbatch_size = min(min(len(train_dataset_obj), args.batch_size), args.n)\n",
        "\t\tdata_min, data_max = get_data_min_max(total_dataset)\n",
        "\n",
        "\t\ttrain_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
        "\t\t\t\tdata_min = data_min, data_max = data_max))\n",
        "\t\ttest_dataloader = DataLoader(test_data, batch_size = n_samples, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"test\",\n",
        "\t\t\t\tdata_min = data_min, data_max = data_max))\n",
        "\n",
        "\t\tattr_names = train_dataset_obj.params\n",
        "\t\tdata_objects = {\"dataset_obj\": train_dataset_obj, \n",
        "\t\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\t\"n_test_batches\": len(test_dataloader),\n",
        "\t\t\t\t\t\"attr\": attr_names, #optional\n",
        "\t\t\t\t\t\"classif_per_tp\": False, #optional\n",
        "\t\t\t\t\t\"n_labels\": 1} #optional\n",
        "\t\treturn data_objects\n",
        "\n",
        "\t##################################################################\n",
        "\t# Human activity dataset\n",
        "\n",
        "\tif dataset_name == \"activity\":\n",
        "\t\tn_samples =  min(10000, args.n)\n",
        "\t\tdataset_obj = PersonActivity('data/PersonActivity', \n",
        "\t\t\t\t\t\t\tdownload=True, n_samples =  n_samples, device = device)\n",
        "\t\tprint(dataset_obj)\n",
        "\t\t# Use custom collate_fn to combine samples with arbitrary time observations.\n",
        "\t\t# Returns the dataset along with mask and time steps\n",
        "\n",
        "\t\t# Shuffle and split\n",
        "\t\ttrain_data, test_data = model_selection.train_test_split(dataset_obj, train_size= 0.8, \n",
        "\t\t\trandom_state = 42, shuffle = True)\n",
        "\n",
        "\t\ttrain_data = [train_data[i] for i in np.random.choice(len(train_data), len(train_data))]\n",
        "\t\ttest_data = [test_data[i] for i in np.random.choice(len(test_data), len(test_data))]\n",
        "\n",
        "\t\trecord_id, tt, vals, mask, labels = train_data[0]\n",
        "\t\tinput_dim = vals.size(-1)\n",
        "\n",
        "\t\tbatch_size = min(min(len(dataset_obj), args.batch_size), args.n)\n",
        "\t\ttrain_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn_activity(batch, args, device, data_type = \"train\"))\n",
        "\t\ttest_dataloader = DataLoader(test_data, batch_size=n_samples, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn_activity(batch, args, device, data_type = \"test\"))\n",
        "\n",
        "\t\tdata_objects = {\"dataset_obj\": dataset_obj, \n",
        "\t\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\t\"n_test_batches\": len(test_dataloader),\n",
        "\t\t\t\t\t\"classif_per_tp\": True, #optional\n",
        "\t\t\t\t\t\"n_labels\": labels.size(-1)}\n",
        "\n",
        "\t\treturn data_objects\n",
        "\n",
        "\t########### 1d datasets ###########\n",
        "\n",
        "\t# Sampling args.timepoints time points in the interval [0, args.max_t]\n",
        "\t# Sample points for both training sequence and explapolation (test)\n",
        "\tdistribution = uniform.Uniform(torch.Tensor([0.0]),torch.Tensor([max_t_extrap]))\n",
        "\ttime_steps_extrap =  distribution.sample(torch.Size([n_total_tp-1]))[:,0]\n",
        "\ttime_steps_extrap = torch.cat((torch.Tensor([0.0]), time_steps_extrap))\n",
        "\ttime_steps_extrap = torch.sort(time_steps_extrap)[0]\n",
        "\n",
        "\tdataset_obj = None\n",
        "\t##################################################################\n",
        "\t# Sample a periodic function\n",
        "\tif dataset_name == \"periodic\":\n",
        "\t\tdataset_obj = Periodic_1d(\n",
        "\t\t\tinit_freq = None, init_amplitude = 1.,\n",
        "\t\t\tfinal_amplitude = 1., final_freq = None, \n",
        "\t\t\tz0 = 1.)\n",
        "\n",
        "\t##################################################################\n",
        "\n",
        "\tif dataset_obj is None:\n",
        "\t\traise Exception(\"Unknown dataset: {}\".format(dataset_name))\n",
        "\n",
        "\tdataset = dataset_obj.sample_traj(time_steps_extrap, n_samples = args.n, \n",
        "\t\tnoise_weight = args.noise_weight)\n",
        "\n",
        "\t# Process small datasets\n",
        "\tdataset = dataset.to(device)\n",
        "\ttime_steps_extrap = time_steps_extrap.to(device)\n",
        "\n",
        "\ttrain_y, test_y = utils.split_train_test(dataset, train_fraq = 0.8)\n",
        "\n",
        "\tn_samples = len(dataset)\n",
        "\tinput_dim = dataset.size(-1)\n",
        "\n",
        "\tbatch_size = min(args.batch_size, args.n)\n",
        "\ttrain_dataloader = DataLoader(train_y, batch_size = batch_size, shuffle=False,\n",
        "\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps_extrap, data_type = \"train\"))\n",
        "\ttest_dataloader = DataLoader(test_y, batch_size = args.n, shuffle=False,\n",
        "\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps_extrap, data_type = \"test\"))\n",
        "\t\n",
        "\tdata_objects = {#\"dataset_obj\": dataset_obj, \n",
        "\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\"n_test_batches\": len(test_dataloader)}\n",
        "\n",
        "\treturn data_objects\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvdaU1g4hy_s",
        "outputId": "23db87b0-2c52-448b-aa83-4702f5e29c62"
      },
      "source": [
        "lag = 5\n",
        "forecast = 5\n",
        "\n",
        "T = lag+forecast\n",
        "print(f\"T is {T}\")\n",
        "D = 10\n",
        "print(f\"D is {D}\")\n",
        "root = 'data'\n",
        "n_training_samples = data.shape[0]\n",
        "training_file = f'data-lag{lag}-forecast{forecast}.csv'\n",
        "\n",
        "data_obj = parse_datasets(args, device)\n",
        "input_dim = data_obj[\"input_dim\"]\n",
        "print(f\"input_dim is {input_dim}\")\n",
        "\n",
        "classif_per_tp = False\n",
        "if (\"classif_per_tp\" in data_obj):\n",
        "    # do classification per time point rather than on a time series as a whole\n",
        "    classif_per_tp = data_obj[\"classif_per_tp\"]\n",
        "\n",
        "if args.classif and (args.dataset == \"hopper\" or args.dataset == \"periodic\"):\n",
        "    raise Exception(\"Classification task is not available for MuJoCo and 1d datasets\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T is 10\n",
            "D is 10\n",
            "input_dim is 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN8eEmH-hy_s"
      },
      "source": [
        "# n_labels, obsrv_std, z0_prior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wMtb7FQhy_s"
      },
      "source": [
        "n_labels = 1\n",
        "if args.classif:\n",
        "    if (\"n_labels\" in data_obj):\n",
        "        n_labels = data_obj[\"n_labels\"]\n",
        "    else:\n",
        "        raise Exception(\"Please provide number of labels for classification task\")\n",
        "\n",
        "##################################################################\n",
        "# Create the model\n",
        "obsrv_std = 0.01\n",
        "if args.dataset == \"stock_lag5_forecast5\":\n",
        "    obsrv_std = 1e-3 \n",
        "\n",
        "obsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
        "\n",
        "z0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58GzgLWxhy_s"
      },
      "source": [
        "# args.latent_ode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExKiCiHthy_s"
      },
      "source": [
        "## encoder_decoder.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pKHw5pqhy_t"
      },
      "source": [
        "# GRU description: \n",
        "# http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/\n",
        "class GRU_unit(nn.Module):\n",
        "\tdef __init__(self, latent_dim, input_dim, \n",
        "\t\tupdate_gate = None,\n",
        "\t\treset_gate = None,\n",
        "\t\tnew_state_net = None,\n",
        "\t\tn_units = 100,\n",
        "\t\tdevice = torch.device(\"cpu\")):\n",
        "\t\tsuper(GRU_unit, self).__init__()\n",
        "\n",
        "\t\tif update_gate is None:\n",
        "\t\t\tself.update_gate = nn.Sequential(\n",
        "\t\t\t   nn.Linear(latent_dim * 2 + input_dim, n_units),\n",
        "\t\t\t   nn.Tanh(),\n",
        "\t\t\t   nn.Linear(n_units, latent_dim),\n",
        "\t\t\t   nn.Sigmoid())\n",
        "\t\t\tutils.init_network_weights(self.update_gate)\n",
        "\t\telse: \n",
        "\t\t\tself.update_gate  = update_gate\n",
        "\n",
        "\t\tif reset_gate is None:\n",
        "\t\t\tself.reset_gate = nn.Sequential(\n",
        "\t\t\t   nn.Linear(latent_dim * 2 + input_dim, n_units),\n",
        "\t\t\t   nn.Tanh(),\n",
        "\t\t\t   nn.Linear(n_units, latent_dim),\n",
        "\t\t\t   nn.Sigmoid())\n",
        "\t\t\tutils.init_network_weights(self.reset_gate)\n",
        "\t\telse: \n",
        "\t\t\tself.reset_gate  = reset_gate\n",
        "\n",
        "\t\tif new_state_net is None:\n",
        "\t\t\tself.new_state_net = nn.Sequential(\n",
        "\t\t\t   nn.Linear(latent_dim * 2 + input_dim, n_units),\n",
        "\t\t\t   nn.Tanh(),\n",
        "\t\t\t   nn.Linear(n_units, latent_dim * 2))\n",
        "\t\t\tutils.init_network_weights(self.new_state_net)\n",
        "\t\telse: \n",
        "\t\t\tself.new_state_net  = new_state_net\n",
        "\n",
        "\n",
        "\tdef forward(self, y_mean, y_std, x, masked_update = True):\n",
        "\t\ty_concat = torch.cat([y_mean, y_std, x], -1)\n",
        "\n",
        "\t\tupdate_gate = self.update_gate(y_concat)\n",
        "\t\treset_gate = self.reset_gate(y_concat)\n",
        "\t\tconcat = torch.cat([y_mean * reset_gate, y_std * reset_gate, x], -1)\n",
        "\t\t\n",
        "\t\tnew_state, new_state_std = utils.split_last_dim(self.new_state_net(concat))\n",
        "\t\tnew_state_std = new_state_std.abs()\n",
        "\n",
        "\t\tnew_y = (1-update_gate) * new_state + update_gate * y_mean\n",
        "\t\tnew_y_std = (1-update_gate) * new_state_std + update_gate * y_std\n",
        "\n",
        "\t\tassert(not torch.isnan(new_y).any())\n",
        "\n",
        "\t\tif masked_update:\n",
        "\t\t\t# IMPORTANT: assumes that x contains both data and mask\n",
        "\t\t\t# update only the hidden states for hidden state only if at least one feature is present for the current time point\n",
        "\t\t\tn_data_dims = x.size(-1)//2\n",
        "\t\t\tmask = x[:, :, n_data_dims:]\n",
        "\t\t\tutils.check_mask(x[:, :, :n_data_dims], mask)\n",
        "\t\t\t\n",
        "\t\t\tmask = (torch.sum(mask, -1, keepdim = True) > 0).float()\n",
        "\n",
        "\t\t\tassert(not torch.isnan(mask).any())\n",
        "\n",
        "\t\t\tnew_y = mask * new_y + (1-mask) * y_mean\n",
        "\t\t\tnew_y_std = mask * new_y_std + (1-mask) * y_std\n",
        "\n",
        "\t\t\tif torch.isnan(new_y).any():\n",
        "\t\t\t\tprint(\"new_y is nan!\")\n",
        "\t\t\t\tprint(mask)\n",
        "\t\t\t\tprint(y_mean)\n",
        "\t\t\t\tprint(prev_new_y)\n",
        "\t\t\t\texit()\n",
        "\n",
        "\t\tnew_y_std = new_y_std.abs()\n",
        "\t\treturn new_y, new_y_std\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_z0_RNN(nn.Module):\n",
        "\tdef __init__(self, latent_dim, input_dim, lstm_output_size = 20, \n",
        "\t\tuse_delta_t = True, device = torch.device(\"cpu\")):\n",
        "\t\t\n",
        "\t\tsuper(Encoder_z0_RNN, self).__init__()\n",
        "\t\n",
        "\t\tself.gru_rnn_output_size = lstm_output_size\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\tself.device = device\n",
        "\t\tself.use_delta_t = use_delta_t\n",
        "\n",
        "\t\tself.hiddens_to_z0 = nn.Sequential(\n",
        "\t\t   nn.Linear(self.gru_rnn_output_size, 50),\n",
        "\t\t   nn.Tanh(),\n",
        "\t\t   nn.Linear(50, latent_dim * 2),)\n",
        "\n",
        "\t\tutils.init_network_weights(self.hiddens_to_z0)\n",
        "\n",
        "\t\tinput_dim = self.input_dim\n",
        "\n",
        "\t\tif use_delta_t:\n",
        "\t\t\tself.input_dim += 1\n",
        "\t\tself.gru_rnn = GRU(self.input_dim, self.gru_rnn_output_size).to(device)\n",
        "\n",
        "\tdef forward(self, data, time_steps, run_backwards = True):\n",
        "\t\t# IMPORTANT: assumes that 'data' already has mask concatenated to it \n",
        "\n",
        "\t\t# data shape: [n_traj, n_tp, n_dims]\n",
        "\t\t# shape required for rnn: (seq_len, batch, input_size)\n",
        "\t\t# t0: not used here\n",
        "\t\tn_traj = data.size(0)\n",
        "\n",
        "\t\tassert(not torch.isnan(data).any())\n",
        "\t\tassert(not torch.isnan(time_steps).any())\n",
        "\n",
        "\t\tdata = data.permute(1,0,2) \n",
        "\n",
        "\t\tif run_backwards:\n",
        "\t\t\t# Look at data in the reverse order: from later points to the first\n",
        "\t\t\tdata = utils.reverse(data)\n",
        "\n",
        "\t\tif self.use_delta_t:\n",
        "\t\t\tdelta_t = time_steps[1:] - time_steps[:-1]\n",
        "\t\t\tif run_backwards:\n",
        "\t\t\t\t# we are going backwards in time with\n",
        "\t\t\t\tdelta_t = utils.reverse(delta_t)\n",
        "\t\t\t# append zero delta t in the end\n",
        "\t\t\tdelta_t = torch.cat((delta_t, torch.zeros(1).to(self.device)))\n",
        "\t\t\tdelta_t = delta_t.unsqueeze(1).repeat((1,n_traj)).unsqueeze(-1)\n",
        "\t\t\tdata = torch.cat((delta_t, data),-1)\n",
        "\n",
        "\t\toutputs, _ = self.gru_rnn(data)\n",
        "\n",
        "\t\t# LSTM output shape: (seq_len, batch, num_directions * hidden_size)\n",
        "\t\tlast_output = outputs[-1]\n",
        "\n",
        "\t\tself.extra_info ={\"rnn_outputs\": outputs, \"time_points\": time_steps}\n",
        "\n",
        "\t\tmean, std = utils.split_last_dim(self.hiddens_to_z0(last_output))\n",
        "\t\tstd = std.abs()\n",
        "\n",
        "\t\tassert(not torch.isnan(mean).any())\n",
        "\t\tassert(not torch.isnan(std).any())\n",
        "\n",
        "\t\treturn mean.unsqueeze(0), std.unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Encoder_z0_ODE_RNN(nn.Module):\n",
        "\t# Derive z0 by running ode backwards.\n",
        "\t# For every y_i we have two versions: encoded from data and derived from ODE by running it backwards from t_i+1 to t_i\n",
        "\t# Compute a weighted sum of y_i from data and y_i from ode. Use weighted y_i as an initial value for ODE runing from t_i to t_i-1\n",
        "\t# Continue until we get to z0\n",
        "\tdef __init__(self, latent_dim, input_dim, z0_diffeq_solver = None, \n",
        "\t\tz0_dim = None, GRU_update = None, \n",
        "\t\tn_gru_units = 100, \n",
        "\t\tdevice = torch.device(\"cpu\")):\n",
        "\t\t\n",
        "\t\tsuper(Encoder_z0_ODE_RNN, self).__init__()\n",
        "\n",
        "\t\tif z0_dim is None:\n",
        "\t\t\tself.z0_dim = latent_dim\n",
        "\t\telse:\n",
        "\t\t\tself.z0_dim = z0_dim\n",
        "\n",
        "\t\tif GRU_update is None:\n",
        "\t\t\tself.GRU_update = GRU_unit(latent_dim, input_dim, \n",
        "\t\t\t\tn_units = n_gru_units, \n",
        "\t\t\t\tdevice=device).to(device)\n",
        "\t\telse:\n",
        "\t\t\tself.GRU_update = GRU_update\n",
        "\n",
        "\t\tself.z0_diffeq_solver = z0_diffeq_solver\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\tself.device = device\n",
        "\t\tself.extra_info = None\n",
        "\n",
        "\t\tself.transform_z0 = nn.Sequential(\n",
        "\t\t   nn.Linear(latent_dim * 2, 100),\n",
        "\t\t   nn.Tanh(),\n",
        "\t\t   nn.Linear(100, self.z0_dim * 2),)\n",
        "\t\tutils.init_network_weights(self.transform_z0)\n",
        "\n",
        "\n",
        "\tdef forward(self, data, time_steps, run_backwards = False, save_info = False):\n",
        "\t\t# data, time_steps -- observations and their time stamps\n",
        "\t\t# IMPORTANT: assumes that 'data' already has mask concatenated to it \n",
        "\t\tassert(not torch.isnan(data).any())\n",
        "\t\tassert(not torch.isnan(time_steps).any())\n",
        "\n",
        "\t\tn_traj, n_tp, n_dims = data.size()\n",
        "\t\tif len(time_steps) == 1:\n",
        "\t\t\tprev_y = torch.zeros((1, n_traj, self.latent_dim)).to(self.device)\n",
        "\t\t\tprev_std = torch.zeros((1, n_traj, self.latent_dim)).to(self.device)\n",
        "\n",
        "\t\t\txi = data[:,0,:].unsqueeze(0)\n",
        "\n",
        "\t\t\tlast_yi, last_yi_std = self.GRU_update(prev_y, prev_std, xi)\n",
        "\t\t\textra_info = None\n",
        "\t\telse:\n",
        "\t\t\t\n",
        "\t\t\tlast_yi, last_yi_std, _, extra_info = self.run_odernn(\n",
        "\t\t\t\tdata, time_steps, run_backwards = run_backwards,\n",
        "\t\t\t\tsave_info = save_info)\n",
        "\n",
        "\t\tmeans_z0 = last_yi.reshape(1, n_traj, self.latent_dim)\n",
        "\t\tstd_z0 = last_yi_std.reshape(1, n_traj, self.latent_dim)\n",
        "\n",
        "\t\tmean_z0, std_z0 = utils.split_last_dim( self.transform_z0( torch.cat((means_z0, std_z0), -1)))\n",
        "\t\tstd_z0 = std_z0.abs()\n",
        "\t\tif save_info:\n",
        "\t\t\tself.extra_info = extra_info\n",
        "\n",
        "\t\treturn mean_z0, std_z0\n",
        "\n",
        "\n",
        "\tdef run_odernn(self, data, time_steps, \n",
        "\t\trun_backwards = True, save_info = False):\n",
        "\t\t# IMPORTANT: assumes that 'data' already has mask concatenated to it \n",
        "\n",
        "\t\tn_traj, n_tp, n_dims = data.size()\n",
        "\t\textra_info = []\n",
        "\n",
        "\t\tt0 = time_steps[-1]\n",
        "\t\tif run_backwards:\n",
        "\t\t\tt0 = time_steps[0]\n",
        "\n",
        "\t\tdevice = get_device(data)\n",
        "\n",
        "\t\tprev_y = torch.zeros((1, n_traj, self.latent_dim)).to(device)\n",
        "\t\tprev_std = torch.zeros((1, n_traj, self.latent_dim)).to(device)\n",
        "\n",
        "\t\tprev_t, t_i = time_steps[-1] + 0.01,  time_steps[-1]\n",
        "\n",
        "\t\tinterval_length = time_steps[-1] - time_steps[0]\n",
        "\t\tminimum_step = interval_length / 50\n",
        "\n",
        "\t\t#print(\"minimum step: {}\".format(minimum_step))\n",
        "\n",
        "\t\tassert(not torch.isnan(data).any())\n",
        "\t\tassert(not torch.isnan(time_steps).any())\n",
        "\n",
        "\t\tlatent_ys = []\n",
        "\t\t# Run ODE backwards and combine the y(t) estimates using gating\n",
        "\t\ttime_points_iter = range(0, len(time_steps))\n",
        "\t\tif run_backwards:\n",
        "\t\t\ttime_points_iter = reversed(time_points_iter)\n",
        "\n",
        "\t\tfor i in time_points_iter:\n",
        "\t\t\tif (prev_t - t_i) < minimum_step:\n",
        "\t\t\t\ttime_points = torch.stack((prev_t, t_i))\n",
        "\t\t\t\tinc = self.z0_diffeq_solver.ode_func(prev_t, prev_y) * (t_i - prev_t)\n",
        "\n",
        "\t\t\t\tassert(not torch.isnan(inc).any())\n",
        "\n",
        "\t\t\t\tode_sol = prev_y + inc\n",
        "\t\t\t\tode_sol = torch.stack((prev_y, ode_sol), 2).to(device)\n",
        "\n",
        "\t\t\t\tassert(not torch.isnan(ode_sol).any())\n",
        "\t\t\telse:\n",
        "\t\t\t\tn_intermediate_tp = max(2, ((prev_t - t_i) / minimum_step).int())\n",
        "\n",
        "\t\t\t\ttime_points = utils.linspace_vector(prev_t, t_i, n_intermediate_tp)\n",
        "\t\t\t\tode_sol = self.z0_diffeq_solver(prev_y, time_points)\n",
        "\n",
        "\t\t\t\tassert(not torch.isnan(ode_sol).any())\n",
        "\n",
        "\t\t\tif torch.mean(ode_sol[:, :, 0, :]  - prev_y) >= 0.001:\n",
        "\t\t\t\tprint(\"Error: first point of the ODE is not equal to initial value\")\n",
        "\t\t\t\tprint(torch.mean(ode_sol[:, :, 0, :]  - prev_y))\n",
        "\t\t\t\texit()\n",
        "\t\t\t#assert(torch.mean(ode_sol[:, :, 0, :]  - prev_y) < 0.001)\n",
        "\n",
        "\t\t\tyi_ode = ode_sol[:, :, -1, :]\n",
        "\t\t\txi = data[:,i,:].unsqueeze(0)\n",
        "\t\t\t\n",
        "\t\t\tyi, yi_std = self.GRU_update(yi_ode, prev_std, xi)\n",
        "\n",
        "\t\t\tprev_y, prev_std = yi, yi_std\t\t\t\n",
        "\t\t\tprev_t, t_i = time_steps[i],  time_steps[i-1]\n",
        "\n",
        "\t\t\tlatent_ys.append(yi)\n",
        "\n",
        "\t\t\tif save_info:\n",
        "\t\t\t\td = {\"yi_ode\": yi_ode.detach(), #\"yi_from_data\": yi_from_data,\n",
        "\t\t\t\t\t \"yi\": yi.detach(), \"yi_std\": yi_std.detach(), \n",
        "\t\t\t\t\t \"time_points\": time_points.detach(), \"ode_sol\": ode_sol.detach()}\n",
        "\t\t\t\textra_info.append(d)\n",
        "\n",
        "\t\tlatent_ys = torch.stack(latent_ys, 1)\n",
        "\n",
        "\t\tassert(not torch.isnan(yi).any())\n",
        "\t\tassert(not torch.isnan(yi_std).any())\n",
        "\n",
        "\t\treturn yi, yi_std, latent_ys, extra_info\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\tdef __init__(self, latent_dim, input_dim):\n",
        "\t\tsuper(Decoder, self).__init__()\n",
        "\t\t# decode data from latent space where we are solving an ODE back to the data space\n",
        "\n",
        "\t\tdecoder = nn.Sequential(\n",
        "\t\t   nn.Linear(latent_dim, input_dim),)\n",
        "\n",
        "\t\tutils.init_network_weights(decoder)\t\n",
        "\t\tself.decoder = decoder\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\t\treturn self.decoder(data)\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MtrsGDjhy_t"
      },
      "source": [
        "## likelihood_eval.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPhI4WBuhy_u"
      },
      "source": [
        "def gaussian_log_likelihood(mu_2d, data_2d, obsrv_std, indices = None):\n",
        "\tn_data_points = mu_2d.size()[-1]\n",
        "\n",
        "\tif n_data_points > 0:\n",
        "\t\tgaussian = Independent(Normal(loc = mu_2d, scale = obsrv_std.repeat(n_data_points)), 1)\n",
        "\t\tlog_prob = gaussian.log_prob(data_2d) \n",
        "\t\tlog_prob = log_prob / n_data_points \n",
        "\telse:\n",
        "\t\tlog_prob = torch.zeros([1]).to(get_device(data_2d)).squeeze()\n",
        "\treturn log_prob\n",
        "\n",
        "\n",
        "def poisson_log_likelihood(masked_log_lambdas, masked_data, indices, int_lambdas):\n",
        "\t# masked_log_lambdas and masked_data \n",
        "\tn_data_points = masked_data.size()[-1]\n",
        "\n",
        "\tif n_data_points > 0:\n",
        "\t\tlog_prob = torch.sum(masked_log_lambdas) - int_lambdas[indices]\n",
        "\t\t#log_prob = log_prob / n_data_points\n",
        "\telse:\n",
        "\t\tlog_prob = torch.zeros([1]).to(get_device(masked_data)).squeeze()\n",
        "\treturn log_prob\n",
        "\n",
        "\n",
        "\n",
        "def compute_binary_CE_loss(label_predictions, mortality_label):\n",
        "\t#print(\"Computing binary classification loss: compute_CE_loss\")\n",
        "\n",
        "\tmortality_label = mortality_label.reshape(-1)\n",
        "\n",
        "\tif len(label_predictions.size()) == 1:\n",
        "\t\tlabel_predictions = label_predictions.unsqueeze(0)\n",
        " \n",
        "\tn_traj_samples = label_predictions.size(0)\n",
        "\tlabel_predictions = label_predictions.reshape(n_traj_samples, -1)\n",
        "\t\n",
        "\tidx_not_nan = ~torch.isnan(mortality_label)\n",
        "\tif len(idx_not_nan) == 0.:\n",
        "\t\tprint(\"All are labels are NaNs!\")\n",
        "\t\tce_loss = torch.Tensor(0.).to(get_device(mortality_label))\n",
        "\n",
        "\tlabel_predictions = label_predictions[:,idx_not_nan]\n",
        "\tmortality_label = mortality_label[idx_not_nan]\n",
        "\n",
        "\tif torch.sum(mortality_label == 0.) == 0 or torch.sum(mortality_label == 1.) == 0:\n",
        "\t\tprint(\"Warning: all examples in a batch belong to the same class -- please increase the batch size.\")\n",
        "\n",
        "\tassert(not torch.isnan(label_predictions).any())\n",
        "\tassert(not torch.isnan(mortality_label).any())\n",
        "\n",
        "\t# For each trajectory, we get n_traj_samples samples from z0 -- compute loss on all of them\n",
        "\tmortality_label = mortality_label.repeat(n_traj_samples, 1)\n",
        "\tce_loss = nn.BCEWithLogitsLoss()(label_predictions, mortality_label)\n",
        "\n",
        "\t# divide by number of patients in a batch\n",
        "\tce_loss = ce_loss / n_traj_samples\n",
        "\treturn ce_loss\n",
        "\n",
        "\n",
        "def compute_multiclass_CE_loss(label_predictions, true_label, mask):\n",
        "\t#print(\"Computing multi-class classification loss: compute_multiclass_CE_loss\")\n",
        "\n",
        "\tif (len(label_predictions.size()) == 3):\n",
        "\t\tlabel_predictions = label_predictions.unsqueeze(0)\n",
        "\n",
        "\tn_traj_samples, n_traj, n_tp, n_dims = label_predictions.size()\n",
        "\n",
        "\t# assert(not torch.isnan(label_predictions).any())\n",
        "\t# assert(not torch.isnan(true_label).any())\n",
        "\n",
        "\t# For each trajectory, we get n_traj_samples samples from z0 -- compute loss on all of them\n",
        "\ttrue_label = true_label.repeat(n_traj_samples, 1, 1)\n",
        "\n",
        "\tlabel_predictions = label_predictions.reshape(n_traj_samples * n_traj * n_tp, n_dims)\n",
        "\ttrue_label = true_label.reshape(n_traj_samples * n_traj * n_tp, n_dims)\n",
        "\n",
        "\t# choose time points with at least one measurement\n",
        "\tmask = torch.sum(mask, -1) > 0\n",
        "\n",
        "\t# repeat the mask for each label to mark that the label for this time point is present\n",
        "\tpred_mask = mask.repeat(n_dims, 1,1).permute(1,2,0)\n",
        "\n",
        "\tlabel_mask = mask\n",
        "\tpred_mask = pred_mask.repeat(n_traj_samples,1,1,1)\n",
        "\tlabel_mask = label_mask.repeat(n_traj_samples,1,1,1)\n",
        "\n",
        "\tpred_mask = pred_mask.reshape(n_traj_samples * n_traj * n_tp,  n_dims)\n",
        "\tlabel_mask = label_mask.reshape(n_traj_samples * n_traj * n_tp, 1)\n",
        "\n",
        "\tif (label_predictions.size(-1) > 1) and (true_label.size(-1) > 1):\n",
        "\t\tassert(label_predictions.size(-1) == true_label.size(-1))\n",
        "\t\t# targets are in one-hot encoding -- convert to indices\n",
        "\t\t_, true_label = true_label.max(-1)\n",
        "\n",
        "\tres = []\n",
        "\tfor i in range(true_label.size(0)):\n",
        "\t\tpred_masked = torch.masked_select(label_predictions[i], pred_mask[i].bool())\n",
        "\t\tlabels = torch.masked_select(true_label[i], label_mask[i].bool())\n",
        "\t\n",
        "\t\tpred_masked = pred_masked.reshape(-1, n_dims)\n",
        "\n",
        "\t\tif (len(labels) == 0):\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tce_loss = nn.CrossEntropyLoss()(pred_masked, labels.long())\n",
        "\t\tres.append(ce_loss)\n",
        "\n",
        "\tce_loss = torch.stack(res, 0).to(get_device(label_predictions))\n",
        "\tce_loss = torch.mean(ce_loss)\n",
        "\t# # divide by number of patients in a batch\n",
        "\t# ce_loss = ce_loss / n_traj_samples\n",
        "\treturn ce_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_masked_likelihood(mu, data, mask, likelihood_func):\n",
        "\t# Compute the likelihood per patient and per attribute so that we don't priorize patients with more measurements\n",
        "\tn_traj_samples, n_traj, n_timepoints, n_dims = data.size()\n",
        "\n",
        "\tres = []\n",
        "\tfor i in range(n_traj_samples):\n",
        "\t\tfor k in range(n_traj):\n",
        "\t\t\tfor j in range(n_dims):\n",
        "\t\t\t\tdata_masked = torch.masked_select(data[i,k,:,j], mask[i,k,:,j].bool())\n",
        "\t\t\t\t\n",
        "\t\t\t\t#assert(torch.sum(data_masked == 0.) < 10)\n",
        "\n",
        "\t\t\t\tmu_masked = torch.masked_select(mu[i,k,:,j], mask[i,k,:,j].bool())\n",
        "\t\t\t\tlog_prob = likelihood_func(mu_masked, data_masked, indices = (i,k,j))\n",
        "\t\t\t\tres.append(log_prob)\n",
        "\t# shape: [n_traj*n_traj_samples, 1]\n",
        "\n",
        "\tres = torch.stack(res, 0).to(get_device(data))\n",
        "\tres = res.reshape((n_traj_samples, n_traj, n_dims))\n",
        "\t# Take mean over the number of dimensions\n",
        "\tres = torch.mean(res, -1) # !!!!!!!!!!! changed from sum to mean\n",
        "\tres = res.transpose(0,1)\n",
        "\treturn res\n",
        "\n",
        "\n",
        "def masked_gaussian_log_density(mu, data, obsrv_std, mask = None):\n",
        "\t# these cases are for plotting through plot_estim_density\n",
        "\tif (len(mu.size()) == 3):\n",
        "\t\t# add additional dimension for gp samples\n",
        "\t\tmu = mu.unsqueeze(0)\n",
        "\n",
        "\tif (len(data.size()) == 2):\n",
        "\t\t# add additional dimension for gp samples and time step\n",
        "\t\tdata = data.unsqueeze(0).unsqueeze(2)\n",
        "\telif (len(data.size()) == 3):\n",
        "\t\t# add additional dimension for gp samples\n",
        "\t\tdata = data.unsqueeze(0)\n",
        "\n",
        "\tn_traj_samples, n_traj, n_timepoints, n_dims = mu.size()\n",
        "\n",
        "\tassert(data.size()[-1] == n_dims)\n",
        "\n",
        "\t# Shape after permutation: [n_traj, n_traj_samples, n_timepoints, n_dims]\n",
        "\tif mask is None:\n",
        "\t\tmu_flat = mu.reshape(n_traj_samples*n_traj, n_timepoints * n_dims)\n",
        "\t\tn_traj_samples, n_traj, n_timepoints, n_dims = data.size()\n",
        "\t\tdata_flat = data.reshape(n_traj_samples*n_traj, n_timepoints * n_dims)\n",
        "\t\n",
        "\t\tres = gaussian_log_likelihood(mu_flat, data_flat, obsrv_std)\n",
        "\t\tres = res.reshape(n_traj_samples, n_traj).transpose(0,1)\n",
        "\telse:\n",
        "\t\t# Compute the likelihood per patient so that we don't priorize patients with more measurements\n",
        "\t\tfunc = lambda mu, data, indices: gaussian_log_likelihood(mu, data, obsrv_std = obsrv_std, indices = indices)\n",
        "\t\tres = compute_masked_likelihood(mu, data, mask, func)\n",
        "\treturn res\n",
        "\n",
        "\n",
        "\n",
        "def mse(mu, data, indices = None):\n",
        "\tn_data_points = mu.size()[-1]\n",
        "\n",
        "\tif n_data_points > 0:\n",
        "\t\tmse = nn.MSELoss()(mu, data)\n",
        "\telse:\n",
        "\t\tmse = torch.zeros([1]).to(get_device(data)).squeeze()\n",
        "\treturn mse\n",
        "\n",
        "\n",
        "def compute_mse(mu, data, mask = None):\n",
        "\t# these cases are for plotting through plot_estim_density\n",
        "\tif (len(mu.size()) == 3):\n",
        "\t\t# add additional dimension for gp samples\n",
        "\t\tmu = mu.unsqueeze(0)\n",
        "\n",
        "\tif (len(data.size()) == 2):\n",
        "\t\t# add additional dimension for gp samples and time step\n",
        "\t\tdata = data.unsqueeze(0).unsqueeze(2)\n",
        "\telif (len(data.size()) == 3):\n",
        "\t\t# add additional dimension for gp samples\n",
        "\t\tdata = data.unsqueeze(0)\n",
        "\n",
        "\tn_traj_samples, n_traj, n_timepoints, n_dims = mu.size()\n",
        "\tassert(data.size()[-1] == n_dims)\n",
        "\n",
        "\t# Shape after permutation: [n_traj, n_traj_samples, n_timepoints, n_dims]\n",
        "\tif mask is None:\n",
        "\t\tmu_flat = mu.reshape(n_traj_samples*n_traj, n_timepoints * n_dims)\n",
        "\t\tn_traj_samples, n_traj, n_timepoints, n_dims = data.size()\n",
        "\t\tdata_flat = data.reshape(n_traj_samples*n_traj, n_timepoints * n_dims)\n",
        "\t\tres = mse(mu_flat, data_flat)\n",
        "\telse:\n",
        "\t\t# Compute the likelihood per patient so that we don't priorize patients with more measurements\n",
        "\t\tres = compute_masked_likelihood(mu, data, mask, mse)\n",
        "\treturn res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_poisson_proc_likelihood(truth, pred_y, info, mask = None):\n",
        "\t# Compute Poisson likelihood\n",
        "\t# https://math.stackexchange.com/questions/344487/log-likelihood-of-a-realization-of-a-poisson-process\n",
        "\t# Sum log lambdas across all time points\n",
        "\tif mask is None:\n",
        "\t\tpoisson_log_l = torch.sum(info[\"log_lambda_y\"], 2) - info[\"int_lambda\"]\n",
        "\t\t# Sum over data dims\n",
        "\t\tpoisson_log_l = torch.mean(poisson_log_l, -1)\n",
        "\telse:\n",
        "\t\t# Compute likelihood of the data under the predictions\n",
        "\t\ttruth_repeated = truth.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\t\tmask_repeated = mask.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\n",
        "\t\t# Compute the likelihood per patient and per attribute so that we don't priorize patients with more measurements\n",
        "\t\tint_lambda = info[\"int_lambda\"]\n",
        "\t\tf = lambda log_lam, data, indices: poisson_log_likelihood(log_lam, data, indices, int_lambda)\n",
        "\t\tpoisson_log_l = compute_masked_likelihood(info[\"log_lambda_y\"], truth_repeated, mask_repeated, f)\n",
        "\t\tpoisson_log_l = poisson_log_l.permute(1,0)\n",
        "\t\t# Take mean over n_traj\n",
        "\t\t#poisson_log_l = torch.mean(poisson_log_l, 1)\n",
        "\t\t\n",
        "\t# poisson_log_l shape: [n_traj_samples, n_traj]\n",
        "\treturn poisson_log_l\n",
        "\n",
        "\t\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4dZh3AMhy_v"
      },
      "source": [
        "## base_models.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX5bWiLXhy_v"
      },
      "source": [
        "###########################\n",
        "# Latent ODEs for Irregularly-Sampled Time Series\n",
        "# Author: Yulia Rubanova\n",
        "###########################\n",
        "\n",
        "\n",
        "def create_classifier(z0_dim, n_labels):\n",
        "\treturn nn.Sequential(\n",
        "\t\t\tnn.Linear(z0_dim, 300),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(300, 300),\n",
        "\t\t\tnn.ReLU(),\n",
        "\t\t\tnn.Linear(300, n_labels),)\n",
        "\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "\tdef __init__(self, input_dim, latent_dim, device, \n",
        "\t\tobsrv_std = 0.01, use_binary_classif = False,\n",
        "\t\tclassif_per_tp = False,\n",
        "\t\tuse_poisson_proc = False,\n",
        "\t\tlinear_classifier = False,\n",
        "\t\tn_labels = 1,\n",
        "\t\ttrain_classif_w_reconstr = False):\n",
        "\t\tsuper(Baseline, self).__init__()\n",
        "\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.n_labels = n_labels\n",
        "\n",
        "\t\tself.obsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
        "\t\tself.device = device\n",
        "\n",
        "\t\tself.use_binary_classif = use_binary_classif\n",
        "\t\tself.classif_per_tp = classif_per_tp\n",
        "\t\tself.use_poisson_proc = use_poisson_proc\n",
        "\t\tself.linear_classifier = linear_classifier\n",
        "\t\tself.train_classif_w_reconstr = train_classif_w_reconstr\n",
        "\n",
        "\t\tz0_dim = latent_dim\n",
        "\t\tif use_poisson_proc:\n",
        "\t\t\tz0_dim += latent_dim\n",
        "\n",
        "\t\tif use_binary_classif: \n",
        "\t\t\tif linear_classifier:\n",
        "\t\t\t\tself.classifier = nn.Sequential(\n",
        "\t\t\t\t\tnn.Linear(z0_dim, n_labels))\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.classifier = create_classifier(z0_dim, n_labels)\n",
        "\t\t\tutils.init_network_weights(self.classifier)\n",
        "\n",
        "\n",
        "\tdef get_gaussian_likelihood(self, truth, pred_y, mask = None):\n",
        "\t\t# pred_y shape [n_traj_samples, n_traj, n_tp, n_dim]\n",
        "\t\t# truth shape  [n_traj, n_tp, n_dim]\n",
        "\t\tif mask is not None:\n",
        "\t\t\tmask = mask.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\n",
        "\t\t# Compute likelihood of the data under the predictions\n",
        "\t\tlog_density_data = masked_gaussian_log_density(pred_y, truth, \n",
        "\t\t\tobsrv_std = self.obsrv_std, mask = mask)\n",
        "\t\tlog_density_data = log_density_data.permute(1,0)\n",
        "\n",
        "\t\t# Compute the total density\n",
        "\t\t# Take mean over n_traj_samples\n",
        "\t\tlog_density = torch.mean(log_density_data, 0)\n",
        "\n",
        "\t\t# shape: [n_traj]\n",
        "\t\treturn log_density\n",
        "\n",
        "\n",
        "\tdef get_mse(self, truth, pred_y, mask = None):\n",
        "\t\t# pred_y shape [n_traj_samples, n_traj, n_tp, n_dim]\n",
        "\t\t# truth shape  [n_traj, n_tp, n_dim]\n",
        "\t\tif mask is not None:\n",
        "\t\t\tmask = mask.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\n",
        "\t\t# Compute likelihood of the data under the predictions\n",
        "\t\tlog_density_data = compute_mse(pred_y, truth, mask = mask)\n",
        "\t\t# shape: [1]\n",
        "\t\treturn torch.mean(log_density_data)\n",
        "\n",
        "\n",
        "\tdef compute_all_losses(self, batch_dict,\n",
        "\t\tn_tp_to_sample = None, n_traj_samples = 1, kl_coef = 1.):\n",
        "\n",
        "\t\t# Condition on subsampled points\n",
        "\t\t# Make predictions for all the points\n",
        "\t\tpred_x, info = self.get_reconstruction(batch_dict[\"tp_to_predict\"], \n",
        "\t\t\tbatch_dict[\"observed_data\"], batch_dict[\"observed_tp\"], \n",
        "\t\t\tmask = batch_dict[\"observed_mask\"], n_traj_samples = n_traj_samples,\n",
        "\t\t\tmode = batch_dict[\"mode\"])\n",
        "\n",
        "\t\t# Compute likelihood of all the points\n",
        "\t\tlikelihood = self.get_gaussian_likelihood(batch_dict[\"data_to_predict\"], pred_x,\n",
        "\t\t\tmask = batch_dict[\"mask_predicted_data\"])\n",
        "\n",
        "\t\tmse = self.get_mse(batch_dict[\"data_to_predict\"], pred_x,\n",
        "\t\t\tmask = batch_dict[\"mask_predicted_data\"])\n",
        "\n",
        "\t\t################################\n",
        "\t\t# Compute CE loss for binary classification on Physionet\n",
        "\t\t# Use only last attribute -- mortatility in the hospital \n",
        "\t\tdevice = get_device(batch_dict[\"data_to_predict\"])\n",
        "\t\tce_loss = torch.Tensor([0.]).to(device)\n",
        "\t\t\n",
        "\t\tif (batch_dict[\"labels\"] is not None) and self.use_binary_classif:\n",
        "\t\t\tif (batch_dict[\"labels\"].size(-1) == 1) or (len(batch_dict[\"labels\"].size()) == 1):\n",
        "\t\t\t\tce_loss = compute_binary_CE_loss(\n",
        "\t\t\t\t\tinfo[\"label_predictions\"], \n",
        "\t\t\t\t\tbatch_dict[\"labels\"])\n",
        "\t\t\telse:\n",
        "\t\t\t\tce_loss = compute_multiclass_CE_loss(\n",
        "\t\t\t\t\tinfo[\"label_predictions\"], \n",
        "\t\t\t\t\tbatch_dict[\"labels\"],\n",
        "\t\t\t\t\tmask = batch_dict[\"mask_predicted_data\"])\n",
        "\n",
        "\t\t\tif torch.isnan(ce_loss):\n",
        "\t\t\t\tprint(\"label pred\")\n",
        "\t\t\t\tprint(info[\"label_predictions\"])\n",
        "\t\t\t\tprint(\"labels\")\n",
        "\t\t\t\tprint( batch_dict[\"labels\"])\n",
        "\t\t\t\traise Exception(\"CE loss is Nan!\")\n",
        "\n",
        "\t\tpois_log_likelihood = torch.Tensor([0.]).to(get_device(batch_dict[\"data_to_predict\"]))\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tpois_log_likelihood = compute_poisson_proc_likelihood(\n",
        "\t\t\t\tbatch_dict[\"data_to_predict\"], pred_x, \n",
        "\t\t\t\tinfo, mask = batch_dict[\"mask_predicted_data\"])\n",
        "\t\t\t# Take mean over n_traj\n",
        "\t\t\tpois_log_likelihood = torch.mean(pois_log_likelihood, 1)\n",
        "\n",
        "\t\tloss = - torch.mean(likelihood)\n",
        "\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tloss = loss - 0.1 * pois_log_likelihood \n",
        "\n",
        "\t\tif self.use_binary_classif:\n",
        "\t\t\tif self.train_classif_w_reconstr:\n",
        "\t\t\t\tloss = loss +  ce_loss * 100\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss =  ce_loss\n",
        "\n",
        "\t\t# Take mean over the number of samples in a batch\n",
        "\t\tresults = {}\n",
        "\t\tresults[\"loss\"] = torch.mean(loss)\n",
        "\t\tresults[\"likelihood\"] = torch.mean(likelihood).detach()\n",
        "\t\tresults[\"mse\"] = torch.mean(mse).detach()\n",
        "\t\tresults[\"pois_likelihood\"] = torch.mean(pois_log_likelihood).detach()\n",
        "\t\tresults[\"ce_loss\"] = torch.mean(ce_loss).detach()\n",
        "\t\tresults[\"kl\"] = 0.\n",
        "\t\tresults[\"kl_first_p\"] =  0.\n",
        "\t\tresults[\"std_first_p\"] = 0.\n",
        "\n",
        "\t\tif batch_dict[\"labels\"] is not None and self.use_binary_classif:\n",
        "\t\t\tresults[\"label_predictions\"] = info[\"label_predictions\"].detach()\n",
        "\t\treturn results\n",
        "\n",
        "\n",
        "\n",
        "class VAE_Baseline(nn.Module):\n",
        "\tdef __init__(self, input_dim, latent_dim, \n",
        "\t\tz0_prior, device,\n",
        "\t\tobsrv_std = 0.01, \n",
        "\t\tuse_binary_classif = False,\n",
        "\t\tclassif_per_tp = False,\n",
        "\t\tuse_poisson_proc = False,\n",
        "\t\tlinear_classifier = False,\n",
        "\t\tn_labels = 1,\n",
        "\t\ttrain_classif_w_reconstr = False):\n",
        "\n",
        "\t\tsuper(VAE_Baseline, self).__init__()\n",
        "\t\t\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.device = device\n",
        "\t\tself.n_labels = n_labels\n",
        "\n",
        "\t\tself.obsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
        "\n",
        "\t\tself.z0_prior = z0_prior\n",
        "\t\tself.use_binary_classif = use_binary_classif\n",
        "\t\tself.classif_per_tp = classif_per_tp\n",
        "\t\tself.use_poisson_proc = use_poisson_proc\n",
        "\t\tself.linear_classifier = linear_classifier\n",
        "\t\tself.train_classif_w_reconstr = train_classif_w_reconstr\n",
        "\n",
        "\t\tz0_dim = latent_dim\n",
        "\t\tif use_poisson_proc:\n",
        "\t\t\tz0_dim += latent_dim\n",
        "\n",
        "\t\tif use_binary_classif: \n",
        "\t\t\tif linear_classifier:\n",
        "\t\t\t\tself.classifier = nn.Sequential(\n",
        "\t\t\t\t\tnn.Linear(z0_dim, n_labels))\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.classifier = create_classifier(z0_dim, n_labels)\n",
        "\t\t\tutils.init_network_weights(self.classifier)\n",
        "\n",
        "\n",
        "\tdef get_gaussian_likelihood(self, truth, pred_y, mask = None):\n",
        "\t\t# pred_y shape [n_traj_samples, n_traj, n_tp, n_dim]\n",
        "\t\t# truth shape  [n_traj, n_tp, n_dim]\n",
        "\t\tn_traj, n_tp, n_dim = truth.size()\n",
        "\n",
        "\t\t# Compute likelihood of the data under the predictions\n",
        "\t\ttruth_repeated = truth.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\t\t\n",
        "\t\tif mask is not None:\n",
        "\t\t\tmask = mask.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\t\tlog_density_data = masked_gaussian_log_density(pred_y, truth_repeated, \n",
        "\t\t\tobsrv_std = self.obsrv_std, mask = mask)\n",
        "\t\tlog_density_data = log_density_data.permute(1,0)\n",
        "\t\tlog_density = torch.mean(log_density_data, 1)\n",
        "\n",
        "\t\t# shape: [n_traj_samples]\n",
        "\t\treturn log_density\n",
        "\n",
        "\n",
        "\tdef get_mse(self, truth, pred_y, mask = None):\n",
        "\t\t# pred_y shape [n_traj_samples, n_traj, n_tp, n_dim]\n",
        "\t\t# truth shape  [n_traj, n_tp, n_dim]\n",
        "\t\tn_traj, n_tp, n_dim = truth.size()\n",
        "\n",
        "\t\t# Compute likelihood of the data under the predictions\n",
        "\t\ttruth_repeated = truth.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\t\t\n",
        "\t\tif mask is not None:\n",
        "\t\t\tmask = mask.repeat(pred_y.size(0), 1, 1, 1)\n",
        "\n",
        "\t\t# Compute likelihood of the data under the predictions\n",
        "\t\tlog_density_data = compute_mse(pred_y, truth_repeated, mask = mask)\n",
        "\t\t# shape: [1]\n",
        "\t\treturn torch.mean(log_density_data)\n",
        "\n",
        "\n",
        "\tdef compute_all_losses(self, batch_dict, n_traj_samples = 1, kl_coef = 1.):\n",
        "\t\t# Condition on subsampled points\n",
        "\t\t# Make predictions for all the points\n",
        "\t\tpred_y, info = self.get_reconstruction(batch_dict[\"tp_to_predict\"], \n",
        "\t\t\tbatch_dict[\"observed_data\"], batch_dict[\"observed_tp\"], \n",
        "\t\t\tmask = batch_dict[\"observed_mask\"], n_traj_samples = n_traj_samples,\n",
        "\t\t\tmode = batch_dict[\"mode\"])\n",
        "\n",
        "\t\t#print(\"get_reconstruction done -- computing likelihood\")\n",
        "\t\tfp_mu, fp_std, fp_enc = info[\"first_point\"]\n",
        "\t\tfp_std = fp_std.abs()\n",
        "\t\tfp_distr = Normal(fp_mu, fp_std)\n",
        "\n",
        "\t\tassert(torch.sum(fp_std < 0) == 0.)\n",
        "\n",
        "\t\tkldiv_z0 = kl_divergence(fp_distr, self.z0_prior)\n",
        "\n",
        "\t\tif torch.isnan(kldiv_z0).any():\n",
        "\t\t\tprint(fp_mu)\n",
        "\t\t\tprint(fp_std)\n",
        "\t\t\traise Exception(\"kldiv_z0 is Nan!\")\n",
        "\n",
        "\t\t# Mean over number of latent dimensions\n",
        "\t\t# kldiv_z0 shape: [n_traj_samples, n_traj, n_latent_dims] if prior is a mixture of gaussians (KL is estimated)\n",
        "\t\t# kldiv_z0 shape: [1, n_traj, n_latent_dims] if prior is a standard gaussian (KL is computed exactly)\n",
        "\t\t# shape after: [n_traj_samples]\n",
        "\t\tkldiv_z0 = torch.mean(kldiv_z0,(1,2))\n",
        "\n",
        "\t\t# Compute likelihood of all the points\n",
        "\t\trec_likelihood = self.get_gaussian_likelihood(\n",
        "\t\t\tbatch_dict[\"data_to_predict\"], pred_y,\n",
        "\t\t\tmask = batch_dict[\"mask_predicted_data\"])\n",
        "\n",
        "\t\tmse = self.get_mse(\n",
        "\t\t\tbatch_dict[\"data_to_predict\"], pred_y,\n",
        "\t\t\tmask = batch_dict[\"mask_predicted_data\"])\n",
        "\n",
        "\t\tpois_log_likelihood = torch.Tensor([0.]).to(get_device(batch_dict[\"data_to_predict\"]))\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tpois_log_likelihood = compute_poisson_proc_likelihood(\n",
        "\t\t\t\tbatch_dict[\"data_to_predict\"], pred_y, \n",
        "\t\t\t\tinfo, mask = batch_dict[\"mask_predicted_data\"])\n",
        "\t\t\t# Take mean over n_traj\n",
        "\t\t\tpois_log_likelihood = torch.mean(pois_log_likelihood, 1)\n",
        "\n",
        "\t\t################################\n",
        "\t\t# Compute CE loss for binary classification on Physionet\n",
        "\t\tdevice = get_device(batch_dict[\"data_to_predict\"])\n",
        "\t\tce_loss = torch.Tensor([0.]).to(device)\n",
        "\t\tif (batch_dict[\"labels\"] is not None) and self.use_binary_classif:\n",
        "\n",
        "\t\t\tif (batch_dict[\"labels\"].size(-1) == 1) or (len(batch_dict[\"labels\"].size()) == 1):\n",
        "\t\t\t\tce_loss = compute_binary_CE_loss(\n",
        "\t\t\t\t\tinfo[\"label_predictions\"], \n",
        "\t\t\t\t\tbatch_dict[\"labels\"])\n",
        "\t\t\telse:\n",
        "\t\t\t\tce_loss = compute_multiclass_CE_loss(\n",
        "\t\t\t\t\tinfo[\"label_predictions\"], \n",
        "\t\t\t\t\tbatch_dict[\"labels\"],\n",
        "\t\t\t\t\tmask = batch_dict[\"mask_predicted_data\"])\n",
        "\n",
        "\t\t# IWAE loss\n",
        "\t\tloss = - torch.logsumexp(rec_likelihood -  kl_coef * kldiv_z0,0)\n",
        "\t\tif torch.isnan(loss):\n",
        "\t\t\tloss = - torch.mean(rec_likelihood - kl_coef * kldiv_z0,0)\n",
        "\t\t\t\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tloss = loss - 0.1 * pois_log_likelihood \n",
        "\n",
        "\t\tif self.use_binary_classif:\n",
        "\t\t\tif self.train_classif_w_reconstr:\n",
        "\t\t\t\tloss = loss +  ce_loss * 100\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss =  ce_loss\n",
        "\n",
        "\t\tresults = {}\n",
        "\t\tresults['pred_y'] = pred_y\n",
        "\t\tresults['true_y'] = batch_dict[\"data_to_predict\"]\n",
        "\t\tresults[\"loss\"] = torch.mean(loss)\n",
        "\t\tresults[\"likelihood\"] = torch.mean(rec_likelihood).detach()\n",
        "\t\tresults[\"mse\"] = torch.mean(mse).detach()\n",
        "\t\tresults[\"pois_likelihood\"] = torch.mean(pois_log_likelihood).detach()\n",
        "\t\tresults[\"ce_loss\"] = torch.mean(ce_loss).detach()\n",
        "\t\tresults[\"kl_first_p\"] =  torch.mean(kldiv_z0).detach()\n",
        "\t\tresults[\"std_first_p\"] = torch.mean(fp_std).detach()\n",
        "\n",
        "\t\tif batch_dict[\"labels\"] is not None and self.use_binary_classif:\n",
        "\t\t\tresults[\"label_predictions\"] = info[\"label_predictions\"].detach()\n",
        "\n",
        "\t\treturn results\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppm_XPFfhy_w"
      },
      "source": [
        "## ode_run.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBD0-pmThy_w"
      },
      "source": [
        "class ODE_RNN(Baseline):\n",
        "\tdef __init__(self, input_dim, latent_dim, device = torch.device(\"cpu\"),\n",
        "\t\tz0_diffeq_solver = None, n_gru_units = 100,  n_units = 100,\n",
        "\t\tconcat_mask = False, obsrv_std = 0.1, use_binary_classif = False,\n",
        "\t\tclassif_per_tp = False, n_labels = 1, train_classif_w_reconstr = False):\n",
        "\n",
        "\t\tBaseline.__init__(self, input_dim, latent_dim, device = device, \n",
        "\t\t\tobsrv_std = obsrv_std, use_binary_classif = use_binary_classif,\n",
        "\t\t\tclassif_per_tp = classif_per_tp,\n",
        "\t\t\tn_labels = n_labels,\n",
        "\t\t\ttrain_classif_w_reconstr = train_classif_w_reconstr)\n",
        "\n",
        "\t\tode_rnn_encoder_dim = latent_dim\n",
        "\t\n",
        "\t\tself.ode_gru = Encoder_z0_ODE_RNN( \n",
        "\t\t\tlatent_dim = ode_rnn_encoder_dim, \n",
        "\t\t\tinput_dim = (input_dim) * 2, # input and the mask\n",
        "\t\t\tz0_diffeq_solver = z0_diffeq_solver, \n",
        "\t\t\tn_gru_units = n_gru_units, \n",
        "\t\t\tdevice = device).to(device)\n",
        "\n",
        "\t\tself.z0_diffeq_solver = z0_diffeq_solver\n",
        "\n",
        "\t\tself.decoder = nn.Sequential(\n",
        "\t\t\tnn.Linear(latent_dim, n_units),\n",
        "\t\t\tnn.Tanh(),\n",
        "\t\t\tnn.Linear(n_units, input_dim),)\n",
        "\n",
        "\t\tutils.init_network_weights(self.decoder)\n",
        "\n",
        "\n",
        "\tdef get_reconstruction(self, time_steps_to_predict, data, truth_time_steps, \n",
        "\t\tmask = None, n_traj_samples = None, mode = None):\n",
        "\n",
        "\t\tif (len(truth_time_steps) != len(time_steps_to_predict)) or (torch.sum(time_steps_to_predict - truth_time_steps) != 0):\n",
        "\t\t\traise Exception(\"Extrapolation mode not implemented for ODE-RNN\")\n",
        "\n",
        "\t\t# time_steps_to_predict and truth_time_steps should be the same \n",
        "\t\tassert(len(truth_time_steps) == len(time_steps_to_predict))\n",
        "\t\tassert(mask is not None)\n",
        "\t\t\n",
        "\t\tdata_and_mask = data\n",
        "\t\tif mask is not None:\n",
        "\t\t\tdata_and_mask = torch.cat([data, mask],-1)\n",
        "\n",
        "\t\t_, _, latent_ys, _ = self.ode_gru.run_odernn(\n",
        "\t\t\tdata_and_mask, truth_time_steps, run_backwards = False)\n",
        "\t\t\n",
        "\t\tlatent_ys = latent_ys.permute(0,2,1,3)\n",
        "\t\tlast_hidden = latent_ys[:,:,-1,:]\n",
        "\n",
        "\t\t\t#assert(torch.sum(int_lambda[0,0,-1,:] <= 0) == 0.)\n",
        "\n",
        "\t\toutputs = self.decoder(latent_ys)\n",
        "\t\t# Shift outputs for computing the loss -- we should compare the first output to the second data point, etc.\n",
        "\t\tfirst_point = data[:,0,:]\n",
        "\t\toutputs = utils.shift_outputs(outputs, first_point)\n",
        "\n",
        "\t\textra_info = {\"first_point\": (latent_ys[:,:,-1,:], 0.0, latent_ys[:,:,-1,:])}\n",
        "\n",
        "\t\tif self.use_binary_classif:\n",
        "\t\t\tif self.classif_per_tp:\n",
        "\t\t\t\textra_info[\"label_predictions\"] = self.classifier(latent_ys)\n",
        "\t\t\telse:\n",
        "\t\t\t\textra_info[\"label_predictions\"] = self.classifier(last_hidden).squeeze(-1)\n",
        "\n",
        "\t\t# outputs shape: [n_traj_samples, n_traj, n_tp, n_dims]\n",
        "\t\treturn outputs, extra_info\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDnVAqzWhy_w"
      },
      "source": [
        "## latent_ode.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF67TleQhy_x"
      },
      "source": [
        "class LatentODE(VAE_Baseline):\n",
        "\tdef __init__(self, input_dim, latent_dim, encoder_z0, decoder, diffeq_solver, \n",
        "\t\tz0_prior, device, obsrv_std = None, \n",
        "\t\tuse_binary_classif = False, use_poisson_proc = False,\n",
        "\t\tlinear_classifier = False,\n",
        "\t\tclassif_per_tp = False,\n",
        "\t\tn_labels = 1,\n",
        "\t\ttrain_classif_w_reconstr = False):\n",
        "\n",
        "\t\tsuper(LatentODE, self).__init__(\n",
        "\t\t\tinput_dim = input_dim, latent_dim = latent_dim, \n",
        "\t\t\tz0_prior = z0_prior, \n",
        "\t\t\tdevice = device, obsrv_std = obsrv_std, \n",
        "\t\t\tuse_binary_classif = use_binary_classif,\n",
        "\t\t\tclassif_per_tp = classif_per_tp, \n",
        "\t\t\tlinear_classifier = linear_classifier,\n",
        "\t\t\tuse_poisson_proc = use_poisson_proc,\n",
        "\t\t\tn_labels = n_labels,\n",
        "\t\t\ttrain_classif_w_reconstr = train_classif_w_reconstr)\n",
        "\n",
        "\t\tself.encoder_z0 = encoder_z0\n",
        "\t\tself.diffeq_solver = diffeq_solver\n",
        "\t\tself.decoder = decoder\n",
        "\t\tself.use_poisson_proc = use_poisson_proc\n",
        "\n",
        "\tdef get_reconstruction(self, time_steps_to_predict, truth, truth_time_steps, \n",
        "\t\tmask = None, n_traj_samples = 1, run_backwards = True, mode = None):\n",
        "\n",
        "\t\tif isinstance(self.encoder_z0, Encoder_z0_ODE_RNN) or \\\n",
        "\t\t\tisinstance(self.encoder_z0, Encoder_z0_RNN):\n",
        "\n",
        "\t\t\ttruth_w_mask = truth\n",
        "\t\t\tif mask is not None:\n",
        "\t\t\t\ttruth_w_mask = torch.cat((truth, mask), -1)\n",
        "\t\t\tfirst_point_mu, first_point_std = self.encoder_z0(\n",
        "\t\t\t\ttruth_w_mask, truth_time_steps, run_backwards = run_backwards)\n",
        "\n",
        "\t\t\tmeans_z0 = first_point_mu.repeat(n_traj_samples, 1, 1)\n",
        "\t\t\tsigma_z0 = first_point_std.repeat(n_traj_samples, 1, 1)\n",
        "\t\t\tfirst_point_enc = utils.sample_standard_gaussian(means_z0, sigma_z0)\n",
        "\n",
        "\t\telse:\n",
        "\t\t\traise Exception(\"Unknown encoder type {}\".format(type(self.encoder_z0).__name__))\n",
        "\t\t\n",
        "\t\tfirst_point_std = first_point_std.abs()\n",
        "\t\tassert(torch.sum(first_point_std < 0) == 0.)\n",
        "\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tn_traj_samples, n_traj, n_dims = first_point_enc.size()\n",
        "\t\t\t# append a vector of zeros to compute the integral of lambda\n",
        "\t\t\tzeros = torch.zeros([n_traj_samples, n_traj,self.input_dim]).to(get_device(truth))\n",
        "\t\t\tfirst_point_enc_aug = torch.cat((first_point_enc, zeros), -1)\n",
        "\t\t\tmeans_z0_aug = torch.cat((means_z0, zeros), -1)\n",
        "\t\telse:\n",
        "\t\t\tfirst_point_enc_aug = first_point_enc\n",
        "\t\t\tmeans_z0_aug = means_z0\n",
        "\t\t\t\n",
        "\t\tassert(not torch.isnan(time_steps_to_predict).any())\n",
        "\t\tassert(not torch.isnan(first_point_enc).any())\n",
        "\t\tassert(not torch.isnan(first_point_enc_aug).any())\n",
        "\n",
        "\t\t# Shape of sol_y [n_traj_samples, n_samples, n_timepoints, n_latents]\n",
        "\t\tsol_y = self.diffeq_solver(first_point_enc_aug, time_steps_to_predict)\n",
        "\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tsol_y, log_lambda_y, int_lambda, _ = self.diffeq_solver.ode_func.extract_poisson_rate(sol_y)\n",
        "\n",
        "\t\t\tassert(torch.sum(int_lambda[:,:,0,:]) == 0.)\n",
        "\t\t\tassert(torch.sum(int_lambda[0,0,-1,:] <= 0) == 0.)\n",
        "\n",
        "\t\tpred_x = self.decoder(sol_y)\n",
        "\n",
        "\t\tall_extra_info = {\n",
        "\t\t\t\"first_point\": (first_point_mu, first_point_std, first_point_enc),\n",
        "\t\t\t\"latent_traj\": sol_y.detach()\n",
        "\t\t}\n",
        "\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\t# intergral of lambda from the last step of ODE Solver\n",
        "\t\t\tall_extra_info[\"int_lambda\"] = int_lambda[:,:,-1,:]\n",
        "\t\t\tall_extra_info[\"log_lambda_y\"] = log_lambda_y\n",
        "\n",
        "\t\tif self.use_binary_classif:\n",
        "\t\t\tif self.classif_per_tp:\n",
        "\t\t\t\tall_extra_info[\"label_predictions\"] = self.classifier(sol_y)\n",
        "\t\t\telse:\n",
        "\t\t\t\tall_extra_info[\"label_predictions\"] = self.classifier(first_point_enc).squeeze(-1)\n",
        "\n",
        "\t\treturn pred_x, all_extra_info\n",
        "\n",
        "\n",
        "\tdef sample_traj_from_prior(self, time_steps_to_predict, n_traj_samples = 1):\n",
        "\t\t# input_dim = starting_point.size()[-1]\n",
        "\t\t# starting_point = starting_point.view(1,1,input_dim)\n",
        "\n",
        "\t\t# Sample z0 from prior\n",
        "\t\tstarting_point_enc = self.z0_prior.sample([n_traj_samples, 1, self.latent_dim]).squeeze(-1)\n",
        "\n",
        "\t\tstarting_point_enc_aug = starting_point_enc\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tn_traj_samples, n_traj, n_dims = starting_point_enc.size()\n",
        "\t\t\t# append a vector of zeros to compute the integral of lambda\n",
        "\t\t\tzeros = torch.zeros(n_traj_samples, n_traj,self.input_dim).to(self.device)\n",
        "\t\t\tstarting_point_enc_aug = torch.cat((starting_point_enc, zeros), -1)\n",
        "\n",
        "\t\tsol_y = self.diffeq_solver.sample_traj_from_prior(starting_point_enc_aug, time_steps_to_predict, \n",
        "\t\t\tn_traj_samples = 3)\n",
        "\n",
        "\t\tif self.use_poisson_proc:\n",
        "\t\t\tsol_y, log_lambda_y, int_lambda, _ = self.diffeq_solver.ode_func.extract_poisson_rate(sol_y)\n",
        "\t\t\n",
        "\t\treturn self.decoder(sol_y)\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7EhkS2Uhy_x"
      },
      "source": [
        "## ode_func.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ard2Qg5chy_x"
      },
      "source": [
        "class ODEFunc(nn.Module):\n",
        "\tdef __init__(self, input_dim, latent_dim, ode_func_net, device = torch.device(\"cpu\")):\n",
        "\t\t\"\"\"\n",
        "\t\tinput_dim: dimensionality of the input\n",
        "\t\tlatent_dim: dimensionality used for ODE. Analog of a continous latent state\n",
        "\t\t\"\"\"\n",
        "\t\t# print(f\"Inside ODEFunc class\")\n",
        "\t\tsuper(ODEFunc, self).__init__()\n",
        "\n",
        "\t\tself.input_dim = input_dim\n",
        "\t\t# print(f\"input_dim is {input_dim}\")\n",
        "\t\t# print(f\"latent_dim is {latent_dim}\")\n",
        "\t\tself.device = device\n",
        "\n",
        "\t\tutils.init_network_weights(ode_func_net)\n",
        "\t\tself.gradient_net = ode_func_net\n",
        "\n",
        "\tdef forward(self, t_local, y, backwards = False):\n",
        "\t\t\"\"\"\n",
        "\t\tPerform one step in solving ODE. Given current data point y and current time point t_local, returns gradient dy/dt at this time point\n",
        "\n",
        "\t\tt_local: current time point\n",
        "\t\ty: value at the current time point\n",
        "\t\t\"\"\"\n",
        "\t\tgrad = self.get_ode_gradient_nn(t_local, y)\n",
        "\t\tif backwards:\n",
        "\t\t\tgrad = -grad\n",
        "\t\treturn grad\n",
        "\n",
        "\tdef get_ode_gradient_nn(self, t_local, y):\n",
        "\t\treturn self.gradient_net(y)\n",
        "\n",
        "\tdef sample_next_point_from_prior(self, t_local, y):\n",
        "\t\t\"\"\"\n",
        "\t\tt_local: current time point\n",
        "\t\ty: value at the current time point\n",
        "\t\t\"\"\"\n",
        "\t\treturn self.get_ode_gradient_nn(t_local, y)\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "class ODEFunc_w_Poisson(ODEFunc):\n",
        "\t\n",
        "\tdef __init__(self, input_dim, latent_dim, ode_func_net,\n",
        "\t\tlambda_net, device = torch.device(\"cpu\")):\n",
        "\t\t\"\"\"\n",
        "\t\tinput_dim: dimensionality of the input\n",
        "\t\tlatent_dim: dimensionality used for ODE. Analog of a continous latent state\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(ODEFunc_w_Poisson, self).__init__(input_dim, latent_dim, ode_func_net, device)\n",
        "\n",
        "\t\tself.latent_ode = ODEFunc(input_dim = input_dim, \n",
        "\t\t\tlatent_dim = latent_dim, \n",
        "\t\t\tode_func_net = ode_func_net,\n",
        "\t\t\tdevice = device)\n",
        "\n",
        "\t\tself.latent_dim = latent_dim\n",
        "\t\tself.lambda_net = lambda_net\n",
        "\t\t# The computation of poisson likelihood can become numerically unstable. \n",
        "\t\t#The integral lambda(t) dt can take large values. In fact, it is equal to the expected number of events on the interval [0,T]\n",
        "\t\t#Exponent of lambda can also take large values\n",
        "\t\t# So we divide lambda by the constant and then multiply the integral of lambda by the constant\n",
        "\t\tself.const_for_lambda = torch.Tensor([100.]).to(device)\n",
        "\n",
        "\tdef extract_poisson_rate(self, augmented, final_result = True):\n",
        "\t\ty, log_lambdas, int_lambda = None, None, None\n",
        "\n",
        "\t\tassert(augmented.size(-1) == self.latent_dim + self.input_dim)\t\t\n",
        "\t\tlatent_lam_dim = self.latent_dim // 2\n",
        "\n",
        "\t\tif len(augmented.size()) == 3:\n",
        "\t\t\tint_lambda  = augmented[:,:,-self.input_dim:] \n",
        "\t\t\ty_latent_lam = augmented[:,:,:-self.input_dim]\n",
        "\n",
        "\t\t\tlog_lambdas  = self.lambda_net(y_latent_lam[:,:,-latent_lam_dim:])\n",
        "\t\t\ty = y_latent_lam[:,:,:-latent_lam_dim]\n",
        "\n",
        "\t\telif len(augmented.size()) == 4:\n",
        "\t\t\tint_lambda  = augmented[:,:,:,-self.input_dim:]\n",
        "\t\t\ty_latent_lam = augmented[:,:,:,:-self.input_dim]\n",
        "\n",
        "\t\t\tlog_lambdas  = self.lambda_net(y_latent_lam[:,:,:,-latent_lam_dim:])\n",
        "\t\t\ty = y_latent_lam[:,:,:,:-latent_lam_dim]\n",
        "\n",
        "\t\t# Multiply the intergral over lambda by a constant \n",
        "\t\t# only when we have finished the integral computation (i.e. this is not a call in get_ode_gradient_nn)\n",
        "\t\tif final_result:\n",
        "\t\t\tint_lambda = int_lambda * self.const_for_lambda\n",
        "\t\t\t\n",
        "\t\t# Latents for performing reconstruction (y) have the same size as latent poisson rate (log_lambdas)\n",
        "\t\tassert(y.size(-1) == latent_lam_dim)\n",
        "\n",
        "\t\treturn y, log_lambdas, int_lambda, y_latent_lam\n",
        "\n",
        "\n",
        "\tdef get_ode_gradient_nn(self, t_local, augmented):\n",
        "\t\ty, log_lam, int_lambda, y_latent_lam = self.extract_poisson_rate(augmented, final_result = False)\n",
        "\t\tdydt_dldt = self.latent_ode(t_local, y_latent_lam)\n",
        "\n",
        "\t\tlog_lam = log_lam - torch.log(self.const_for_lambda)\n",
        "\t\treturn torch.cat((dydt_dldt, torch.exp(log_lam)),-1)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98sA9jVlhy_y"
      },
      "source": [
        "## create_latent_ode_model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kv36UrKvhy_y"
      },
      "source": [
        "def create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
        "\tclassif_per_tp = False, n_labels = 1):\n",
        "\n",
        "\tdim = args.latents\n",
        "\tif args.poisson:\n",
        "\t\tlambda_net = utils.create_net(dim, input_dim, \n",
        "\t\t\tn_layers = 1, n_units = args.units, nonlinear = nn.Tanh)\n",
        "\n",
        "\t\t# ODE function produces the gradient for latent state and for poisson rate\n",
        "\t\tode_func_net = utils.create_net(dim * 2, args.latents * 2, \n",
        "\t\t\tn_layers = args.gen_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
        "\n",
        "\t\tgen_ode_func = ODEFunc_w_Poisson(\n",
        "\t\t\tinput_dim = input_dim, \n",
        "\t\t\tlatent_dim = args.latents * 2,\n",
        "\t\t\tode_func_net = ode_func_net,\n",
        "\t\t\tlambda_net = lambda_net,\n",
        "\t\t\tdevice = device).to(device)\n",
        "\telse:\n",
        "\t\tdim = args.latents \n",
        "\t\tode_func_net = utils.create_net(dim, args.latents, \n",
        "\t\t\tn_layers = args.gen_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
        "\n",
        "\t\tgen_ode_func = ODEFunc(\n",
        "\t\t\tinput_dim = input_dim, \n",
        "\t\t\tlatent_dim = args.latents, \n",
        "\t\t\tode_func_net = ode_func_net,\n",
        "\t\t\tdevice = device).to(device)\n",
        "\n",
        "\tz0_diffeq_solver = None\n",
        "\tn_rec_dims = args.rec_dims\n",
        "\tenc_input_dim = int(input_dim) * 2 # we concatenate the mask\n",
        "\tgen_data_dim = input_dim\n",
        "\n",
        "\tz0_dim = args.latents\n",
        "\tif args.poisson:\n",
        "\t\tz0_dim += args.latents # predict the initial poisson rate\n",
        "\n",
        "\tif args.z0_encoder == \"odernn\":\n",
        "\t\tode_func_net = utils.create_net(n_rec_dims, n_rec_dims, \n",
        "\t\t\tn_layers = args.rec_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
        "\n",
        "\t\trec_ode_func = ODEFunc(\n",
        "\t\t\tinput_dim = enc_input_dim, \n",
        "\t\t\tlatent_dim = n_rec_dims,\n",
        "\t\t\tode_func_net = ode_func_net,\n",
        "\t\t\tdevice = device).to(device)\n",
        "\n",
        "\t\tz0_diffeq_solver = DiffeqSolver(enc_input_dim, rec_ode_func, \"euler\", args.latents, \n",
        "\t\t\todeint_rtol = 1e-3, odeint_atol = 1e-4, device = device)\n",
        "\t\t\n",
        "\t\tencoder_z0 = Encoder_z0_ODE_RNN(n_rec_dims, enc_input_dim, z0_diffeq_solver, \n",
        "\t\t\tz0_dim = z0_dim, n_gru_units = args.gru_units, device = device).to(device)\n",
        "\n",
        "\telif args.z0_encoder == \"rnn\":\n",
        "\t\tencoder_z0 = Encoder_z0_RNN(z0_dim, enc_input_dim,\n",
        "\t\t\tlstm_output_size = n_rec_dims, device = device).to(device)\n",
        "\telse:\n",
        "\t\traise Exception(\"Unknown encoder for Latent ODE model: \" + args.z0_encoder)\n",
        "\n",
        "\tdecoder = Decoder(args.latents, gen_data_dim).to(device)\n",
        "\n",
        "\tdiffeq_solver = DiffeqSolver(gen_data_dim, gen_ode_func, 'dopri5', args.latents, \n",
        "\t\todeint_rtol = 1e-3, odeint_atol = 1e-4, device = device)\n",
        "\n",
        "\tmodel = LatentODE(\n",
        "\t\tinput_dim = gen_data_dim, \n",
        "\t\tlatent_dim = args.latents, \n",
        "\t\tencoder_z0 = encoder_z0, \n",
        "\t\tdecoder = decoder, \n",
        "\t\tdiffeq_solver = diffeq_solver, \n",
        "\t\tz0_prior = z0_prior, \n",
        "\t\tdevice = device,\n",
        "\t\tobsrv_std = obsrv_std,\n",
        "\t\tuse_poisson_proc = args.poisson, \n",
        "\t\tuse_binary_classif = args.classif,\n",
        "\t\tlinear_classifier = args.linear_classif,\n",
        "\t\tclassif_per_tp = classif_per_tp,\n",
        "\t\tn_labels = n_labels,\n",
        "\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
        "\t\t).to(device)\n",
        "\n",
        "\treturn model\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzYUKIc9hy_z"
      },
      "source": [
        "if args.latent_ode:\n",
        "    model = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
        "        classif_per_tp = classif_per_tp,\n",
        "        n_labels = n_labels)\n",
        "else:\n",
        "\traise Exception(\"Model not specified\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afWXMKznhy_z"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y99JVbgThy_z",
        "outputId": "aafffb7c-4a53-4118-b142-63853aaf04f9"
      },
      "source": [
        "#Load checkpoint and evaluate the model\n",
        "if args.load is not None:\n",
        "\tutils.get_ckpt_model(ckpt_path, model, device)\n",
        "\texit()\n",
        "else:\n",
        "\tfile_name = os.path.abspath('')\n",
        "\tlog_path = \"logs/\" + file_name + \"_\" + str(experimentID) + \".log\"\n",
        "\tif not os.path.exists(\"logs/\"):\n",
        "\t\tutils.makedirs(\"logs/\")\n",
        "\tlogger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(''))\n",
        "\tlogger.info(input_command)\n",
        "\n",
        "\toptimizer = optim.Adamax(model.parameters(), lr=args.lr)\n",
        "\t# print(f\"optimizer is {optimizer}\")\n",
        "\tnum_batches = data_obj[\"n_train_batches\"]\n",
        "\tprint(f\"num_batches is {num_batches}\")\n",
        "\n",
        "\tfor itr in range(1, num_batches * (args.niters + 1)):\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tutils.update_learning_rate(optimizer, decay_rate = 0.999, lowest = args.lr / 10)\n",
        "\n",
        "\t\twait_until_kl_inc = 10\n",
        "\t\tif itr // num_batches < wait_until_kl_inc:\n",
        "\t\t\tkl_coef = 0.\n",
        "\t\telse:\n",
        "\t\t\tkl_coef = (1-0.99** (itr // num_batches - wait_until_kl_inc))\n",
        "\n",
        "\t\tbatch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
        "\t\ttrain_res = model.compute_all_losses(batch_dict, n_traj_samples = 3, kl_coef = kl_coef)\n",
        "\t\ttrain_res[\"loss\"].backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tn_iters_to_viz = 1\n",
        "\t\tif itr % (n_iters_to_viz * num_batches) == 0:\n",
        "\t\t\twith torch.no_grad():\n",
        "\n",
        "\t\t\t\ttest_res = compute_loss_all_batches(model, \n",
        "\t\t\t\t\tdata_obj[\"test_dataloader\"], args,\n",
        "\t\t\t\t\tn_batches = data_obj[\"n_test_batches\"],\n",
        "\t\t\t\t\texperimentID = experimentID,\n",
        "\t\t\t\t\tdevice = device,\n",
        "\t\t\t\t\tn_traj_samples = 3, kl_coef = kl_coef)\n",
        "\n",
        "\t\t\t\tmessage = 'Epoch {:04d} [Test seq (cond on sampled tp)] | Loss {:.6f} | Likelihood {:.6f} | KL fp {:.4f} | FP STD {:.4f}|'.format(\n",
        "\t\t\t\t\titr//num_batches, \n",
        "\t\t\t\t\ttest_res[\"loss\"].detach(), test_res[\"likelihood\"].detach(), \n",
        "\t\t\t\t\ttest_res[\"kl_first_p\"], test_res[\"std_first_p\"])\n",
        "\t\t\t\n",
        "\t\t\t\tlogger.info(\"Experiment \" + str(experimentID))\n",
        "\t\t\t\tlogger.info(message)\n",
        "\t\t\t\tlogger.info(\"KL coef: {}\".format(kl_coef))\n",
        "\t\t\t\tlogger.info(\"Train loss (one batch): {}\".format(train_res[\"loss\"].detach()))\n",
        "\t\t\t\tlogger.info(\"Train CE loss (one batch): {}\".format(train_res[\"ce_loss\"].detach()))\n",
        "\t\t\t\t\n",
        "\t\t\t\tif \"auc\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Classification AUC (TEST): {:.4f}\".format(test_res[\"auc\"]))\n",
        "\n",
        "\t\t\t\tif \"mse\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Test MSE: {:.4f}\".format(test_res[\"mse\"]))\n",
        "\n",
        "\t\t\t\tif \"accuracy\" in train_res:\n",
        "\t\t\t\t\tlogger.info(\"Classification accuracy (TRAIN): {:.4f}\".format(train_res[\"accuracy\"]))\n",
        "\n",
        "\t\t\t\tif \"accuracy\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Classification accuracy (TEST): {:.4f}\".format(test_res[\"accuracy\"]))\n",
        "\n",
        "\t\t\t\tif \"pois_likelihood\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Poisson likelihood: {}\".format(test_res[\"pois_likelihood\"]))\n",
        "\n",
        "\t\t\t\tif \"ce_loss\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"CE loss: {}\".format(test_res[\"ce_loss\"]))\n",
        "\n",
        "\t\t\ttorch.save({\n",
        "\t\t\t\t'args': args,\n",
        "\t\t\t\t'state_dict': model.state_dict(),\n",
        "\t\t\t}, ckpt_path)\n",
        "\n",
        "\n",
        "\t\t\t# Plotting\n",
        "\t\t\tif args.viz:\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\ttest_dict = utils.get_next_batch(data_obj[\"test_dataloader\"])\n",
        "\n",
        "\t\t\t\t\tprint(\"plotting....\")\n",
        "\t\t\t\t\tif isinstance(model, LatentODE) and (args.dataset == \"periodic\"): #and not args.classic_rnn and not args.ode_rnn:\n",
        "\t\t\t\t\t\tplot_id = itr // num_batches // n_iters_to_viz\n",
        "\t\t\t\t\t\tviz.draw_all_plots_one_dim(test_dict, model, \n",
        "\t\t\t\t\t\t\tplot_name = file_name + \"_\" + str(experimentID) + \"_{:03d}\".format(plot_id) + \".png\",\n",
        "\t\t\t\t\t\t\texperimentID = experimentID, save=True)\n",
        "\t\t\t\t\t\tplt.pause(0.01)\n",
        "\ttorch.save({\n",
        "\t\t'args': args,\n",
        "\t\t'state_dict': model.state_dict(),\n",
        "\t}, ckpt_path)\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content\n",
            "-f\n",
            "/content/torchdiffeq/torchdiffeq/_impl/misc.py:296: UserWarning: t is not on the same device as y0. Coercing to y0.device.\n",
            "  warnings.warn(\"t is not on the same device as y0. Coercing to y0.device.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_batches is 387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0001 [Test seq (cond on sampled tp)] | Loss 3681963264.000000 | Likelihood -3682078720.000000 | KL fp 11.0158 | FP STD 0.8325|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 670596864.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7364.1577\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0002 [Test seq (cond on sampled tp)] | Loss 3663025664.000000 | Likelihood -3663202048.000000 | KL fp 11.8821 | FP STD 0.8586|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 659996928.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7326.4053\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0003 [Test seq (cond on sampled tp)] | Loss 3649664256.000000 | Likelihood -3649736704.000000 | KL fp 12.5012 | FP STD 0.8815|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 651877440.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7299.4746\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0004 [Test seq (cond on sampled tp)] | Loss 3640628992.000000 | Likelihood -3640892928.000000 | KL fp 12.8738 | FP STD 0.8926|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 645745280.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7281.7856\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0005 [Test seq (cond on sampled tp)] | Loss 3634263808.000000 | Likelihood -3634414336.000000 | KL fp 13.1454 | FP STD 0.9072|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 643610752.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7268.8296\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0006 [Test seq (cond on sampled tp)] | Loss 3629772800.000000 | Likelihood -3630304256.000000 | KL fp 13.3619 | FP STD 0.9147|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 640876416.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7260.6099\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0007 [Test seq (cond on sampled tp)] | Loss 3627073280.000000 | Likelihood -3627273216.000000 | KL fp 13.5127 | FP STD 0.9159|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 639052992.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7254.5488\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0008 [Test seq (cond on sampled tp)] | Loss 3623525376.000000 | Likelihood -3623635712.000000 | KL fp 13.6594 | FP STD 0.9222|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 636993344.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7247.2729\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0009 [Test seq (cond on sampled tp)] | Loss 3619952128.000000 | Likelihood -3620290048.000000 | KL fp 13.8451 | FP STD 0.9242|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 634161088.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7240.5811\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0010 [Test seq (cond on sampled tp)] | Loss 3616777472.000000 | Likelihood -3616963328.000000 | KL fp 13.9640 | FP STD 0.9301|\n",
            "KL coef: 0.0\n",
            "Train loss (one batch): 632178944.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7233.9277\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0011 [Test seq (cond on sampled tp)] | Loss 3613647104.000000 | Likelihood -3613906688.000000 | KL fp 14.1250 | FP STD 0.9318|\n",
            "KL coef: 0.010000000000000009\n",
            "Train loss (one batch): 629954752.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7227.8145\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0012 [Test seq (cond on sampled tp)] | Loss 3609885696.000000 | Likelihood -3610214400.000000 | KL fp 14.2158 | FP STD 0.9392|\n",
            "KL coef: 0.01990000000000003\n",
            "Train loss (one batch): 628004864.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7220.4302\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0013 [Test seq (cond on sampled tp)] | Loss 3606681600.000000 | Likelihood -3606852608.000000 | KL fp 14.3391 | FP STD 0.9457|\n",
            "KL coef: 0.029700999999999977\n",
            "Train loss (one batch): 626234240.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7213.7065\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n",
            "Experiment 41822\n",
            "Epoch 0014 [Test seq (cond on sampled tp)] | Loss 3603259136.000000 | Likelihood -3603459840.000000 | KL fp 14.4746 | FP STD 0.9444|\n",
            "KL coef: 0.039403990000000055\n",
            "Train loss (one batch): 623911104.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7206.9209\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0015 [Test seq (cond on sampled tp)] | Loss 3599851008.000000 | Likelihood -3600077824.000000 | KL fp 14.5797 | FP STD 0.9500|\n",
            "KL coef: 0.04900995010000009\n",
            "Train loss (one batch): 622239040.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7200.1562\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0016 [Test seq (cond on sampled tp)] | Loss 3596087296.000000 | Likelihood -3596529408.000000 | KL fp 14.6895 | FP STD 0.9569|\n",
            "KL coef: 0.058519850599\n",
            "Train loss (one batch): 620958464.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7193.0601\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0017 [Test seq (cond on sampled tp)] | Loss 3592680960.000000 | Likelihood -3592986112.000000 | KL fp 14.7794 | FP STD 0.9630|\n",
            "KL coef: 0.06793465209301008\n",
            "Train loss (one batch): 619902272.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7185.9736\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0018 [Test seq (cond on sampled tp)] | Loss 3589200640.000000 | Likelihood -3589560064.000000 | KL fp 14.8626 | FP STD 0.9700|\n",
            "KL coef: 0.07725530557207994\n",
            "Train loss (one batch): 616610240.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7179.1206\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0019 [Test seq (cond on sampled tp)] | Loss 3585750272.000000 | Likelihood -3586255360.000000 | KL fp 14.9199 | FP STD 0.9694|\n",
            "KL coef: 0.08648275251635917\n",
            "Train loss (one batch): 614372160.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7172.5117\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0020 [Test seq (cond on sampled tp)] | Loss 3582498560.000000 | Likelihood -3582708480.000000 | KL fp 14.9174 | FP STD 0.9753|\n",
            "KL coef: 0.09561792499119559\n",
            "Train loss (one batch): 611910720.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7165.4175\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0021 [Test seq (cond on sampled tp)] | Loss 3578501120.000000 | Likelihood -3579003904.000000 | KL fp 15.0623 | FP STD 0.9730|\n",
            "KL coef: 0.10466174574128362\n",
            "Train loss (one batch): 610581248.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7158.0088\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0022 [Test seq (cond on sampled tp)] | Loss 3575824896.000000 | Likelihood -3576018944.000000 | KL fp 15.2325 | FP STD 0.9777|\n",
            "KL coef: 0.11361512828387077\n",
            "Train loss (one batch): 609556032.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7152.0396\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0023 [Test seq (cond on sampled tp)] | Loss 3571960320.000000 | Likelihood -3572214272.000000 | KL fp 15.4116 | FP STD 0.9841|\n",
            "KL coef: 0.12247897700103216\n",
            "Train loss (one batch): 607641856.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7144.4292\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0024 [Test seq (cond on sampled tp)] | Loss 3567986688.000000 | Likelihood -3568133888.000000 | KL fp 15.6103 | FP STD 0.9902|\n",
            "KL coef: 0.13125418723102178\n",
            "Train loss (one batch): 606115008.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7136.2690\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0025 [Test seq (cond on sampled tp)] | Loss 3564208640.000000 | Likelihood -3565080576.000000 | KL fp 15.6955 | FP STD 0.9954|\n",
            "KL coef: 0.13994164535871156\n",
            "Train loss (one batch): 603600832.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7130.1626\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0026 [Test seq (cond on sampled tp)] | Loss 3561169920.000000 | Likelihood -3561464832.000000 | KL fp 15.8565 | FP STD 1.0026|\n",
            "KL coef: 0.14854222890512447\n",
            "Train loss (one batch): 601035264.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7122.9307\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0027 [Test seq (cond on sampled tp)] | Loss 3557386240.000000 | Likelihood -3557677056.000000 | KL fp 16.1697 | FP STD 1.0064|\n",
            "KL coef: 0.15705680661607324\n",
            "Train loss (one batch): 598609152.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7115.3550\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0028 [Test seq (cond on sampled tp)] | Loss 3553688576.000000 | Likelihood -3553874688.000000 | KL fp 16.1069 | FP STD 1.0092|\n",
            "KL coef: 0.1654862385499125\n",
            "Train loss (one batch): 596751296.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7107.7510\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0029 [Test seq (cond on sampled tp)] | Loss 3550348288.000000 | Likelihood -3550713600.000000 | KL fp 16.3342 | FP STD 1.0146|\n",
            "KL coef: 0.17383137616441335\n",
            "Train loss (one batch): 593388800.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7101.4277\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0030 [Test seq (cond on sampled tp)] | Loss 3546214656.000000 | Likelihood -3546433536.000000 | KL fp 16.3734 | FP STD 1.0182|\n",
            "KL coef: 0.18209306240276923\n",
            "Train loss (one batch): 592845312.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7092.8687\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0031 [Test seq (cond on sampled tp)] | Loss 3542241792.000000 | Likelihood -3542675968.000000 | KL fp 16.4205 | FP STD 1.0285|\n",
            "KL coef: 0.19027213177874158\n",
            "Train loss (one batch): 588722496.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7085.3525\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0032 [Test seq (cond on sampled tp)] | Loss 3538930944.000000 | Likelihood -3539453696.000000 | KL fp 16.6253 | FP STD 1.0322|\n",
            "KL coef: 0.19836941046095413\n",
            "Train loss (one batch): 586832512.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7078.9082\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0033 [Test seq (cond on sampled tp)] | Loss 3535380736.000000 | Likelihood -3535875584.000000 | KL fp 16.7337 | FP STD 1.0386|\n",
            "KL coef: 0.20638571635634462\n",
            "Train loss (one batch): 583220992.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7071.7520\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0034 [Test seq (cond on sampled tp)] | Loss 3531689728.000000 | Likelihood -3531995648.000000 | KL fp 16.7734 | FP STD 1.0424|\n",
            "KL coef: 0.21432185919278124\n",
            "Train loss (one batch): 585804544.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7063.9922\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0035 [Test seq (cond on sampled tp)] | Loss 3527125248.000000 | Likelihood -3528091136.000000 | KL fp 16.9582 | FP STD 1.0442|\n",
            "KL coef: 0.22217864060085335\n",
            "Train loss (one batch): 582605440.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7056.1826\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0036 [Test seq (cond on sampled tp)] | Loss 3524112896.000000 | Likelihood -3524417280.000000 | KL fp 17.2889 | FP STD 1.0493|\n",
            "KL coef: 0.2299568541948449\n",
            "Train loss (one batch): 580676288.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7048.8350\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0037 [Test seq (cond on sampled tp)] | Loss 3520279296.000000 | Likelihood -3520617216.000000 | KL fp 17.0807 | FP STD 1.0555|\n",
            "KL coef: 0.23765728565289646\n",
            "Train loss (one batch): 578213952.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7041.2354\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0038 [Test seq (cond on sampled tp)] | Loss 3516041984.000000 | Likelihood -3516489472.000000 | KL fp 17.2084 | FP STD 1.0553|\n",
            "KL coef: 0.24528071279636743\n",
            "Train loss (one batch): 575771904.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7032.9790\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0039 [Test seq (cond on sampled tp)] | Loss 3512581632.000000 | Likelihood -3512772096.000000 | KL fp 17.4044 | FP STD 1.0588|\n",
            "KL coef: 0.25282790566840385\n",
            "Train loss (one batch): 572164608.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7025.5459\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0040 [Test seq (cond on sampled tp)] | Loss 3508937472.000000 | Likelihood -3509154560.000000 | KL fp 17.6333 | FP STD 1.0640|\n",
            "KL coef: 0.2602996266117198\n",
            "Train loss (one batch): 566930880.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7018.3101\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0041 [Test seq (cond on sampled tp)] | Loss 3504936960.000000 | Likelihood -3505362432.000000 | KL fp 17.6743 | FP STD 1.0728|\n",
            "KL coef: 0.26769663034560254\n",
            "Train loss (one batch): 568822272.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7010.7256\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0042 [Test seq (cond on sampled tp)] | Loss 3500677120.000000 | Likelihood -3500878592.000000 | KL fp 17.8596 | FP STD 1.0798|\n",
            "KL coef: 0.2750196640421466\n",
            "Train loss (one batch): 567562880.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 7001.7583\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0043 [Test seq (cond on sampled tp)] | Loss 3497594368.000000 | Likelihood -3497913856.000000 | KL fp 18.1860 | FP STD 1.0891|\n",
            "KL coef: 0.2822694674017251\n",
            "Train loss (one batch): 562894656.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6995.8276\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0044 [Test seq (cond on sampled tp)] | Loss 3493469696.000000 | Likelihood -3493672960.000000 | KL fp 18.3523 | FP STD 1.0940|\n",
            "KL coef: 0.2894467727277079\n",
            "Train loss (one batch): 561178880.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6987.3472\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0045 [Test seq (cond on sampled tp)] | Loss 3489450496.000000 | Likelihood -3489993472.000000 | KL fp 18.4339 | FP STD 1.0997|\n",
            "KL coef: 0.29655230500043084\n",
            "Train loss (one batch): 558899520.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6979.9888\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0046 [Test seq (cond on sampled tp)] | Loss 3485429760.000000 | Likelihood -3485839360.000000 | KL fp 18.7908 | FP STD 1.1001|\n",
            "KL coef: 0.30358678195042654\n",
            "Train loss (one batch): 558465856.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6971.6802\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0047 [Test seq (cond on sampled tp)] | Loss 3481551104.000000 | Likelihood -3481742848.000000 | KL fp 18.9463 | FP STD 1.1014|\n",
            "KL coef: 0.3105509141309223\n",
            "Train loss (one batch): 554128320.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6963.4858\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0048 [Test seq (cond on sampled tp)] | Loss 3477678592.000000 | Likelihood -3477915136.000000 | KL fp 18.9298 | FP STD 1.1096|\n",
            "KL coef: 0.31744540498961304\n",
            "Train loss (one batch): 554436032.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6955.8315\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0049 [Test seq (cond on sampled tp)] | Loss 3473719808.000000 | Likelihood -3473956352.000000 | KL fp 19.1651 | FP STD 1.1145|\n",
            "KL coef: 0.3242709509397169\n",
            "Train loss (one batch): 552483776.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6947.9126\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Experiment 41822\n",
            "Epoch 0050 [Test seq (cond on sampled tp)] | Loss 3469402880.000000 | Likelihood -3470054144.000000 | KL fp 19.2870 | FP STD 1.1189|\n",
            "KL coef: 0.3310282414303197\n",
            "Train loss (one batch): 548460736.0\n",
            "Train CE loss (one batch): 0.0\n",
            "Test MSE: 6940.1089\n",
            "Poisson likelihood: 0.0\n",
            "CE loss: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing loss... 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfq6eEnmA4l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}