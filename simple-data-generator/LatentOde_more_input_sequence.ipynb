{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Install the latest version of author's repo neural ode implementation\n",
    "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
    "!cd torchdiffeq && pip install -e .\n",
    "!ls torchdiffeq/torchdiffeq"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'torchdiffeq'...\n",
      "remote: Enumerating objects: 1121, done.\u001b[K\n",
      "remote: Counting objects: 100% (417/417), done.\u001b[K\n",
      "remote: Compressing objects: 100% (179/179), done.\u001b[K\n",
      "remote: Total 1121 (delta 247), reused 398 (delta 238), pack-reused 704\u001b[K\n",
      "Receiving objects: 100% (1121/1121), 8.29 MiB | 21.27 MiB/s, done.\n",
      "Resolving deltas: 100% (673/673), done.\n",
      "Obtaining file:///content/torchdiffeq\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu102)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
      "Installing collected packages: torchdiffeq\n",
      "  Running setup.py develop for torchdiffeq\n",
      "Successfully installed torchdiffeq-0.2.2\n",
      "_impl  __init__.py\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXgalPX5EJZg",
    "outputId": "be66d376-afaa-4e7c-b5a5-2925ef887b40"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## m1m2m3m4m5m6m7 as input and m1m2m3m4m5m6m7m8m9m10 as output for training."
   ],
   "metadata": {
    "id": "pP2XEQ-6D2O4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from pandas import concat\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "from torchdiffeq.torchdiffeq import odeint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dict_tickers = {\n",
    "    'Apple': 'AAPL',\n",
    "    'Microsoft': 'MSFT',\n",
    "    'Google': 'GOOG',\n",
    "    'Bitcoin': 'BTC-USD',\n",
    "    'Facebook': 'FB',\n",
    "    'Walmart': 'WMT',\n",
    "    'Amazon': 'AMZN',\n",
    "    'CVS': 'CVS',\n",
    "    'Berkshire': 'BRK-B',\n",
    "    'ExxonMobil': 'XOM',\n",
    "    'AtandT': 'T',\n",
    "    'Costco': 'COST',\n",
    "    'Walgreens': 'WBA',\n",
    "    'Kroger': 'KR',\n",
    "    'JPMorgan': 'JPM',\n",
    "    'Verizon': 'VZ',\n",
    "    'FordMotor': 'F',\n",
    "    'GeneralMotors': 'GM',\n",
    "    'Dell': 'DELL',\n",
    "    'BankOfAmerica': 'BAC',\n",
    "    'Target': 'TGT',\n",
    "    'GeneralElectric': 'GE',\n",
    "    'JohnsonandJohnson': 'JNJ',\n",
    "    'Nvidia': 'NVDA',\n",
    "    'Intel': 'INTC',\n",
    "}\n",
    "\n",
    "\n",
    "def stockDataTransformer(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df1 = df[['Open', 'Close']].copy()\n",
    "    data = df1.values\n",
    "    n_samples = data.shape[0]//10*10\n",
    "    reshape_number = n_samples*data.shape[1]//10\n",
    "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
    "    return data1\n",
    "\n",
    "# FileName\n",
    "tickerName = 'ExxonMobil'\n",
    "# Filepath\n",
    "filepath = f\"raw-stock-data/{tickerName}.csv\"\n",
    "# Get the data in the required format\n",
    "data = stockDataTransformer(filepath)\n",
    "print('Data after Data Transformation')\n",
    "# # Total Data Size\n",
    "data_size = data.shape[0]\n",
    "\n",
    "print(pd.DataFrame(data))\n",
    "print('\\n')\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "lag = 7\n",
    "forecast = 3\n",
    "data_orig = series_to_supervised(data, lag, forecast).values\n",
    "print('Data Original after series to supervised on data')\n",
    "print(data_orig.shape)\n",
    "print(pd.DataFrame(data_orig))\n",
    "print('\\n')\n",
    "\n",
    "def get_median(array, axis = 1):\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.median.html\n",
    "    return np.median(array, axis = axis).reshape(data_size, 1)  #, keepdims=True)\n",
    "\n",
    "median_data = get_median(data)\n",
    "print('Median data')\n",
    "# Median data for each week\n",
    "print(median_data.shape)\n",
    "print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
    "print('\\n')\n",
    "\n",
    "lag = 7\n",
    "forecast = 3\n",
    "# Convert median_data to (n_samples, 5) matrix\n",
    "data_m1 = series_to_supervised(median_data, lag, forecast).values\n",
    "print('Median data after series to supervised')\n",
    "print(data_m1.shape)\n",
    "print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n",
    "print('\\n')\n",
    "\n",
    "def split_data(perc_train, perc_valid, lag, data_orig, data_m1, n_features_orig, n_features_median):\n",
    "    values = data_m1\n",
    "    \n",
    "    sizeOfReframed = len(data_m1)\n",
    "    len_train = int(perc_train*sizeOfReframed) # int(sizeOfReframed - len_test) # - len_valid)\n",
    "    train_data_orig = data_orig[:len_train, :]\n",
    "    # valid = values[len_train:len_valid+len_train, :]\n",
    "    test_data_orig = data_orig[len_train:, :]  # [len_valid+len_train:, :]\n",
    "    # n_features = n_features\n",
    "    \n",
    "    train_data_ml = values[:len_train, :]\n",
    "    test_data_ml = values[len_train:, :] \n",
    "    # split into input and outputs\n",
    "    n_obs = lag * n_features_orig\n",
    "    n_obs_median = (lag+forecast) * n_features_median\n",
    "    train_X, train_y = train_data_orig[:, :n_obs], train_data_ml[:, :n_obs_median]\n",
    "    test_X, test_y = test_data_orig[:, :n_obs], test_data_ml[:, :n_obs_median]\n",
    "    # valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
    "    print(train_X.shape, len(train_X), train_y.shape)\n",
    "    \n",
    "    # reshape input to be 3D [samples, features, lag]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_features_orig, lag))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_features_orig, lag))\n",
    "    # valid_X = valid_X.reshape((valid_X.shape[0], lag, n_features))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # , valid_X.shape, valid_y.shape)\n",
    "    \n",
    "    dataload = {\n",
    "        'train_data_orig': train_data_orig,\n",
    "        'test_data_orig': test_data_orig,\n",
    "        'train_data_ml': train_data_ml,\n",
    "        'test_data_ml': test_data_ml,\n",
    "        # 'valid': valid,\n",
    "        'train_X': train_X,\n",
    "        'train_y': train_y,\n",
    "        'test_X': test_X,\n",
    "        'test_y': test_y,\n",
    "        'n_features_orig': n_features_orig,\n",
    "        'n_features_median': n_features_median,\n",
    "        'n_obs': n_obs,\n",
    "        'n_obs_median': n_obs_median,\n",
    "        # 'valid_X': valid_X,\n",
    "        # 'valid_y': valid_y\n",
    "    }\n",
    "    \n",
    "    return dataload\n",
    "\n",
    "dataload = split_data(0.8, 0, lag, data_orig, data_m1, data.shape[1], 1)\n",
    "\n",
    "print('Get Train and Test data')\n",
    "train_X = torch.from_numpy(dataload['train_X']).to(device)\n",
    "print(f\"train_X shape: {train_X.shape}\")  # (#training, 1, 5)\n",
    "\n",
    "train_y = torch.from_numpy(dataload['train_y']).to(device)\n",
    "print(f\"train_y shape: {train_y.shape}\")  # (#training, 5)\n",
    "train_y = torch.reshape(train_y, (train_X.shape[0], 1, train_y.shape[1])).to(device)\n",
    "print(f\"train_y shape: {train_y.shape}\")  # (#training, 1, 5)\n",
    "test_X = torch.from_numpy(dataload['test_X']).to(device)\n",
    "print(f\"test_X.shape : {test_X.shape}\")  # (#testing, 1, 5)\n",
    "test_y = torch.from_numpy(dataload['test_y']).to(device)\n",
    "print(f\"test_y.shape : {test_y.shape}\")  # (#testing, 5)\n",
    "test_y = torch.reshape(test_y, (test_X.shape[0], 1, test_y.shape[1])).to(device)\n",
    "print(f\"test_y.shape : {test_y.shape}\")\n",
    "print('\\n')\n",
    "# https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "class LatentODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, nhidden=20):\n",
    "        super(LatentODEfunc, self).__init__()\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, nhidden)\n",
    "        self.fc3 = nn.Linear(nhidden, latent_dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.fc1(x).to(device)\n",
    "        out = self.elu(out).to(device)\n",
    "        out = self.fc2(out).to(device)\n",
    "        out = self.elu(out).to(device)\n",
    "        out = self.fc3(out).to(device)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RecognitionRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=25, nbatch=1):\n",
    "        super(RecognitionRNN, self).__init__()\n",
    "        self.nhidden = nhidden\n",
    "        self.nbatch = nbatch\n",
    "        self.i2h = nn.Linear(obs_dim + nhidden, nhidden)\n",
    "        self.i2h = self.i2h.float()\n",
    "        self.h2o = nn.Linear(nhidden, latent_dim * 2)\n",
    "        self.h2o = self.h2o.float()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        combined = torch.cat((x, h), dim=1).to(device)\n",
    "        h = torch.tanh(self.i2h(combined.float())).to(device)\n",
    "        out = self.h2o(h).to(device)\n",
    "        return out, h\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.nbatch, self.nhidden)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=20):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, obs_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc1(z).to(device)\n",
    "        out = self.relu(out).to(device)\n",
    "        out = self.fc2(out).to(device)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val\n",
    "\n",
    "\n",
    "def log_normal_pdf(x, mean, logvar):\n",
    "    const = torch.from_numpy(np.array([2. * np.pi])).float().to(x.device).to(device)\n",
    "    const = torch.log(const).to(device)\n",
    "    return -.5 * (const + logvar + (x - mean) ** 2. / torch.exp(logvar))\n",
    "\n",
    "def normal_kl(mu1, lv1, mu2, lv2):\n",
    "    v1 = torch.exp(lv1)\n",
    "    v2 = torch.exp(lv2)\n",
    "    lstd1 = lv1 / 2.\n",
    "    lstd2 = lv2 / 2.\n",
    "\n",
    "    kl = lstd2 - lstd1 + ((v1 + (mu1 - mu2) ** 2.) / (2. * v2)) - .5\n",
    "    return kl.to(device)\n",
    "\n",
    "def train(loss_str, niters):\n",
    "    loss_list = []\n",
    "    for itr in range(1, niters + 1):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # backward in time to infer q(z_0)\n",
    "        h = rec.initHidden().to(device)  # (# nbatches_train, rnn_hidden)\n",
    "        for t_r in reversed(range(train_X.shape[2])):  # input_dimension\n",
    "            obs = train_X[:, :, t_r].to(device)\n",
    "            # obs = torch.reshape(obs, (1, 1)).to(device)\n",
    "            out, h = rec.forward(obs, h)\n",
    "        qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
    "        epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "        z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "        # forward in time and solve ode for reconstructions\n",
    "        pred_z = odeint(func, z0, t.to(device)).permute(1, 0, 2)  # [:, -1, :]\n",
    "        pred_x = dec(pred_z).to(device)\n",
    "        pred_x = torch.reshape(pred_x, (train_X.shape[0], lag+forecast)).to(device)\n",
    "\n",
    "        # compute loss\n",
    "        if loss_str == 'mse':\n",
    "            loss = torch.nn.MSELoss()(pred_x.float(), train_X[i, :, :].float().to(device)).float()\n",
    "        elif loss_str == 'elbo':\n",
    "            noise_std_ = torch.zeros(pred_x.size()).to(device) + noise_std\n",
    "            noise_logvar = 2. * torch.log(noise_std_).to(device)\n",
    "            logpx = log_normal_pdf(\n",
    "                train_y[:, 0, :].to(device), pred_x, noise_logvar).sum(-1)  # .sum(-1)\n",
    "            pz0_mean = pz0_logvar = torch.zeros(z0.size()).to(device)\n",
    "            analytic_kl = normal_kl(qz0_mean, qz0_logvar,\n",
    "                                    pz0_mean, pz0_logvar).sum(-1)\n",
    "            loss = torch.mean(-logpx + analytic_kl, dim=0).to(device)\n",
    "            # loss = torch.reshape(loss, (1, 1)).to(device)\n",
    "        loss_list.append(loss)\n",
    "            # loss_ = torch.mean(torch.cat([x.float() for x in loss_list])).to(device)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss_meter.update(loss.item())\n",
    "        if itr%10==0:\n",
    "            print('Iter: {}, running: {:.4f}'.format(itr, loss.item()))\n",
    "    return loss_list\n",
    "\n",
    "def train_loss(h):\n",
    "    train_loss = 0.0\n",
    "    predictions = []\n",
    "    \n",
    "    for t_r in reversed(range(train_X.shape[2])):\n",
    "        obs = train_X[:, :, t_r].to(device)\n",
    "        # obs = torch.reshape(obs, (1, 1)).to(device)\n",
    "        out, h = rec.forward(obs, h)\n",
    "\n",
    "    qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
    "    epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "    z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "    # forward in time and solve ode for reconstructions\n",
    "    pred_z = odeint(func, z0, t.to(device)).permute(1, 0, 2)\n",
    "    pred_x = dec(pred_z).to(device)\n",
    "    pred_x = torch.reshape(pred_x, (pred_x.shape[0], 1, lag+forecast))\n",
    "    pred_x_recon = \n",
    "    pred_x_extrapol = \n",
    "    rmse = RMSELoss()\n",
    "    # loss = torch.nn.MSELoss()(pred_x, train_X[:, :, :].to(device))\n",
    "    loss_recon = rmse(pred_x_recon, train_y_recon)\n",
    "    loss_extrapol = rmse(pred_x_extrapol, train_y_extrapol)\n",
    "    # train_loss += loss\n",
    "    # predictions.append(pred_x)\n",
    "    \n",
    "    # train_loss = torch.sqrt(train_loss)\n",
    "    # train_pred = torch.cat([x.float() for x in predictions])\n",
    "    # train_pred = torch.reshape(train_pred, (train_pred.shape[0], 1, lag))\n",
    "    with torch.no_grad():\n",
    "        print('Reconstruction Loss')\n",
    "        print('Total Train Loss {:.6f}'.format(loss_recon.item()))\n",
    "        print('Extrapolation Loss')\n",
    "        print('Total Train Extrapolation Loss {:.6f}'.format(loss_extrapol.item()))\n",
    "    return pred_x, pred_x_recon, pred_x_extrapol\n",
    "\n",
    "def test_loss(h, t_test):\n",
    "    # print(h.shape)\n",
    "    test_loss = 0.0\n",
    "    predictions = []\n",
    "    rmse = RMSELoss()\n",
    "    for t_r in reversed(range(test_y.size(2))):\n",
    "        obs = test_X[:, :, t_r].to(device)\n",
    "        # obs = torch.reshape(obs, (1, 1)).to(device)\n",
    "        out, h = rec.forward(obs, h)\n",
    "    qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
    "    epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "    z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "    # forward in time and solve ode for reconstructions\n",
    "    pred_z = odeint(func, z0, t_test).permute(1, 0, 2).to(device)\n",
    "    pred_x = dec(pred_z).to(device)\n",
    "    pred_x = torch.reshape(pred_x, (pred_x.shape[0], 1, lag+forecast))\n",
    "    pred_test_X = pred_x[:, :, :lag]\n",
    "    pred_test_y = pred_x[:, :, lag:]\n",
    "    # pred_test_X = torch.reshape(pred_test_X, (pred_test_X.shape[0], pred_test_X.shape[1]))\n",
    "    # pred_test_y = torch.reshape(pred_test_y, (1, pred_test_y.shape[0], 1, lag))\n",
    "    # print(test_pred_y.shape)\n",
    "    rmse = RMSELoss()\n",
    "    # loss = torch.nn.MSELoss()(pred_x, train_X[i, :, :].to(device))\n",
    "    loss_test_X = rmse(pred_test_X, test_X.to(device))\n",
    "    loss_test_y = rmse(pred_test_y, test_y)\n",
    "\n",
    "    # loss = torch.nn.MSELoss()(pred_x, torch.reshape(test_X[i, :, :].to(device), (1, 5))).to(device)\n",
    "    # test_loss += loss\n",
    "    # predictions.append(pred_x)\n",
    "    \n",
    "    # test_loss = torch.sqrt(test_loss)\n",
    "    # test_pred = torch.cat([x.float() for x in predictions])\n",
    "    \n",
    "    # loss = torch.nn.MSELoss()(test_pred_y[train_size-test_size:, batch_time2-2, :], label_batch_y[train_size-test_size:, batch_time2-2, :])\n",
    "    with torch.no_grad():\n",
    "        print('Total Loss on test X is {:.6f}'.format(loss_test_X.item()))\n",
    "        print('Total Loss on test y is {:.6f}'.format(loss_test_y.item()))\n",
    "    return pred_test_X, pred_test_y\n",
    "\n",
    "def plot_train(i, train_pred):\n",
    "    t_ = torch.linspace(1., train_y.shape[0], train_y.shape[0])\n",
    "    plt.figure()\n",
    "    plt.plot(t_.numpy(), train_y.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
    "    with torch.no_grad():\n",
    "        rmse = np.sqrt(((train_y.cpu().numpy()[:, :, i-1] - train_pred.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
    "        plt.plot(t_.numpy(), train_pred.cpu().numpy()[:, :, i-1], '--', label = f\"pred_week{i}\")\n",
    "    plt.title(f\"Trial No. {trial}: Train: Apple's Median Stock price for week{i}: RMSE {rmse}\")\n",
    "    plt.legend(framealpha=1, frameon=True);\n",
    "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Train: Apple's Median Stock price for week{i}-all.pdf\", dpi = 150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_test(i, test_pred):\n",
    "    t_ = torch.linspace(1., test_X.shape[0], test_X.shape[0])\n",
    "    plt.figure()\n",
    "    plt.plot(t_.numpy(), test_y.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
    "    with torch.no_grad():\n",
    "        rmse = np.sqrt(((test_y.cpu().numpy()[:, :, i-1] - test_pred.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
    "        plt.plot(t_.numpy(), test_pred.cpu().numpy()[:, :, i-1], '--', label = f\"pred_week{i+lag} : RMSE {rmse}\")\n",
    "    plt.title(f\"Trial No. {trial}: Test_X: Apple's Median Stock price for week{i}: RMSE {rmse}\")\n",
    "    plt.legend(framealpha=1, frameon=True);\n",
    "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Test_X: Apple's Median Stock price for week{i}.pdf\", dpi = 150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_test_y(i):\n",
    "    t_ = torch.linspace(1., test_y.shape[0], test_y.shape[0])\n",
    "    plt.figure()\n",
    "    plt.plot(t_.numpy(), test_y.cpu().numpy()[:, :, i-1].reshape(test_y.shape[0]), 'g', label = f\"orig_week{i+lag}\")\n",
    "    with torch.no_grad():\n",
    "        rmse = np.sqrt(((test_y.cpu().numpy()[:, :, i-1] - pred_test_y.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
    "        plt.plot(t_.numpy(), pred_test_y.cpu().numpy()[:, :, i-1].reshape(test_y.shape[0]), '--', label = f\"pred_week{i+lag}\")\n",
    "    plt.title(f\"Trial No. {trial}: Test_y: Apple's Median Stock price for week{i+lag}: RMSE {rmse}\")\n",
    "    plt.legend(framealpha=1, frameon=True);\n",
    "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Test_y: Apple's Median Stock price for week{i+lag}.pdf\", dpi = 150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(loss_list):\n",
    "    plt.plot(loss_list)\n",
    "    plt.title('Train Loss')\n",
    "    plt.ylabel('RMSE Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.savefig(f\"plots-latentode/Trial No. {trial}: RMSE Train Loss.pdf\", dpi = 150)\n",
    "    plt.show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data after Data Transformation\n",
      "              0          1          2  ...          7          8          9\n",
      "0      0.178288   0.179010   0.179010  ...   0.180453   0.180453   0.180814\n",
      "1      0.180814   0.181536   0.181536  ...   0.179731   0.179731   0.177927\n",
      "2      0.177927   0.178649   0.178649  ...   0.177566   0.177566   0.176483\n",
      "3      0.176483   0.173957   0.173957  ...   0.167821   0.166378   0.164212\n",
      "4      0.163852   0.162769   0.162769  ...   0.164573   0.164573   0.164212\n",
      "...         ...        ...        ...  ...        ...        ...        ...\n",
      "2549  41.166637  40.800957  40.508416  ...  38.853703  39.447934  40.819241\n",
      "2550  40.362144  39.868473  39.676488  ...  40.289005  40.142736  39.813622\n",
      "2551  39.959889  40.252434  39.429652  ...  38.625149  38.707430  39.740482\n",
      "2552  40.362140  40.087875  39.676486  ...  40.691257  41.596317  41.111790\n",
      "2553  41.447394  41.102467  40.552445  ...  39.750713  39.704109  39.554951\n",
      "\n",
      "[2554 rows x 10 columns]\n",
      "\n",
      "\n",
      "Data Original after series to supervised on data\n",
      "(2545, 100)\n",
      "             0          1          2   ...         97         98         99\n",
      "0      0.178288   0.179010   0.179010  ...   0.157716   0.157716   0.157716\n",
      "1      0.180814   0.181536   0.181536  ...   0.158799   0.158799   0.159521\n",
      "2      0.177927   0.178649   0.178649  ...   0.165656   0.165656   0.166739\n",
      "3      0.176483   0.173957   0.173957  ...   0.166739   0.166378   0.165656\n",
      "4      0.163852   0.162769   0.162769  ...   0.161686   0.161686   0.161325\n",
      "...         ...        ...        ...  ...        ...        ...        ...\n",
      "2540  41.352184  41.020363  41.523178  ...  38.853703  39.447934  40.819241\n",
      "2541  40.590692  41.450043  41.642029  ...  40.289005  40.142736  39.813622\n",
      "2542  41.815729  41.971142  42.656795  ...  38.625149  38.707430  39.740482\n",
      "2543  42.839633  43.251026  43.881827  ...  40.691257  41.596317  41.111790\n",
      "2544  48.343151  48.928242  48.909956  ...  39.750713  39.704109  39.554951\n",
      "\n",
      "[2545 rows x 100 columns]\n",
      "\n",
      "\n",
      "Median data\n",
      "(2554, 1)\n",
      "   median_stockprice_week\n",
      "0                0.180634\n",
      "1                0.179912\n",
      "2                0.177927\n",
      "3                0.173416\n",
      "4                0.164032\n",
      "5                0.156814\n",
      "6                0.148332\n",
      "7                0.158438\n",
      "8                0.154468\n",
      "9                0.157355\n",
      "\n",
      "\n",
      "Median data after series to supervised\n",
      "(2545, 10)\n",
      "       week i+1   week i+2   week i+3  ...   week i+8   week i+9  week i+10\n",
      "0      0.180634   0.179912   0.177927  ...   0.158438   0.154468   0.157355\n",
      "1      0.179912   0.177927   0.173416  ...   0.154468   0.157355   0.158257\n",
      "2      0.177927   0.173416   0.164032  ...   0.157355   0.158257   0.163671\n",
      "3      0.173416   0.164032   0.156814  ...   0.158257   0.163671   0.166017\n",
      "4      0.164032   0.156814   0.148332  ...   0.163671   0.166017   0.162408\n",
      "...         ...        ...        ...  ...        ...        ...        ...\n",
      "2540  39.365658  40.782675  41.893435  ...  40.439850  39.109681  40.494702\n",
      "2541  40.782675  41.893435  44.951447  ...  39.109681  40.494702  39.909611\n",
      "2542  41.893435  44.951447  43.685277  ...  40.494702  39.909611  38.666289\n",
      "2543  44.951447  43.685277  42.761927  ...  39.909611  38.666289  40.151875\n",
      "2544  43.685277  42.761927  40.855813  ...  38.666289  40.151875  40.170223\n",
      "\n",
      "[2545 rows x 10 columns]\n",
      "\n",
      "\n",
      "(2036, 70) 2036 (2036, 10)\n",
      "(2036, 10, 7) (2036, 10) (509, 10, 7) (509, 10)\n",
      "Get Train and Test data\n",
      "train_X shape: torch.Size([2036, 10, 7])\n",
      "train_y shape: torch.Size([2036, 10])\n",
      "train_y shape: torch.Size([2036, 1, 10])\n",
      "test_X.shape : torch.Size([509, 10, 7])\n",
      "test_y.shape : torch.Size([509, 10])\n",
      "test_y.shape : torch.Size([509, 1, 10])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TE0sTKyZjp0V",
    "outputId": "d42d2827-b021-4642-c6a1-b8bf66c60424"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "trial = 1\n",
    "# predictions on test_data's time size (test_size-batch_time, 1)\n",
    "# Time steps\n",
    "t = torch.linspace(0, lag+forecast-1, lag+forecast)\n",
    "print(t.shape)\n",
    "\n",
    "latent_dim = 4\n",
    "nhidden = 20\n",
    "rnn_nhidden = 25\n",
    "obs_dim = 1\n",
    "out_dim = 1\n",
    "noise_std = .3\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "koK-YkO83BxX",
    "outputId": "9fb82e3f-8013-4b0d-dd6f-b090a41de233"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Training\n",
    "# model\n",
    "nbatches_train = train_X.shape[0]\n",
    "func = LatentODEfunc(latent_dim, nhidden).to(device)\n",
    "rec = RecognitionRNN(latent_dim, obs_dim, rnn_nhidden, nbatches_train).to(device)\n",
    "dec = Decoder(latent_dim, out_dim, nhidden).to(device)\n",
    "params = (list(func.parameters()) + list(dec.parameters()) + list(rec.parameters()))\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(params, lr=lr)\n",
    "loss_str = 'elbo'\n",
    "# all_values = True\n",
    "niters = 2000  # training epochs\n",
    "# loss_list = train(loss_str, niters)\n",
    "\n",
    "# # https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html\n",
    "# torch.save(func.state_dict(), f\"model-latentode/trial{trial}-func-moresequences.pth\")\n",
    "# torch.save(rec.state_dict(), f\"model-latentode/trial{trial}-rec-moresequences.pth\")\n",
    "# torch.save(dec.state_dict(), f\"model-latentode/trial{trial}-dec-moresequences.pth\")\n",
    "\n",
    "func = LatentODEfunc(latent_dim, nhidden).to(device)\n",
    "rec = RecognitionRNN(latent_dim, obs_dim, rnn_nhidden, nbatches_train).to(device)\n",
    "dec = Decoder(latent_dim, out_dim, nhidden).to(device)\n",
    "\n",
    "func.load_state_dict(torch.load(f\"model-latentode/trial{trial}-func-moresequences.pth\"))\n",
    "rec.load_state_dict(torch.load(f\"model-latentode/trial{trial}-rec-moresequences.pth\"))\n",
    "dec.load_state_dict(torch.load(f\"model-latentode/trial{trial}-dec-moresequences.pth\"))"
   ],
   "outputs": [],
   "metadata": {
    "id": "seActaQN6BLf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "1LZ61VIQ6B78"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "WjAQwM7a6B_E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "G66momzc6CBL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rough"
   ],
   "metadata": {
    "id": "fMbezLnFOgdE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from pandas import concat\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "from torchdiffeq.torchdiffeq import odeint\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dict_tickers = {\n",
    "    'Apple': 'AAPL',\n",
    "    'Microsoft': 'MSFT',\n",
    "    'Google': 'GOOG',\n",
    "    'Bitcoin': 'BTC-USD',\n",
    "    'Facebook': 'FB',\n",
    "    'Walmart': 'WMT',\n",
    "    'Amazon': 'AMZN',\n",
    "    'CVS': 'CVS',\n",
    "    'Berkshire': 'BRK-B',\n",
    "    'ExxonMobil': 'XOM',\n",
    "    'AtandT': 'T',\n",
    "    'Costco': 'COST',\n",
    "    'Walgreens': 'WBA',\n",
    "    'Kroger': 'KR',\n",
    "    'JPMorgan': 'JPM',\n",
    "    'Verizon': 'VZ',\n",
    "    'FordMotor': 'F',\n",
    "    'GeneralMotors': 'GM',\n",
    "    'Dell': 'DELL',\n",
    "    'BankOfAmerica': 'BAC',\n",
    "    'Target': 'TGT',\n",
    "    'GeneralElectric': 'GE',\n",
    "    'JohnsonandJohnson': 'JNJ',\n",
    "    'Nvidia': 'NVDA',\n",
    "    'Intel': 'INTC',\n",
    "}\n",
    "\n",
    "# FileName\n",
    "tickerName = 'ExxonMobil'\n",
    "# Filepath\n",
    "filepath = f\"raw-stock-data/{tickerName}.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "df.set_index('Date', inplace=True)\n",
    "df1 = df[['Open', 'Close']].copy()\n",
    "data = df1.values \n",
    "n_samples = data.shape[0]//10*10\n",
    "print(n_samples)\n",
    "reshape_number = n_samples*data.shape[1]//10\n",
    "print(reshape_number)\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "lag = 7\n",
    "forecast = 3\n",
    "data_orig = series_to_supervised(data, lag, forecast).values\n",
    "\n",
    "print(data_orig.shape)\n",
    "pd.DataFrame(data_orig)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "12770\n",
      "2554\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXQO-il_Dwob",
    "outputId": "105fc7fb-7f9b-4ad6-be76-3cfb1bf22c2c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "data[:n_samples].reshape((reshape_number, 10))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.17828774,  0.17900951,  0.17900955, ...,  0.18045305,\n",
       "         0.18045325,  0.18081413],\n",
       "       [ 0.18081397,  0.18153583,  0.18153592, ...,  0.17973146,\n",
       "         0.17973144,  0.17792684],\n",
       "       [ 0.17792687,  0.17864873,  0.17864873, ...,  0.1775659 ,\n",
       "         0.17756587,  0.17648312],\n",
       "       ...,\n",
       "       [39.95988854, 40.25243378, 39.42965186, ..., 38.62514877,\n",
       "        38.70742997, 39.74048233],\n",
       "       [40.36213957, 40.08787537, 39.67648592, ..., 40.69125748,\n",
       "        41.59631712, 41.1117897 ],\n",
       "       [41.44739443, 41.10246658, 40.55244513, ..., 39.75071335,\n",
       "        39.704109  , 39.55495071]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43pyrY7aVVpp",
    "outputId": "719e8fc7-ae6a-46eb-f522-71ee090d52ea"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def stockDataTransformer(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df1 = df[['Open', 'Close']].copy()\n",
    "    data = df1.values\n",
    "    n_samples = data.shape[0]//10*10\n",
    "    reshape_number = n_samples*data.shape[1]//10\n",
    "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
    "    return data1\n",
    "\n",
    "# FileName\n",
    "tickerName = 'ExxonMobil'\n",
    "# Filepath\n",
    "filepath = f\"raw-stock-data/{tickerName}.csv\"\n",
    "# Get the data in the required format\n",
    "data = stockDataTransformer(filepath)\n",
    "\n",
    "# # Total Data Size\n",
    "data_size = data.shape[0]\n",
    "\n",
    "print(pd.DataFrame(data))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              0          1          2  ...          7          8          9\n",
      "0      0.178288   0.179010   0.179010  ...   0.180453   0.180453   0.180814\n",
      "1      0.180814   0.181536   0.181536  ...   0.179731   0.179731   0.177927\n",
      "2      0.177927   0.178649   0.178649  ...   0.177566   0.177566   0.176483\n",
      "3      0.176483   0.173957   0.173957  ...   0.167821   0.166378   0.164212\n",
      "4      0.163852   0.162769   0.162769  ...   0.164573   0.164573   0.164212\n",
      "...         ...        ...        ...  ...        ...        ...        ...\n",
      "2549  41.166637  40.800957  40.508416  ...  38.853703  39.447934  40.819241\n",
      "2550  40.362144  39.868473  39.676488  ...  40.289005  40.142736  39.813622\n",
      "2551  39.959889  40.252434  39.429652  ...  38.625149  38.707430  39.740482\n",
      "2552  40.362140  40.087875  39.676486  ...  40.691257  41.596317  41.111790\n",
      "2553  41.447394  41.102467  40.552445  ...  39.750713  39.704109  39.554951\n",
      "\n",
      "[2554 rows x 10 columns]\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIX7RphBUmcz",
    "outputId": "fc6d6c6f-5103-4f5a-fa6d-3e35c04ee530"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "lag = 7\n",
    "forecast = 3\n",
    "data_orig = series_to_supervised(data, lag, forecast).values\n",
    "\n",
    "print(data_orig.shape)\n",
    "pd.DataFrame(data_orig)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2545, 100)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178288</td>\n",
       "      <td>0.179010</td>\n",
       "      <td>0.179010</td>\n",
       "      <td>0.181897</td>\n",
       "      <td>0.181897</td>\n",
       "      <td>0.181536</td>\n",
       "      <td>0.181536</td>\n",
       "      <td>0.180453</td>\n",
       "      <td>0.180453</td>\n",
       "      <td>0.180814</td>\n",
       "      <td>0.180814</td>\n",
       "      <td>0.181536</td>\n",
       "      <td>0.181536</td>\n",
       "      <td>0.180092</td>\n",
       "      <td>0.180092</td>\n",
       "      <td>0.179371</td>\n",
       "      <td>0.179370</td>\n",
       "      <td>0.179731</td>\n",
       "      <td>0.179731</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177566</td>\n",
       "      <td>0.177566</td>\n",
       "      <td>0.176483</td>\n",
       "      <td>0.176483</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.174318</td>\n",
       "      <td>0.173596</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.167821</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150859</td>\n",
       "      <td>0.150137</td>\n",
       "      <td>0.146528</td>\n",
       "      <td>0.145445</td>\n",
       "      <td>0.145445</td>\n",
       "      <td>0.144723</td>\n",
       "      <td>0.146528</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.155190</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154107</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.155551</td>\n",
       "      <td>0.156994</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.180814</td>\n",
       "      <td>0.181536</td>\n",
       "      <td>0.181536</td>\n",
       "      <td>0.180092</td>\n",
       "      <td>0.180092</td>\n",
       "      <td>0.179371</td>\n",
       "      <td>0.179370</td>\n",
       "      <td>0.179731</td>\n",
       "      <td>0.179731</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177566</td>\n",
       "      <td>0.177566</td>\n",
       "      <td>0.176483</td>\n",
       "      <td>0.176483</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.174318</td>\n",
       "      <td>0.173596</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.167821</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>0.163852</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.155190</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154107</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.155551</td>\n",
       "      <td>0.156994</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.159521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.178649</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177927</td>\n",
       "      <td>0.177566</td>\n",
       "      <td>0.177566</td>\n",
       "      <td>0.176483</td>\n",
       "      <td>0.176483</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.174318</td>\n",
       "      <td>0.173596</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.167821</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>0.163852</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>0.162047</td>\n",
       "      <td>0.160242</td>\n",
       "      <td>0.160242</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.150859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>0.155190</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.154107</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.155551</td>\n",
       "      <td>0.156994</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.162047</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.166739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.176483</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.173957</td>\n",
       "      <td>0.174318</td>\n",
       "      <td>0.173596</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.173235</td>\n",
       "      <td>0.167821</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>0.163852</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>0.162047</td>\n",
       "      <td>0.160242</td>\n",
       "      <td>0.160242</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.150859</td>\n",
       "      <td>0.150859</td>\n",
       "      <td>0.150137</td>\n",
       "      <td>0.146528</td>\n",
       "      <td>0.145445</td>\n",
       "      <td>0.145445</td>\n",
       "      <td>0.144723</td>\n",
       "      <td>0.146528</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154107</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.155551</td>\n",
       "      <td>0.156994</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.162047</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.166739</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166739</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.165656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.163852</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.162769</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164934</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164573</td>\n",
       "      <td>0.164212</td>\n",
       "      <td>0.162047</td>\n",
       "      <td>0.160242</td>\n",
       "      <td>0.160242</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.153746</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154468</td>\n",
       "      <td>0.150859</td>\n",
       "      <td>0.150859</td>\n",
       "      <td>0.150137</td>\n",
       "      <td>0.146528</td>\n",
       "      <td>0.145445</td>\n",
       "      <td>0.145445</td>\n",
       "      <td>0.144723</td>\n",
       "      <td>0.146528</td>\n",
       "      <td>0.152663</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.154829</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.158438</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.156272</td>\n",
       "      <td>0.155912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157716</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.157355</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.158799</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.159160</td>\n",
       "      <td>0.159521</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.162047</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.166739</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.165295</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166017</td>\n",
       "      <td>0.166739</td>\n",
       "      <td>0.166378</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.165656</td>\n",
       "      <td>0.162408</td>\n",
       "      <td>0.162408</td>\n",
       "      <td>0.162408</td>\n",
       "      <td>0.162408</td>\n",
       "      <td>0.163130</td>\n",
       "      <td>0.163130</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.161686</td>\n",
       "      <td>0.161325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>41.352184</td>\n",
       "      <td>41.020363</td>\n",
       "      <td>41.523178</td>\n",
       "      <td>40.334713</td>\n",
       "      <td>39.987315</td>\n",
       "      <td>38.332603</td>\n",
       "      <td>37.546389</td>\n",
       "      <td>38.670860</td>\n",
       "      <td>38.744001</td>\n",
       "      <td>38.396603</td>\n",
       "      <td>40.590692</td>\n",
       "      <td>41.450043</td>\n",
       "      <td>41.642029</td>\n",
       "      <td>40.170158</td>\n",
       "      <td>40.791817</td>\n",
       "      <td>41.477470</td>\n",
       "      <td>41.541463</td>\n",
       "      <td>40.736965</td>\n",
       "      <td>40.270721</td>\n",
       "      <td>40.773533</td>\n",
       "      <td>41.815729</td>\n",
       "      <td>41.971142</td>\n",
       "      <td>42.656795</td>\n",
       "      <td>42.272831</td>\n",
       "      <td>42.446529</td>\n",
       "      <td>41.175785</td>\n",
       "      <td>40.974651</td>\n",
       "      <td>41.568886</td>\n",
       "      <td>41.431755</td>\n",
       "      <td>42.309391</td>\n",
       "      <td>42.839633</td>\n",
       "      <td>43.251026</td>\n",
       "      <td>43.881827</td>\n",
       "      <td>45.015442</td>\n",
       "      <td>44.686329</td>\n",
       "      <td>44.887451</td>\n",
       "      <td>47.218676</td>\n",
       "      <td>48.525990</td>\n",
       "      <td>50.025287</td>\n",
       "      <td>50.043571</td>\n",
       "      <td>...</td>\n",
       "      <td>42.876205</td>\n",
       "      <td>42.693363</td>\n",
       "      <td>41.971142</td>\n",
       "      <td>40.682114</td>\n",
       "      <td>40.270720</td>\n",
       "      <td>41.294628</td>\n",
       "      <td>41.029513</td>\n",
       "      <td>39.877613</td>\n",
       "      <td>40.051309</td>\n",
       "      <td>40.517551</td>\n",
       "      <td>39.703911</td>\n",
       "      <td>40.883236</td>\n",
       "      <td>40.672971</td>\n",
       "      <td>39.959888</td>\n",
       "      <td>40.791817</td>\n",
       "      <td>40.298149</td>\n",
       "      <td>40.837527</td>\n",
       "      <td>40.581551</td>\n",
       "      <td>40.133587</td>\n",
       "      <td>39.530212</td>\n",
       "      <td>39.447937</td>\n",
       "      <td>39.438793</td>\n",
       "      <td>39.219384</td>\n",
       "      <td>37.811508</td>\n",
       "      <td>37.592098</td>\n",
       "      <td>38.990833</td>\n",
       "      <td>39.292522</td>\n",
       "      <td>38.999977</td>\n",
       "      <td>38.881129</td>\n",
       "      <td>40.289005</td>\n",
       "      <td>41.166637</td>\n",
       "      <td>40.800957</td>\n",
       "      <td>40.508416</td>\n",
       "      <td>40.480988</td>\n",
       "      <td>40.526697</td>\n",
       "      <td>39.786190</td>\n",
       "      <td>39.502787</td>\n",
       "      <td>38.853703</td>\n",
       "      <td>39.447934</td>\n",
       "      <td>40.819241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>40.590692</td>\n",
       "      <td>41.450043</td>\n",
       "      <td>41.642029</td>\n",
       "      <td>40.170158</td>\n",
       "      <td>40.791817</td>\n",
       "      <td>41.477470</td>\n",
       "      <td>41.541463</td>\n",
       "      <td>40.736965</td>\n",
       "      <td>40.270721</td>\n",
       "      <td>40.773533</td>\n",
       "      <td>41.815729</td>\n",
       "      <td>41.971142</td>\n",
       "      <td>42.656795</td>\n",
       "      <td>42.272831</td>\n",
       "      <td>42.446529</td>\n",
       "      <td>41.175785</td>\n",
       "      <td>40.974651</td>\n",
       "      <td>41.568886</td>\n",
       "      <td>41.431755</td>\n",
       "      <td>42.309391</td>\n",
       "      <td>42.839633</td>\n",
       "      <td>43.251026</td>\n",
       "      <td>43.881827</td>\n",
       "      <td>45.015442</td>\n",
       "      <td>44.686329</td>\n",
       "      <td>44.887451</td>\n",
       "      <td>47.218676</td>\n",
       "      <td>48.525990</td>\n",
       "      <td>50.025287</td>\n",
       "      <td>50.043571</td>\n",
       "      <td>48.343151</td>\n",
       "      <td>48.928242</td>\n",
       "      <td>48.909956</td>\n",
       "      <td>46.304474</td>\n",
       "      <td>43.452155</td>\n",
       "      <td>42.217979</td>\n",
       "      <td>43.918400</td>\n",
       "      <td>43.123039</td>\n",
       "      <td>41.459184</td>\n",
       "      <td>43.095612</td>\n",
       "      <td>...</td>\n",
       "      <td>39.703911</td>\n",
       "      <td>40.883236</td>\n",
       "      <td>40.672971</td>\n",
       "      <td>39.959888</td>\n",
       "      <td>40.791817</td>\n",
       "      <td>40.298149</td>\n",
       "      <td>40.837527</td>\n",
       "      <td>40.581551</td>\n",
       "      <td>40.133587</td>\n",
       "      <td>39.530212</td>\n",
       "      <td>39.447937</td>\n",
       "      <td>39.438793</td>\n",
       "      <td>39.219384</td>\n",
       "      <td>37.811508</td>\n",
       "      <td>37.592098</td>\n",
       "      <td>38.990833</td>\n",
       "      <td>39.292522</td>\n",
       "      <td>38.999977</td>\n",
       "      <td>38.881129</td>\n",
       "      <td>40.289005</td>\n",
       "      <td>41.166637</td>\n",
       "      <td>40.800957</td>\n",
       "      <td>40.508416</td>\n",
       "      <td>40.480988</td>\n",
       "      <td>40.526697</td>\n",
       "      <td>39.786190</td>\n",
       "      <td>39.502787</td>\n",
       "      <td>38.853703</td>\n",
       "      <td>39.447934</td>\n",
       "      <td>40.819241</td>\n",
       "      <td>40.362144</td>\n",
       "      <td>39.868473</td>\n",
       "      <td>39.676488</td>\n",
       "      <td>39.950748</td>\n",
       "      <td>40.115307</td>\n",
       "      <td>39.703915</td>\n",
       "      <td>39.521073</td>\n",
       "      <td>40.289005</td>\n",
       "      <td>40.142736</td>\n",
       "      <td>39.813622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>41.815729</td>\n",
       "      <td>41.971142</td>\n",
       "      <td>42.656795</td>\n",
       "      <td>42.272831</td>\n",
       "      <td>42.446529</td>\n",
       "      <td>41.175785</td>\n",
       "      <td>40.974651</td>\n",
       "      <td>41.568886</td>\n",
       "      <td>41.431755</td>\n",
       "      <td>42.309391</td>\n",
       "      <td>42.839633</td>\n",
       "      <td>43.251026</td>\n",
       "      <td>43.881827</td>\n",
       "      <td>45.015442</td>\n",
       "      <td>44.686329</td>\n",
       "      <td>44.887451</td>\n",
       "      <td>47.218676</td>\n",
       "      <td>48.525990</td>\n",
       "      <td>50.025287</td>\n",
       "      <td>50.043571</td>\n",
       "      <td>48.343151</td>\n",
       "      <td>48.928242</td>\n",
       "      <td>48.909956</td>\n",
       "      <td>46.304474</td>\n",
       "      <td>43.452155</td>\n",
       "      <td>42.217979</td>\n",
       "      <td>43.918400</td>\n",
       "      <td>43.123039</td>\n",
       "      <td>41.459184</td>\n",
       "      <td>43.095612</td>\n",
       "      <td>45.317130</td>\n",
       "      <td>44.064671</td>\n",
       "      <td>43.881827</td>\n",
       "      <td>42.629368</td>\n",
       "      <td>42.153982</td>\n",
       "      <td>42.894485</td>\n",
       "      <td>44.082958</td>\n",
       "      <td>42.035137</td>\n",
       "      <td>41.770013</td>\n",
       "      <td>42.437382</td>\n",
       "      <td>...</td>\n",
       "      <td>39.447937</td>\n",
       "      <td>39.438793</td>\n",
       "      <td>39.219384</td>\n",
       "      <td>37.811508</td>\n",
       "      <td>37.592098</td>\n",
       "      <td>38.990833</td>\n",
       "      <td>39.292522</td>\n",
       "      <td>38.999977</td>\n",
       "      <td>38.881129</td>\n",
       "      <td>40.289005</td>\n",
       "      <td>41.166637</td>\n",
       "      <td>40.800957</td>\n",
       "      <td>40.508416</td>\n",
       "      <td>40.480988</td>\n",
       "      <td>40.526697</td>\n",
       "      <td>39.786190</td>\n",
       "      <td>39.502787</td>\n",
       "      <td>38.853703</td>\n",
       "      <td>39.447934</td>\n",
       "      <td>40.819241</td>\n",
       "      <td>40.362144</td>\n",
       "      <td>39.868473</td>\n",
       "      <td>39.676488</td>\n",
       "      <td>39.950748</td>\n",
       "      <td>40.115307</td>\n",
       "      <td>39.703915</td>\n",
       "      <td>39.521073</td>\n",
       "      <td>40.289005</td>\n",
       "      <td>40.142736</td>\n",
       "      <td>39.813622</td>\n",
       "      <td>39.959889</td>\n",
       "      <td>40.252434</td>\n",
       "      <td>39.429652</td>\n",
       "      <td>38.277752</td>\n",
       "      <td>37.628672</td>\n",
       "      <td>38.469742</td>\n",
       "      <td>38.442307</td>\n",
       "      <td>38.625149</td>\n",
       "      <td>38.707430</td>\n",
       "      <td>39.740482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>42.839633</td>\n",
       "      <td>43.251026</td>\n",
       "      <td>43.881827</td>\n",
       "      <td>45.015442</td>\n",
       "      <td>44.686329</td>\n",
       "      <td>44.887451</td>\n",
       "      <td>47.218676</td>\n",
       "      <td>48.525990</td>\n",
       "      <td>50.025287</td>\n",
       "      <td>50.043571</td>\n",
       "      <td>48.343151</td>\n",
       "      <td>48.928242</td>\n",
       "      <td>48.909956</td>\n",
       "      <td>46.304474</td>\n",
       "      <td>43.452155</td>\n",
       "      <td>42.217979</td>\n",
       "      <td>43.918400</td>\n",
       "      <td>43.123039</td>\n",
       "      <td>41.459184</td>\n",
       "      <td>43.095612</td>\n",
       "      <td>45.317130</td>\n",
       "      <td>44.064671</td>\n",
       "      <td>43.881827</td>\n",
       "      <td>42.629368</td>\n",
       "      <td>42.153982</td>\n",
       "      <td>42.894485</td>\n",
       "      <td>44.082958</td>\n",
       "      <td>42.035137</td>\n",
       "      <td>41.770013</td>\n",
       "      <td>42.437382</td>\n",
       "      <td>42.876205</td>\n",
       "      <td>42.693363</td>\n",
       "      <td>41.971142</td>\n",
       "      <td>40.682114</td>\n",
       "      <td>40.270720</td>\n",
       "      <td>41.294628</td>\n",
       "      <td>41.029513</td>\n",
       "      <td>39.877613</td>\n",
       "      <td>40.051309</td>\n",
       "      <td>40.517551</td>\n",
       "      <td>...</td>\n",
       "      <td>41.166637</td>\n",
       "      <td>40.800957</td>\n",
       "      <td>40.508416</td>\n",
       "      <td>40.480988</td>\n",
       "      <td>40.526697</td>\n",
       "      <td>39.786190</td>\n",
       "      <td>39.502787</td>\n",
       "      <td>38.853703</td>\n",
       "      <td>39.447934</td>\n",
       "      <td>40.819241</td>\n",
       "      <td>40.362144</td>\n",
       "      <td>39.868473</td>\n",
       "      <td>39.676488</td>\n",
       "      <td>39.950748</td>\n",
       "      <td>40.115307</td>\n",
       "      <td>39.703915</td>\n",
       "      <td>39.521073</td>\n",
       "      <td>40.289005</td>\n",
       "      <td>40.142736</td>\n",
       "      <td>39.813622</td>\n",
       "      <td>39.959889</td>\n",
       "      <td>40.252434</td>\n",
       "      <td>39.429652</td>\n",
       "      <td>38.277752</td>\n",
       "      <td>37.628672</td>\n",
       "      <td>38.469742</td>\n",
       "      <td>38.442307</td>\n",
       "      <td>38.625149</td>\n",
       "      <td>38.707430</td>\n",
       "      <td>39.740482</td>\n",
       "      <td>40.362140</td>\n",
       "      <td>40.087875</td>\n",
       "      <td>39.676486</td>\n",
       "      <td>39.895893</td>\n",
       "      <td>39.521069</td>\n",
       "      <td>39.713051</td>\n",
       "      <td>40.215874</td>\n",
       "      <td>40.691257</td>\n",
       "      <td>41.596317</td>\n",
       "      <td>41.111790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>48.343151</td>\n",
       "      <td>48.928242</td>\n",
       "      <td>48.909956</td>\n",
       "      <td>46.304474</td>\n",
       "      <td>43.452155</td>\n",
       "      <td>42.217979</td>\n",
       "      <td>43.918400</td>\n",
       "      <td>43.123039</td>\n",
       "      <td>41.459184</td>\n",
       "      <td>43.095612</td>\n",
       "      <td>45.317130</td>\n",
       "      <td>44.064671</td>\n",
       "      <td>43.881827</td>\n",
       "      <td>42.629368</td>\n",
       "      <td>42.153982</td>\n",
       "      <td>42.894485</td>\n",
       "      <td>44.082958</td>\n",
       "      <td>42.035137</td>\n",
       "      <td>41.770013</td>\n",
       "      <td>42.437382</td>\n",
       "      <td>42.876205</td>\n",
       "      <td>42.693363</td>\n",
       "      <td>41.971142</td>\n",
       "      <td>40.682114</td>\n",
       "      <td>40.270720</td>\n",
       "      <td>41.294628</td>\n",
       "      <td>41.029513</td>\n",
       "      <td>39.877613</td>\n",
       "      <td>40.051309</td>\n",
       "      <td>40.517551</td>\n",
       "      <td>39.703911</td>\n",
       "      <td>40.883236</td>\n",
       "      <td>40.672971</td>\n",
       "      <td>39.959888</td>\n",
       "      <td>40.791817</td>\n",
       "      <td>40.298149</td>\n",
       "      <td>40.837527</td>\n",
       "      <td>40.581551</td>\n",
       "      <td>40.133587</td>\n",
       "      <td>39.530212</td>\n",
       "      <td>...</td>\n",
       "      <td>40.362144</td>\n",
       "      <td>39.868473</td>\n",
       "      <td>39.676488</td>\n",
       "      <td>39.950748</td>\n",
       "      <td>40.115307</td>\n",
       "      <td>39.703915</td>\n",
       "      <td>39.521073</td>\n",
       "      <td>40.289005</td>\n",
       "      <td>40.142736</td>\n",
       "      <td>39.813622</td>\n",
       "      <td>39.959889</td>\n",
       "      <td>40.252434</td>\n",
       "      <td>39.429652</td>\n",
       "      <td>38.277752</td>\n",
       "      <td>37.628672</td>\n",
       "      <td>38.469742</td>\n",
       "      <td>38.442307</td>\n",
       "      <td>38.625149</td>\n",
       "      <td>38.707430</td>\n",
       "      <td>39.740482</td>\n",
       "      <td>40.362140</td>\n",
       "      <td>40.087875</td>\n",
       "      <td>39.676486</td>\n",
       "      <td>39.895893</td>\n",
       "      <td>39.521069</td>\n",
       "      <td>39.713051</td>\n",
       "      <td>40.215874</td>\n",
       "      <td>40.691257</td>\n",
       "      <td>41.596317</td>\n",
       "      <td>41.111790</td>\n",
       "      <td>41.447394</td>\n",
       "      <td>41.102467</td>\n",
       "      <td>40.552445</td>\n",
       "      <td>40.095646</td>\n",
       "      <td>39.713428</td>\n",
       "      <td>40.272774</td>\n",
       "      <td>40.244799</td>\n",
       "      <td>39.750713</td>\n",
       "      <td>39.704109</td>\n",
       "      <td>39.554951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2545 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2   ...         97         98         99\n",
       "0      0.178288   0.179010   0.179010  ...   0.157716   0.157716   0.157716\n",
       "1      0.180814   0.181536   0.181536  ...   0.158799   0.158799   0.159521\n",
       "2      0.177927   0.178649   0.178649  ...   0.165656   0.165656   0.166739\n",
       "3      0.176483   0.173957   0.173957  ...   0.166739   0.166378   0.165656\n",
       "4      0.163852   0.162769   0.162769  ...   0.161686   0.161686   0.161325\n",
       "...         ...        ...        ...  ...        ...        ...        ...\n",
       "2540  41.352184  41.020363  41.523178  ...  38.853703  39.447934  40.819241\n",
       "2541  40.590692  41.450043  41.642029  ...  40.289005  40.142736  39.813622\n",
       "2542  41.815729  41.971142  42.656795  ...  38.625149  38.707430  39.740482\n",
       "2543  42.839633  43.251026  43.881827  ...  40.691257  41.596317  41.111790\n",
       "2544  48.343151  48.928242  48.909956  ...  39.750713  39.704109  39.554951\n",
       "\n",
       "[2545 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "id": "0Y5mebapWHxm",
    "outputId": "b5698a8b-db07-44bc-cb48-22f1e581a572"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def get_median(array, axis = 1):\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.median.html\n",
    "    return np.median(array, axis = axis).reshape(data_size, 1)  #, keepdims=True)\n",
    "\n",
    "median_data = get_median(data)\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "# Median data for each week\n",
    "print(median_data.shape)\n",
    "print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
    "lag = 7\n",
    "forecast = 3\n",
    "# Convert median_data to (n_samples, 5) matrix\n",
    "data_m1 = series_to_supervised(median_data, lag, forecast).values\n",
    "print(data_m1.shape)\n",
    "print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2554, 1)\n",
      "   median_stockprice_week\n",
      "0                0.180634\n",
      "1                0.179912\n",
      "2                0.177927\n",
      "3                0.173416\n",
      "4                0.164032\n",
      "5                0.156814\n",
      "6                0.148332\n",
      "7                0.158438\n",
      "8                0.154468\n",
      "9                0.157355\n",
      "(2545, 10)\n",
      "       week i+1   week i+2   week i+3  ...   week i+8   week i+9  week i+10\n",
      "0      0.180634   0.179912   0.177927  ...   0.158438   0.154468   0.157355\n",
      "1      0.179912   0.177927   0.173416  ...   0.154468   0.157355   0.158257\n",
      "2      0.177927   0.173416   0.164032  ...   0.157355   0.158257   0.163671\n",
      "3      0.173416   0.164032   0.156814  ...   0.158257   0.163671   0.166017\n",
      "4      0.164032   0.156814   0.148332  ...   0.163671   0.166017   0.162408\n",
      "...         ...        ...        ...  ...        ...        ...        ...\n",
      "2540  39.365658  40.782675  41.893435  ...  40.439850  39.109681  40.494702\n",
      "2541  40.782675  41.893435  44.951447  ...  39.109681  40.494702  39.909611\n",
      "2542  41.893435  44.951447  43.685277  ...  40.494702  39.909611  38.666289\n",
      "2543  44.951447  43.685277  42.761927  ...  39.909611  38.666289  40.151875\n",
      "2544  43.685277  42.761927  40.855813  ...  38.666289  40.151875  40.170223\n",
      "\n",
      "[2545 rows x 10 columns]\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mA0O8fXZOQsn",
    "outputId": "e0f96027-132d-4e37-c0f3-e62c99dbe27a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "def split_data(perc_train, perc_valid, lag, data_orig, data_m1, n_features_orig, n_features_median):\n",
    "    values = data_m1\n",
    "    \n",
    "    sizeOfReframed = len(data_m1)\n",
    "    len_train = int(perc_train*sizeOfReframed) # int(sizeOfReframed - len_test) # - len_valid)\n",
    "    train_data_orig = data_orig[:len_train, :]\n",
    "    # valid = values[len_train:len_valid+len_train, :]\n",
    "    test_data_orig = data_orig[len_train:, :]  # [len_valid+len_train:, :]\n",
    "    # n_features = n_features\n",
    "    \n",
    "    train_data_ml = values[:len_train, :]\n",
    "    test_data_ml = values[len_train:, :] \n",
    "    # split into input and outputs\n",
    "    n_obs = lag * n_features_orig\n",
    "    n_obs_median = (lag+forecast) * n_features_median\n",
    "    train_X, train_y = train_data_orig[:, :n_obs], train_data_ml[:, :n_obs_median]\n",
    "    test_X, test_y = test_data_orig[:, :n_obs], test_data_ml[:, :n_obs_median]\n",
    "    # valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
    "    print(train_X.shape, len(train_X), train_y.shape)\n",
    "    \n",
    "    # reshape input to be 3D [samples, features, lag]\n",
    "    train_X = train_X.reshape((train_X.shape[0], n_features_orig, lag))\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_features_orig, lag))\n",
    "    # valid_X = valid_X.reshape((valid_X.shape[0], lag, n_features))\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # , valid_X.shape, valid_y.shape)\n",
    "    \n",
    "    dataload = {\n",
    "        'train_data_orig': train_data_orig,\n",
    "        'test_data_orig': test_data_orig,\n",
    "        'train_data_ml': train_data_ml,\n",
    "        'test_data_ml': test_data_ml,\n",
    "        # 'valid': valid,\n",
    "        'train_X': train_X,\n",
    "        'train_y': train_y,\n",
    "        'test_X': test_X,\n",
    "        'test_y': test_y,\n",
    "        'n_features_orig': n_features_orig,\n",
    "        'n_features_median': n_features_median,\n",
    "        'n_obs': n_obs,\n",
    "        'n_obs_median': n_obs_median,\n",
    "        # 'valid_X': valid_X,\n",
    "        # 'valid_y': valid_y\n",
    "    }\n",
    "    \n",
    "    return dataload\n",
    "\n",
    "dataload = split_data(0.8, 0, lag, data_orig, data_m1, data.shape[1], 1)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2036, 70) 2036 (2036, 10)\n",
      "(2036, 10, 7) (2036, 10) (509, 10, 7) (509, 10)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wRtqw1eOQu4",
    "outputId": "7cf84b8b-ea5c-4c48-fdb8-043f6c363018"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "train_X = torch.from_numpy(dataload['train_X']).to(device)\n",
    "print(train_X.shape)  # (#training, 1, 5)\n",
    "\n",
    "train_y = torch.from_numpy(dataload['train_y']).to(device)\n",
    "print(train_y.shape)  # (#training, 5)\n",
    "train_y = torch.reshape(train_y, (train_X.shape[0], 1, train_y.shape[1])).to(device)\n",
    "print(train_y.shape)  # (#training, 1, 5)\n",
    "test_X = torch.from_numpy(dataload['test_X']).to(device)\n",
    "print(test_X.shape)  # (#testing, 1, 5)\n",
    "test_y = torch.from_numpy(dataload['test_y']).to(device)\n",
    "print(test_y.shape)  # (#testing, 5)\n",
    "test_y = torch.reshape(test_y, (test_X.shape[0], 1, test_y.shape[1])).to(device)\n",
    "print(test_y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2036, 10, 7])\n",
      "torch.Size([2036, 10])\n",
      "torch.Size([2036, 1, 10])\n",
      "torch.Size([509, 10, 7])\n",
      "torch.Size([509, 10])\n",
      "torch.Size([509, 1, 10])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJG0Ur67OQyb",
    "outputId": "47fcf9ba-c325-4ce7-fdd9-f88f8f30adc4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "class LatentODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, nhidden=20):\n",
    "        super(LatentODEfunc, self).__init__()\n",
    "        self.elu = nn.ELU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, nhidden)\n",
    "        self.fc3 = nn.Linear(nhidden, latent_dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.fc1(x).to(device)\n",
    "        out = self.elu(out).to(device)\n",
    "        out = self.fc2(out).to(device)\n",
    "        out = self.elu(out).to(device)\n",
    "        out = self.fc3(out).to(device)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RecognitionRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=25, nbatch=1):\n",
    "        super(RecognitionRNN, self).__init__()\n",
    "        self.nhidden = nhidden\n",
    "        self.nbatch = nbatch\n",
    "        self.i2h = nn.Linear(obs_dim + nhidden, nhidden)\n",
    "        self.i2h = self.i2h.float()\n",
    "        self.h2o = nn.Linear(nhidden, latent_dim * 2)\n",
    "        self.h2o = self.h2o.float()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        combined = torch.cat((x, h), dim=1).to(device)\n",
    "        h = torch.tanh(self.i2h(combined.float())).to(device)\n",
    "        out = self.h2o(h).to(device)\n",
    "        return out, h\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.nbatch, self.nhidden)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=20):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, obs_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc1(z).to(device)\n",
    "        out = self.relu(out).to(device)\n",
    "        out = self.fc2(out).to(device)\n",
    "        return out\n",
    "\n",
    "\n",
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val\n",
    "\n",
    "\n",
    "def log_normal_pdf(x, mean, logvar):\n",
    "    const = torch.from_numpy(np.array([2. * np.pi])).float().to(x.device).to(device)\n",
    "    const = torch.log(const).to(device)\n",
    "    return -.5 * (const + logvar + (x - mean) ** 2. / torch.exp(logvar))\n",
    "\n",
    "def normal_kl(mu1, lv1, mu2, lv2):\n",
    "    v1 = torch.exp(lv1)\n",
    "    v2 = torch.exp(lv2)\n",
    "    lstd1 = lv1 / 2.\n",
    "    lstd2 = lv2 / 2.\n",
    "\n",
    "    kl = lstd2 - lstd1 + ((v1 + (mu1 - mu2) ** 2.) / (2. * v2)) - .5\n",
    "    return kl.to(device)\n",
    "\n",
    "def train(loss_str, niters):\n",
    "    loss_list = []\n",
    "    for itr in range(1, niters + 1):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        # backward in time to infer q(z_0)\n",
    "        h = rec.initHidden().to(device)  # (# nbatches_train, rnn_hidden)\n",
    "        for t_r in reversed(range(train_X.shape[2])):\n",
    "            obs = train_X[:, :, t_r].to(device)\n",
    "            # obs = torch.reshape(obs, (1, 1)).to(device)\n",
    "            out, h = rec.forward(obs, h)\n",
    "        qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
    "        epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "        z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "        # forward in time and solve ode for reconstructions\n",
    "        pred_z = odeint(func, z0, t.to(device)).permute(1, 0, 2)  # [:, -1, :]\n",
    "        pred_x = dec(pred_z).to(device)\n",
    "        pred_x = torch.reshape(pred_x, (train_X.shape[0], lag)).to(device)\n",
    "\n",
    "        # compute loss\n",
    "        if loss_str == 'mse':\n",
    "            loss = torch.nn.MSELoss()(pred_x.float(), train_X[i, :, :].float().to(device)).float()\n",
    "        elif loss_str == 'elbo':\n",
    "            noise_std_ = torch.zeros(pred_x.size()).to(device) + noise_std\n",
    "            noise_logvar = 2. * torch.log(noise_std_).to(device)\n",
    "            logpx = log_normal_pdf(\n",
    "                train_y[:, 0, :].to(device), pred_x, noise_logvar).sum(-1)  # .sum(-1)\n",
    "            pz0_mean = pz0_logvar = torch.zeros(z0.size()).to(device)\n",
    "            analytic_kl = normal_kl(qz0_mean, qz0_logvar,\n",
    "                                    pz0_mean, pz0_logvar).sum(-1)\n",
    "            loss = torch.mean(-logpx + analytic_kl, dim=0).to(device)\n",
    "            # loss = torch.reshape(loss, (1, 1)).to(device)\n",
    "        loss_list.append(loss)\n",
    "            # loss_ = torch.mean(torch.cat([x.float() for x in loss_list])).to(device)  \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # loss_meter.update(loss.item())\n",
    "        if itr%10==0:\n",
    "            print('Iter: {}, running: {:.4f}'.format(itr, loss.item()))\n",
    "    return loss_list\n",
    "\n",
    "# def train_loss(h):\n",
    "#     train_loss = 0.0\n",
    "#     predictions = []\n",
    "#     for i in range(train_X.shape[0]):\n",
    "#         for t_r in reversed(range(train_X.shape[2])):\n",
    "#             obs = train_X[i, :, t_r].to(device)\n",
    "#             obs = torch.reshape(obs, (1, 1)).to(device)\n",
    "#             out, h = rec.forward(obs, h)\n",
    "    \n",
    "#         qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
    "#         epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "#         z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "#         # forward in time and solve ode for reconstructions\n",
    "#         pred_z = odeint(func, z0, t.to(device)).permute(1, 0, 2)\n",
    "#         pred_x = dec(pred_z).to(device)\n",
    "#         rmse = RMSELoss()\n",
    "#         # loss = torch.nn.MSELoss()(pred_x, train_X[i, :, :].to(device))\n",
    "#         loss = rmse(pred_x, train_X[i, :, :].to(device))\n",
    "#         train_loss += loss\n",
    "#         predictions.append(pred_x)\n",
    "    \n",
    "#     # train_loss = torch.sqrt(train_loss)\n",
    "#     train_pred = torch.cat([x.float() for x in predictions])\n",
    "#     train_pred = torch.reshape(train_pred, (train_pred.shape[0], 1, lag))\n",
    "#     with torch.no_grad():\n",
    "#         print('Total Train Loss {:.6f}'.format(train_loss.item()))\n",
    "#     return train_pred\n",
    "\n",
    "def plot_train(i, train_pred):\n",
    "    t_ = torch.linspace(1., train_y.shape[0], train_y.shape[0])\n",
    "    plt.figure()\n",
    "    plt.plot(t_.numpy(), train_y.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
    "    with torch.no_grad():\n",
    "        rmse = np.sqrt(((train_y.cpu().numpy()[:, :, i-1] - train_pred.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
    "        plt.plot(t_.numpy(), train_pred.cpu().numpy()[:, :, i-1], '--', label = f\"pred_week{i}\")\n",
    "    plt.title(f\"Trial No. {trial}: Train: Apple's Median Stock price for week{i}: RMSE {rmse}\")\n",
    "    plt.legend(framealpha=1, frameon=True);\n",
    "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Train: Apple's Median Stock price for week{i}-all.pdf\", dpi = 150)\n",
    "    plt.show()\n",
    "\n",
    "# def test_loss(h, t_test):\n",
    "#     # print(h.shape)\n",
    "#     test_loss = 0.0\n",
    "#     predictions = []\n",
    "#     rmse = RMSELoss()\n",
    "#     for i in range(test_X.shape[0]):\n",
    "#         for t_r in reversed(range(test_y.size(3))):\n",
    "#             obs = test_X[i, :, t_r].to(device)\n",
    "#             obs = torch.reshape(obs, (1, 1)).to(device)\n",
    "#             out, h = rec.forward(obs, h)\n",
    "#         qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
    "#         epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "#         z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "#         # forward in time and solve ode for reconstructions\n",
    "#         pred_z = odeint(func, z0, t_test).permute(1, 0, 2).to(device)\n",
    "#         pred_x = dec(pred_z).to(device)\n",
    "#         pred_x = torch.reshape(pred_x, (1, lag))\n",
    "#         # print(test_pred_y.shape)\n",
    "#         rmse = RMSELoss()\n",
    "#         # loss = torch.nn.MSELoss()(pred_x, train_X[i, :, :].to(device))\n",
    "#         loss = rmse(pred_x, test_X[i, :, :].to(device))\n",
    "\n",
    "#         # loss = torch.nn.MSELoss()(pred_x, torch.reshape(test_X[i, :, :].to(device), (1, 5))).to(device)\n",
    "#         test_loss += loss\n",
    "#         predictions.append(pred_x)\n",
    "    \n",
    "#     # test_loss = torch.sqrt(test_loss)\n",
    "#     test_pred = torch.cat([x.float() for x in predictions])\n",
    "    \n",
    "#     # loss = torch.nn.MSELoss()(test_pred_y[train_size-test_size:, batch_time2-2, :], label_batch_y[train_size-test_size:, batch_time2-2, :])\n",
    "#     with torch.no_grad():\n",
    "#         print('Total Loss {:.6f}'.format(test_loss.item()))\n",
    "#     return test_pred\n",
    "def plot_test(i, test_pred):\n",
    "    t_ = torch.linspace(1., test_X.shape[0], test_X.shape[0])\n",
    "    plt.figure()\n",
    "    plt.plot(t_.numpy(), test_y.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
    "    with torch.no_grad():\n",
    "        rmse = np.sqrt(((test_y.cpu().numpy()[:, :, i-1] - test_pred.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
    "        plt.plot(t_.numpy(), test_pred.cpu().numpy()[:, :, i-1], '--', label = f\"pred_week{i+lag} : RMSE {rmse}\")\n",
    "    plt.title(f\"Trial No. {trial}: Test_X: Apple's Median Stock price for week{i}: RMSE {rmse}\")\n",
    "    plt.legend(framealpha=1, frameon=True);\n",
    "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Test_X: Apple's Median Stock price for week{i}.pdf\", dpi = 150)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(loss_list):\n",
    "    plt.plot(loss_list)\n",
    "    plt.title('Train Loss')\n",
    "    plt.ylabel('RMSE Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.savefig(f\"plots-latentode/Trial No. {trial}: RMSE Train Loss.pdf\", dpi = 150)\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "SXZ9ebv3Y3_b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "t = torch.linspace(0, lag+forecast-1, lag+forecast)\n",
    "print(t.shape)\n",
    "\n",
    "latent_dim = 4\n",
    "nhidden = 20\n",
    "rnn_nhidden = 25\n",
    "obs_dim = data.shape[1]\n",
    "out_dim = 1\n",
    "noise_std = .3\n",
    "\n",
    "## Training\n",
    "# model\n",
    "nbatches_train = train_X.shape[0]\n",
    "func = LatentODEfunc(latent_dim, nhidden).to(device)\n",
    "rec = RecognitionRNN(latent_dim, obs_dim, rnn_nhidden, nbatches_train).to(device)\n",
    "dec = Decoder(latent_dim, out_dim, nhidden).to(device)\n",
    "params = (list(func.parameters()) + list(dec.parameters()) + list(rec.parameters()))\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(params, lr=lr)\n",
    "loss_str = 'elbo'\n",
    "# all_values = True\n",
    "niters = 100  # training epochs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1ujtJbkOQ0U",
    "outputId": "17fab3fd-6092-4c9f-811e-14c1339e7afb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "h = rec.initHidden().to(device) # (# nbatches_train, rnn_hidden)\n",
    "print(h.shape)\n",
    "obs = train_X[:, :, 0].to(device)\n",
    "print(obs.shape)\n",
    "# obs = torch.reshape(obs, (1, 1)).to(device)\n",
    "out, h = rec.forward(obs, h)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2036, 25])\n",
      "torch.Size([2036, 10])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fff2kD-HOQ2Z",
    "outputId": "c31c12f6-ca29-4592-b641-6a51942f3a07"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
    "\n",
    "epsilon = torch.randn(qz0_mean.size()).to(device)\n",
    "z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
    "\n",
    "# forward in time and solve ode for reconstructions\n",
    "pred_z = odeint(func, z0, t.to(device)).permute(1, 0, 2)  # [:, -1, :]\n",
    "print(pred_z.shape)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2036, 10, 4])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCCpGx1fZvnY",
    "outputId": "aab4bba4-1c48-4b40-a930-acb50acca587"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "pred_x = dec(pred_z).to(device)\n",
    "print(pred_x.shape)\n",
    "pred_x = torch.reshape(pred_x, (train_X.shape[0], lag+forecast)).to(device)\n",
    "print(pred_x.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2036, 10, 1])\n",
      "torch.Size([2036, 10])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UY1s6HS1Zvqv",
    "outputId": "788c7c54-1c41-4e9b-c6b4-e674843e63be"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "noise_std_ = torch.zeros(pred_x.size()).to(device) + noise_std\n",
    "noise_logvar = 2. * torch.log(noise_std_).to(device)\n",
    "print(noise_logvar.shape)\n",
    "print(train_y[:, 0, :].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2036, 10])\n",
      "torch.Size([2036, 10])\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0vZLGmXZvsL",
    "outputId": "d3cb2ede-4470-43a6-8daa-977d0effbd54"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "logpx = log_normal_pdf(\n",
    "    train_y[:, 0, :].to(device), pred_x, noise_logvar).sum(-1)  # .sum(-1)\n",
    "pz0_mean = pz0_logvar = torch.zeros(z0.size()).to(device)\n",
    "analytic_kl = normal_kl(qz0_mean, qz0_logvar,\n",
    "                        pz0_mean, pz0_logvar).sum(-1)\n",
    "loss = torch.mean(-logpx + analytic_kl, dim=0).to(device)\n",
    "loss"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(19548.2856, dtype=torch.float64, grad_fn=<MeanBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0_yeEr5Zvvj",
    "outputId": "e3209068-8bd9-427c-8c4a-85f73f670e43"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "G0xzH-OcZXR7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "ISd_1954ZXXN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## m1m2m3m4m5m6m7m8 as input and m1m2m3m4m5m6m7m8m9m10 as output. m9m10 as output of test"
   ],
   "metadata": {
    "id": "ma7p4FaZD-jl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "Ojl_6uq7EH1F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "oISamVwGEZcH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "JzpKd0FCEZZz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "g1id7XxtEZXh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "E4znZ3SiEZUv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "tW9gGuB-EZR_"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ma7p4FaZD-jl"
   ],
   "name": "LatentOde_more_input_sequence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}