{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multivariate_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXD7yeBQeKl3"
      },
      "source": [
        "# Install (Important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g85N8X8QEka8"
      },
      "source": [
        "# Install the latest version of author's repo neural ode implementation\n",
        "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "!cd torchdiffeq && pip install -e .\n",
        "!ls torchdiffeq/torchdiffeq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kElcTmaWeia3"
      },
      "source": [
        "# Latent ODE on Multi-variate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDaTGnuMemJX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpGaiLNfem35"
      },
      "source": [
        "# Rough"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbH27wGrennu"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "from pandas import concat\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "from torchdiffeq.torchdiffeq import odeint\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "def stockDataTransformer(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    df1 = df[['Open', 'Close']].copy()\n",
        "    data = df1.values\n",
        "    n_samples = data.shape[0]//10*10\n",
        "    reshape_number = n_samples*data.shape[1]//10\n",
        "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
        "    return data1\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "def get_median(array, axis = 1):\n",
        "    # https://numpy.org/doc/stable/reference/generated/numpy.median.html\n",
        "    return np.median(array, axis = axis).reshape(data_size, 1)  #, keepdims=True)\n",
        "\n",
        "def split_data(perc_train, perc_valid, lag, data_orig, data_m1, n_features_orig, n_features_median):\n",
        "    values = data_m1\n",
        "    \n",
        "    sizeOfReframed = len(data_m1)\n",
        "    len_train = int(perc_train*sizeOfReframed) # int(sizeOfReframed - len_test) # - len_valid)\n",
        "    train_data_orig = data_orig[:len_train, :]\n",
        "    # valid = values[len_train:len_valid+len_train, :]\n",
        "    test_data_orig = data_orig[len_train:, :]  # [len_valid+len_train:, :]\n",
        "    # n_features = n_features\n",
        "    \n",
        "    train_data_ml = values[:len_train, :]\n",
        "    test_data_ml = values[len_train:, :] \n",
        "    # split into input and outputs\n",
        "    n_obs = lag * n_features_orig\n",
        "    n_obs_median = (lag+forecast) * n_features_median\n",
        "    train_X, train_y = train_data_orig[:, :n_obs], train_data_ml[:, :n_obs_median]\n",
        "    test_X, test_y = test_data_orig[:, :n_obs], test_data_ml[:, :n_obs_median]\n",
        "    # valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
        "    print(train_X.shape, len(train_X), train_y.shape)\n",
        "    \n",
        "    # reshape input to be 3D [samples, features, lag]\n",
        "    train_X = train_X.reshape((train_X.shape[0], n_features_orig, lag))\n",
        "    test_X = test_X.reshape((test_X.shape[0], n_features_orig, lag))\n",
        "    # valid_X = valid_X.reshape((valid_X.shape[0], lag, n_features))\n",
        "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # , valid_X.shape, valid_y.shape)\n",
        "    \n",
        "    # Get the reconstruction train_y, test_y and extrapolated train_y, test_y\n",
        "    train_y_recon, train_y_extrapol = train_y[:, :lag], train_y[:, lag:]\n",
        "    test_y_recon, test_y_extrapol = test_y[:, :lag], test_y[:, lag:]\n",
        "    dataload = {\n",
        "        'train_data_orig': train_data_orig,\n",
        "        'test_data_orig': test_data_orig,\n",
        "        'train_data_ml': train_data_ml,\n",
        "        'test_data_ml': test_data_ml,\n",
        "        # 'valid': valid,\n",
        "        'train_X': train_X,\n",
        "        'train_y': train_y,\n",
        "        'test_X': test_X,\n",
        "        'test_y': test_y,\n",
        "        'n_features_orig': n_features_orig,\n",
        "        'n_features_median': n_features_median,\n",
        "        'n_obs': n_obs,\n",
        "        'n_obs_median': n_obs_median,\n",
        "        # 'valid_X': valid_X,\n",
        "        # 'valid_y': valid_y,\n",
        "        'train_y_recon': train_y_recon,\n",
        "        'train_y_extrapol': train_y_extrapol,\n",
        "        'test_y_recon': test_y_recon,\n",
        "        'test_y_extrapol': test_y_extrapol\n",
        "    }\n",
        "    \n",
        "    return dataload\n",
        "\n",
        "# https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self,yhat,y):\n",
        "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
        "        return loss\n",
        "class LatentODEfunc(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim=4, nhidden=20):\n",
        "        super(LatentODEfunc, self).__init__()\n",
        "        self.elu = nn.ELU(inplace=True)\n",
        "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
        "        self.fc2 = nn.Linear(nhidden, nhidden)\n",
        "        self.fc3 = nn.Linear(nhidden, latent_dim)\n",
        "        self.nfe = 0\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        self.nfe += 1\n",
        "        out = self.fc1(x).to(device)\n",
        "        out = self.elu(out).to(device)\n",
        "        out = self.fc2(out).to(device)\n",
        "        out = self.elu(out).to(device)\n",
        "        out = self.fc3(out).to(device)\n",
        "        return out\n",
        "\n",
        "class RecognitionRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=25, nbatch=1):\n",
        "        super(RecognitionRNN, self).__init__()\n",
        "        self.nhidden = nhidden\n",
        "        self.nbatch = nbatch\n",
        "        self.i2h = nn.Linear(obs_dim + nhidden, nhidden)\n",
        "        self.i2h = self.i2h.float()\n",
        "        self.h2o = nn.Linear(nhidden, latent_dim * 2)\n",
        "        self.h2o = self.h2o.float()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        combined = torch.cat((x, h), dim=1).to(device)\n",
        "        h = torch.tanh(self.i2h(combined.float())).to(device)\n",
        "        out = self.h2o(h).to(device)\n",
        "        return out, h\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.nbatch, self.nhidden)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
        "        self.fc2 = nn.Linear(nhidden, obs_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.fc1(z).to(device)\n",
        "        out = self.relu(out).to(device)\n",
        "        out = self.fc2(out).to(device)\n",
        "        return out\n",
        "\n",
        "class RunningAverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, momentum=0.99):\n",
        "        self.momentum = momentum\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = None\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        if self.val is None:\n",
        "            self.avg = val\n",
        "        else:\n",
        "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
        "        self.val = val\n",
        "\n",
        "\n",
        "def log_normal_pdf(x, mean, logvar):\n",
        "    const = torch.from_numpy(np.array([2. * np.pi])).float().to(x.device).to(device)\n",
        "    const = torch.log(const).to(device)\n",
        "    return -.5 * (const + logvar + (x - mean) ** 2. / torch.exp(logvar))\n",
        "\n",
        "def normal_kl(mu1, lv1, mu2, lv2):\n",
        "    v1 = torch.exp(lv1)\n",
        "    v2 = torch.exp(lv2)\n",
        "    lstd1 = lv1 / 2.\n",
        "    lstd2 = lv2 / 2.\n",
        "\n",
        "    kl = lstd2 - lstd1 + ((v1 + (mu1 - mu2) ** 2.) / (2. * v2)) - .5\n",
        "    return kl.to(device)\n",
        "\n",
        "def train(loss_str, niters):\n",
        "    loss_list = []\n",
        "    for itr in range(1, niters + 1):\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        # backward in time to infer q(z_0)\n",
        "        h = rec.initHidden().to(device)  # (# nbatches_train, rnn_hidden)\n",
        "        for t_r in reversed(range(train_X.shape[2])):  # input_dimension\n",
        "            obs = train_X[:, :, t_r].to(device)\n",
        "            # obs = torch.reshape(obs, (1, 1)).to(device)\n",
        "            out, h = rec.forward(obs, h)\n",
        "        qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
        "        epsilon = torch.randn(qz0_mean.size()).to(device)\n",
        "        z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
        "\n",
        "        # forward in time and solve ode for reconstructions\n",
        "        pred_z = odeint(func, z0, t.to(device)).permute(1, 0, 2)  # [:, -1, :]\n",
        "        pred_x = dec(pred_z).to(device)\n",
        "        pred_x = torch.reshape(pred_x, (train_X.shape[0], lag+forecast)).to(device)\n",
        "\n",
        "        # compute loss\n",
        "        if loss_str == 'mse':\n",
        "            loss = torch.nn.MSELoss()(pred_x.float(), train_X[i, :, :].float().to(device)).float()\n",
        "        elif loss_str == 'elbo':\n",
        "            noise_std_ = torch.zeros(pred_x.size()).to(device) + noise_std\n",
        "            noise_logvar = 2. * torch.log(noise_std_).to(device)\n",
        "            logpx = log_normal_pdf(\n",
        "                train_y[:, 0, :].to(device), pred_x, noise_logvar).sum(-1)  # .sum(-1)\n",
        "            pz0_mean = pz0_logvar = torch.zeros(z0.size()).to(device)\n",
        "            analytic_kl = normal_kl(qz0_mean, qz0_logvar,\n",
        "                                    pz0_mean, pz0_logvar).sum(-1)\n",
        "            loss = torch.mean(-logpx + analytic_kl, dim=0).to(device)\n",
        "            # loss = torch.reshape(loss, (1, 1)).to(device)\n",
        "        loss_list.append(loss)\n",
        "            # loss_ = torch.mean(torch.cat([x.float() for x in loss_list])).to(device)  \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss_meter.update(loss.item())\n",
        "        if itr%10==0:\n",
        "            print('Iter: {}, running: {:.4f}'.format(itr, loss.item()))\n",
        "    return loss_list\n",
        "\n",
        "def train_loss(h):\n",
        "    train_loss = 0.0\n",
        "    predictions = []\n",
        "    \n",
        "    for t_r in reversed(range(train_X.shape[2])):\n",
        "        obs = train_X[:, :, t_r].to(device)\n",
        "        # obs = torch.reshape(obs, (1, 1)).to(device)\n",
        "        out, h = rec.forward(obs, h)\n",
        "\n",
        "    qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
        "    epsilon = torch.randn(qz0_mean.size()).to(device)\n",
        "    z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
        "\n",
        "    # forward in time and solve ode for reconstructions\n",
        "    pred_z = odeint(func, z0, t.to(device)).permute(1, 0, 2)\n",
        "    pred_x = dec(pred_z).to(device)\n",
        "    pred_x = torch.reshape(pred_x, (pred_x.shape[0], 1, lag+forecast))\n",
        "    pred_x_recon = pred_x[:, :, :lag]\n",
        "    pred_x_extrapol = pred_x[:, :, lag:]\n",
        "    rmse = RMSELoss()\n",
        "    # loss = torch.nn.MSELoss()(pred_x, train_X[:, :, :].to(device))\n",
        "    loss_recon = rmse(pred_x_recon, train_y_recon)\n",
        "    loss_extrapol = rmse(pred_x_extrapol, train_y_extrapol)\n",
        "    # train_loss += loss\n",
        "    # predictions.append(pred_x)\n",
        "    \n",
        "    # train_loss = torch.sqrt(train_loss)\n",
        "    # train_pred = torch.cat([x.float() for x in predictions])\n",
        "    # train_pred = torch.reshape(train_pred, (train_pred.shape[0], 1, lag))\n",
        "    with torch.no_grad():\n",
        "        print('Train: Reconstruction Loss')\n",
        "        print('Total Train Loss {:.6f}'.format(loss_recon.item()))\n",
        "        print('Train: Extrapolation Loss')\n",
        "        print('Total Train Extrapolation Loss {:.6f}'.format(loss_extrapol.item()))\n",
        "    return pred_x, pred_x_recon, pred_x_extrapol\n",
        "\n",
        "def test_loss(h, t_test):\n",
        "    # print(h.shape)\n",
        "    test_loss = 0.0\n",
        "    predictions = []\n",
        "    rmse = RMSELoss()\n",
        "    for t_r in reversed(range(test_X.shape[2])):\n",
        "        obs = test_X[:, :, t_r].to(device)\n",
        "        # obs = torch.reshape(obs, (1, 1)).to(device)\n",
        "        out, h = rec.forward(obs, h)\n",
        "    qz0_mean, qz0_logvar = out[:, :latent_dim].to(device), out[:, latent_dim:].to(device)\n",
        "    epsilon = torch.randn(qz0_mean.size()).to(device)\n",
        "    z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
        "\n",
        "    # forward in time and solve ode for reconstructions\n",
        "    pred_z = odeint(func, z0, t_test).permute(1, 0, 2).to(device)\n",
        "    pred_x = dec(pred_z).to(device)\n",
        "    pred_x = torch.reshape(pred_x, (pred_x.shape[0], 1, lag+forecast))\n",
        "    pred_x_recon = pred_x[:, :, :lag]\n",
        "    pred_x_extrapol = pred_x[:, :, lag:]\n",
        "    # pred_test_X = torch.reshape(pred_test_X, (pred_test_X.shape[0], pred_test_X.shape[1]))\n",
        "    # pred_test_y = torch.reshape(pred_test_y, (1, pred_test_y.shape[0], 1, lag))\n",
        "    # print(test_pred_y.shape)\n",
        "    rmse = RMSELoss()\n",
        "    # loss = torch.nn.MSELoss()(pred_x, train_X[i, :, :].to(device))\n",
        "    loss_recon = rmse(pred_x_recon, test_y_recon)\n",
        "    loss_extrapol = rmse(pred_x_extrapol, test_y_extrapol)\n",
        "\n",
        "    # loss = torch.nn.MSELoss()(pred_x, torch.reshape(test_X[i, :, :].to(device), (1, 5))).to(device)\n",
        "    # test_loss += loss\n",
        "    # predictions.append(pred_x)\n",
        "    \n",
        "    # test_loss = torch.sqrt(test_loss)\n",
        "    # test_pred = torch.cat([x.float() for x in predictions])\n",
        "    \n",
        "    # loss = torch.nn.MSELoss()(test_pred_y[train_size-test_size:, batch_time2-2, :], label_batch_y[train_size-test_size:, batch_time2-2, :])\n",
        "    with torch.no_grad():\n",
        "        print('Test: Reconstruction Loss')\n",
        "        print('Total Loss {:.6f}'.format(loss_recon.item()))\n",
        "        print('Test: Extrapolation Loss')\n",
        "        print('Total Loss {:.6f}'.format(loss_extrapol.item()))\n",
        "    return pred_x, pred_x_recon, pred_x_extrapol\n",
        "\n",
        "def plot_train_recon(i, train_y_recon, pred_y_recon, fig, axes):\n",
        "    t_ = torch.linspace(1., train_y_recon.shape[0], train_y_recon.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), train_y_recon.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((train_y_recon.cpu().numpy()[:, :, i-1] - pred_y_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_recon.cpu().numpy()[:, :, i-1], '--', label = f\"recon_week{i}\")\n",
        "    plt.title(f\"Trial No. {trial}: Train Recon: {tickerName}'s Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Train Recon: {tickerName}'s Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_extrapol(i, train_y_extrapol, pred_y_extrapol, fig, axes):\n",
        "    t_ = torch.linspace(1., train_y_extrapol.shape[0], train_y_extrapol.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), train_y_extrapol.cpu().numpy()[:, :, i-1-lag], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((train_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_y_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_extrapol.cpu().numpy()[:, :, i-1-lag], '--', label = f\"extrapol_week{i}\")\n",
        "    plt.title(f\"Trial No. {trial}: Train Extrapol: {tickerName}'s Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Train Extrapol: {tickerName}'s Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_test_recon(i, test_y_recon, pred_y_recon, fig, axes):\n",
        "    t_ = torch.linspace(1., test_y_recon.shape[0], test_y_recon.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), test_y_recon.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((test_y_recon.cpu().numpy()[:, :, i-1] - pred_y_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_recon.cpu().numpy()[:, :, i-1], '--', label = f\"recon_week{i} : RMSE {rmse}\")\n",
        "    plt.title(f\"Trial No. {trial}: Test Recon: {tickerName}'s Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Test Recon: {tickerName}'s Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_test_extrapol(i, test_y_extrapol, pred_y_extrapol, fig, axes):\n",
        "    t_ = torch.linspace(1., test_y_extrapol.shape[0], test_y_extrapol.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), test_y_extrapol.cpu().numpy()[:, :, i-1-lag], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((test_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_y_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_extrapol.cpu().numpy()[:, :, i-1-lag], '--', label = f\"extrapol_week{i} : RMSE {rmse}\")\n",
        "    plt.title(f\"Trial No. {trial}: Test Extrapol: {tickerName}'s  Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Test Extrapol: {tickerName}'s  Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(loss_list):\n",
        "    plt.plot(loss_list)\n",
        "    plt.title(f'{tickerName}: Train Loss (Recon+Extrapol')\n",
        "    plt.ylabel('ELBO Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.savefig(f\"plots-latentode/{tickerName}: Trial No. {trial}: ELBO Train Loss.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def rmse_table(recon_rmse_filepath, extrapol_rmse_filepath, train_y_recon, pred_train_recon, train_y_extrapol, pred_train_extrapol, test_y_recon, pred_test_recon, test_y_extrapol, pred_test_extrapol):\n",
        "    recon_rmse_data = {\n",
        "        'week': list(),\n",
        "        'train_recon_rmse': list(),\n",
        "        'test_recon_rmse': list()\n",
        "    }\n",
        "\n",
        "    extrapol_rmse_data = {\n",
        "        'week': list(),\n",
        "        'train_extrapol_rmse': list(),\n",
        "        'test_extrapol_rmse': list()\n",
        "    }\n",
        "    for i in range(1, lag+forecast+1):\n",
        "        with torch.no_grad():\n",
        "            if i<=lag:\n",
        "                recon_rmse_data['week'].append(i)\n",
        "                train_recon_rmse = np.sqrt(((train_y_recon.cpu().numpy()[:, :, i-1] - pred_train_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "                recon_rmse_data['train_recon_rmse'].append(train_recon_rmse)\n",
        "                test_recon_rmse = np.sqrt(((test_y_recon.cpu().numpy()[:, :, i-1] - pred_test_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "                recon_rmse_data['test_recon_rmse'].append(test_recon_rmse)\n",
        "            elif i>lag:\n",
        "                extrapol_rmse_data['week'].append(i)\n",
        "                train_extrapol_rmse = np.sqrt(((train_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_train_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "                extrapol_rmse_data['train_extrapol_rmse'].append(train_extrapol_rmse)\n",
        "                test_extrapol_rmse = np.sqrt(((test_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_test_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "                extrapol_rmse_data['test_extrapol_rmse'].append(test_extrapol_rmse)\n",
        "    recon_rmse_df = pd.DataFrame(recon_rmse_data)\n",
        "    recon_rmse_df.to_csv(recon_rmse_filepath)\n",
        "    extrapol_rmse_df = pd.DataFrame(extrapol_rmse_data)\n",
        "    extrapol_rmse_df.to_csv(extrapol_rmse_filepath)\n",
        "\n",
        "    return recon_rmse_data, extrapol_rmse_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQjJzA2eewsQ"
      },
      "source": [
        "\n",
        "# FileName\n",
        "tickerName = 'ExxonMobil'\n",
        "# Filepath\n",
        "filepath = f\"raw-stock-data/{tickerName}.csv\"\n",
        "# Get the data in the required format\n",
        "data = stockDataTransformer(filepath)\n",
        "print('Data after Data Transformation')\n",
        "# # Total Data Size\n",
        "data_size = data.shape[0]\n",
        "\n",
        "print(pd.DataFrame(data))\n",
        "print('\\n')\n",
        "\n",
        "lag = 7\n",
        "forecast = 3\n",
        "data_orig = series_to_supervised(data, lag, forecast).values\n",
        "print('Data Original after series to supervised on data')\n",
        "print(data_orig.shape)\n",
        "print(pd.DataFrame(data_orig))\n",
        "print('\\n')\n",
        "\n",
        "median_data = get_median(data)\n",
        "print('Median data')\n",
        "# Median data for each week\n",
        "print(median_data.shape)\n",
        "print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
        "print('\\n')\n",
        "\n",
        "# Convert median_data to (n_samples, 5) matrix\n",
        "data_m1 = series_to_supervised(median_data, lag, forecast).values\n",
        "print('Median data after series to supervised')\n",
        "print(data_m1.shape)\n",
        "print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n",
        "print('\\n')\n",
        "\n",
        "dataload = split_data(0.8, 0, lag, data_orig, data_m1, data.shape[1], 1)\n",
        "\n",
        "print('Get Train and Test data')\n",
        "train_X = torch.from_numpy(dataload['train_X']).to(device)\n",
        "print(f\"train_X shape: {train_X.shape}\")  # (#training, 1, 5)\n",
        "\n",
        "train_y = torch.from_numpy(dataload['train_y']).to(device)\n",
        "print(f\"train_y shape: {train_y.shape}\")  # (#training, 5)\n",
        "train_y = torch.reshape(train_y, (train_X.shape[0], 1, train_y.shape[1])).to(device)\n",
        "print(f\"train_y shape: {train_y.shape}\")  # (#training, 1, 5)\n",
        "test_X = torch.from_numpy(dataload['test_X']).to(device)\n",
        "print(f\"test_X.shape : {test_X.shape}\")  # (#testing, 1, 5)\n",
        "test_y = torch.from_numpy(dataload['test_y']).to(device)\n",
        "print(f\"test_y.shape : {test_y.shape}\")  # (#testing, 5)\n",
        "test_y = torch.reshape(test_y, (test_X.shape[0], 1, test_y.shape[1])).to(device)\n",
        "print(f\"test_y.shape : {test_y.shape}\")\n",
        "\n",
        "train_y_recon = torch.from_numpy(dataload['train_y_recon']).to(device)\n",
        "train_y_recon = torch.reshape(train_y_recon, (train_y_recon.shape[0], 1,train_y_recon.shape[1]))\n",
        "train_y_extrapol = torch.from_numpy(dataload['train_y_extrapol']).to(device)\n",
        "train_y_extrapol = torch.reshape(train_y_extrapol, (train_y_extrapol.shape[0], 1, train_y_extrapol.shape[1]))\n",
        "test_y_recon = torch.from_numpy(dataload['test_y_recon']).to(device)\n",
        "test_y_recon = torch.reshape(test_y_recon, (test_y_recon.shape[0], 1, test_y_recon.shape[1]))\n",
        "test_y_extrapol = torch.from_numpy(dataload['test_y_extrapol']).to(device)\n",
        "test_y_extrapol = torch.reshape(test_y_extrapol, (test_y_extrapol.shape[0], 1, test_y_extrapol.shape[1]))\n",
        "print(f\"train_y_recon.shape : {train_y_recon.shape}\")\n",
        "print(f\"train_y_extrapol.shape : {train_y_extrapol.shape}\")\n",
        "print(f\"test_y_recon.shape : {test_y_recon.shape}\")\n",
        "print(f\"test_y_extrapol.shape : {test_y_extrapol.shape}\")\n",
        "print('\\n')\n",
        "\n",
        "trial = 2\n",
        "# predictions on test_data's time size (test_size-batch_time, 1)\n",
        "# Time steps\n",
        "t = torch.linspace(0, lag+forecast-1, lag+forecast).to(device)\n",
        "print(f\"t.shape : {t.shape}\")\n",
        "\n",
        "latent_dim = 50\n",
        "nhidden = 100\n",
        "rnn_nhidden = 125\n",
        "obs_dim = data.shape[1]\n",
        "out_dim = 1\n",
        "noise_std = .3\n",
        "\n",
        "lr = 0.001\n",
        "loss_str = 'elbo'\n",
        "# all_values = True\n",
        "niters = 2000  # training epochs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}