{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multivariate_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXD7yeBQeKl3"
      },
      "source": [
        "# Install (Important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g85N8X8QEka8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7508ff3e-75f0-42fb-c374-c8316e61f298"
      },
      "source": [
        "# Install the latest version of author's repo neural ode implementation\n",
        "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "!cd torchdiffeq && pip install -e .\n",
        "!ls torchdiffeq/torchdiffeq\n",
        "!pip install yfinance"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdiffeq'...\n",
            "remote: Enumerating objects: 1138, done.\u001b[K\n",
            "remote: Counting objects: 100% (434/434), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 1138 (delta 256), reused 400 (delta 240), pack-reused 704\u001b[K\n",
            "Receiving objects: 100% (1138/1138), 8.29 MiB | 15.78 MiB/s, done.\n",
            "Resolving deltas: 100% (682/682), done.\n",
            "Obtaining file:///content/torchdiffeq\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu102)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "  Running setup.py develop for torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n",
            "_impl  __init__.py\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.63.tar.gz (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.63-py2.py3-none-any.whl size=23918 sha256=8ba2fe3183bd91e55902b9407756777d8316920351655ba02974bc20df5b53f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/87/8b/7ec24486e001d3926537f5f7801f57a74d181be25b11157983\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.3 yfinance-0.1.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kElcTmaWeia3"
      },
      "source": [
        "# Latent ODE on Multi-variate data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDaTGnuMemJX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpGaiLNfem35"
      },
      "source": [
        "# Rough"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbH27wGrennu"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import numpy.random as npr\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd \n",
        "from pandas import concat\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "%matplotlib inline\n",
        "from torchdiffeq.torchdiffeq import odeint\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "def stockDataTransformer(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    df1 = df[['Open', 'Close']].copy()\n",
        "    data = df1.values\n",
        "    n_samples = data.shape[0]//10*10\n",
        "    reshape_number = n_samples*data.shape[1]//10\n",
        "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
        "    return data1\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "def get_median(array, axis = 1):\n",
        "    # https://numpy.org/doc/stable/reference/generated/numpy.median.html\n",
        "    return np.median(array, axis = axis).reshape(data_size, 1)  #, keepdims=True)\n",
        "\n",
        "def split_data(perc_train, perc_valid, lag, data_orig, data_m1, n_features_orig, n_features_median):\n",
        "    values = data_m1\n",
        "    \n",
        "    sizeOfReframed = len(data_m1)\n",
        "    len_train = int(perc_train*sizeOfReframed) # int(sizeOfReframed - len_test) # - len_valid)\n",
        "    train_data_orig = data_orig[:len_train, :]\n",
        "    # valid = values[len_train:len_valid+len_train, :]\n",
        "    test_data_orig = data_orig[len_train:, :]  # [len_valid+len_train:, :]\n",
        "    # n_features = n_features\n",
        "    \n",
        "    train_data_ml = values[:len_train, :]\n",
        "    test_data_ml = values[len_train:, :] \n",
        "    # split into input and outputs\n",
        "    n_obs = lag * n_features_orig\n",
        "    n_obs_median = (lag+forecast) * n_features_median\n",
        "    train_X, train_y = train_data_orig[:, :n_obs], train_data_ml[:, :n_obs_median]\n",
        "    test_X, test_y = test_data_orig[:, :n_obs], test_data_ml[:, :n_obs_median]\n",
        "    # valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
        "    print(train_X.shape, len(train_X), train_y.shape)\n",
        "    \n",
        "    # reshape input to be 3D [samples, features, lag]\n",
        "    train_X = train_X.reshape((train_X.shape[0], n_features_orig, lag))\n",
        "    test_X = test_X.reshape((test_X.shape[0], n_features_orig, lag))\n",
        "    # valid_X = valid_X.reshape((valid_X.shape[0], lag, n_features))\n",
        "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # , valid_X.shape, valid_y.shape)\n",
        "    \n",
        "    # Get the reconstruction train_y, test_y and extrapolated train_y, test_y\n",
        "    train_y_recon, train_y_extrapol = train_y[:, :lag], train_y[:, lag:]\n",
        "    test_y_recon, test_y_extrapol = test_y[:, :lag], test_y[:, lag:]\n",
        "    dataload = {\n",
        "        'train_data_orig': train_data_orig,\n",
        "        'test_data_orig': test_data_orig,\n",
        "        'train_data_ml': train_data_ml,\n",
        "        'test_data_ml': test_data_ml,\n",
        "        # 'valid': valid,\n",
        "        'train_X': train_X,\n",
        "        'train_y': train_y,\n",
        "        'test_X': test_X,\n",
        "        'test_y': test_y,\n",
        "        'n_features_orig': n_features_orig,\n",
        "        'n_features_median': n_features_median,\n",
        "        'n_obs': n_obs,\n",
        "        'n_obs_median': n_obs_median,\n",
        "        # 'valid_X': valid_X,\n",
        "        # 'valid_y': valid_y,\n",
        "        'train_y_recon': train_y_recon,\n",
        "        'train_y_extrapol': train_y_extrapol,\n",
        "        'test_y_recon': test_y_recon,\n",
        "        'test_y_extrapol': test_y_extrapol\n",
        "    }\n",
        "    \n",
        "    return dataload\n",
        "\n",
        "# https://discuss.pytorch.org/t/rmse-loss-function/16540/3\n",
        "class RMSELoss(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss()\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self,yhat,y):\n",
        "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
        "        return loss\n",
        "\n",
        "class LatentODEfunc(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim=4, nhidden=20):\n",
        "        super(LatentODEfunc, self).__init__()\n",
        "        self.elu = nn.ELU(inplace=True)\n",
        "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
        "        self.fc2 = nn.Linear(nhidden, nhidden)\n",
        "        self.fc3 = nn.Linear(nhidden, latent_dim)\n",
        "        self.nfe = 0\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        self.nfe += 1\n",
        "        out = self.fc1(x).to(device)\n",
        "        out = self.elu(out).to(device)\n",
        "        out = self.fc2(out).to(device)\n",
        "        out = self.elu(out).to(device)\n",
        "        out = self.fc3(out).to(device)\n",
        "        return out\n",
        "\n",
        "class RecognitionRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=25, nbatch=1, assets = 4):\n",
        "        super(RecognitionRNN, self).__init__()\n",
        "        self.nhidden = nhidden\n",
        "        self.nbatch = nbatch\n",
        "        self.assets = assets\n",
        "        self.i2h = nn.Linear(obs_dim + nhidden, nhidden)\n",
        "        self.i2h = self.i2h.float()\n",
        "        self.h2o = nn.Linear(nhidden, latent_dim * 2)\n",
        "        self.h2o = self.h2o.float()\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        combined = torch.cat((x, h), dim=2).to(device)\n",
        "        h = torch.tanh(self.i2h(combined.float())).to(device)\n",
        "        out = self.h2o(h).to(device)\n",
        "        return out, h\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.assets, self.nbatch, self.nhidden)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, latent_dim=4, obs_dim=5, nhidden=20):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
        "        self.fc2 = nn.Linear(nhidden, obs_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.fc1(z).to(device)\n",
        "        out = self.relu(out).to(device)\n",
        "        out = self.fc2(out).to(device)\n",
        "        return out\n",
        "\n",
        "class RunningAverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, momentum=0.99):\n",
        "        self.momentum = momentum\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = None\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        if self.val is None:\n",
        "            self.avg = val\n",
        "        else:\n",
        "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
        "        self.val = val\n",
        "\n",
        "\n",
        "def log_normal_pdf(x, mean, logvar):\n",
        "    const = torch.from_numpy(np.array([2. * np.pi])).float().to(x.device).to(device)\n",
        "    const = torch.log(const).to(device)\n",
        "    return -.5 * (const + logvar + (x - mean) ** 2. / torch.exp(logvar))\n",
        "\n",
        "def normal_kl(mu1, lv1, mu2, lv2):\n",
        "    v1 = torch.exp(lv1)\n",
        "    v2 = torch.exp(lv2)\n",
        "    lstd1 = lv1 / 2.\n",
        "    lstd2 = lv2 / 2.\n",
        "\n",
        "    kl = lstd2 - lstd1 + ((v1 + (mu1 - mu2) ** 2.) / (2. * v2)) - .5\n",
        "    return kl.to(device)\n",
        "\n",
        "def train(loss_str, niters):\n",
        "    loss_list = []\n",
        "    for itr in range(1, niters + 1):\n",
        "    \n",
        "        optimizer.zero_grad()\n",
        "        # backward in time to infer q(z_0)\n",
        "        h = rec.initHidden().to(device)  # (# nbatches_train, rnn_hidden)\n",
        "        for t_r in reversed(range(train_X.shape[3])):  # input_dimension\n",
        "            obs = train_X[:, :, :, t_r].to(device)\n",
        "            # obs = torch.reshape(obs, (1, 1)).to(device)\n",
        "            out, h = rec.forward(obs, h)\n",
        "        qz0_mean, qz0_logvar = out[:, :, :latent_dim].to(device), out[:, :, latent_dim:].to(device)\n",
        "        epsilon = torch.randn(qz0_mean.size()).to(device)\n",
        "        z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
        "\n",
        "        # forward in time and solve ode for reconstructions\n",
        "        pred_z = odeint(func, z0, t.to(device)).permute(1, 2, 0, 3)  # [:, -1, :]\n",
        "        pred_x = dec(pred_z).to(device)\n",
        "        pred_x = torch.reshape(pred_x, (train_X.shape[0], train_X.shape[1], lag+forecast)).to(device)\n",
        "\n",
        "        # compute loss\n",
        "        if loss_str == 'mse':\n",
        "            loss = torch.nn.MSELoss()(pred_x.float(), train_X[i, :, :].float().to(device)).float()\n",
        "        elif loss_str == 'elbo':\n",
        "            noise_std_ = torch.zeros(pred_x.size()).to(device) + noise_std\n",
        "            noise_logvar = 2. * torch.log(noise_std_).to(device)\n",
        "            logpx = log_normal_pdf(\n",
        "                train_y.to(device), pred_x, noise_logvar).sum(-1)  # .sum(-1)\n",
        "            pz0_mean = pz0_logvar = torch.zeros(z0.size()).to(device)\n",
        "            analytic_kl = normal_kl(qz0_mean, qz0_logvar,\n",
        "                                    pz0_mean, pz0_logvar).sum(-1)\n",
        "            loss = torch.mean(-logpx + analytic_kl, dim=0).to(device)\n",
        "            # loss = torch.reshape(loss, (1, 1)).to(device)\n",
        "        loss_list.append(loss)\n",
        "            # loss_ = torch.mean(torch.cat([x.float() for x in loss_list])).to(device)  \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # loss_meter.update(loss.item())\n",
        "        if itr%10==0:\n",
        "            print('Iter: {}, running: {:.4f}'.format(itr, loss.item()))\n",
        "    return loss_list\n",
        "\n",
        "def train_loss(h):\n",
        "    train_loss = 0.0\n",
        "    predictions = []\n",
        "    \n",
        "    for t_r in reversed(range(train_X.shape[3])):\n",
        "        obs = train_X[:, :, :, t_r].to(device)\n",
        "        # obs = torch.reshape(obs, (1, 1)).to(device)\n",
        "        out, h = rec.forward(obs, h)\n",
        "\n",
        "    qz0_mean, qz0_logvar = out[:, :, :latent_dim].to(device), out[:, :, latent_dim:].to(device)\n",
        "    epsilon = torch.randn(qz0_mean.size()).to(device)\n",
        "    z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
        "\n",
        "    # forward in time and solve ode for reconstructions\n",
        "    pred_z = odeint(func, z0, t.to(device)).permute(1, 2, 0, 3)\n",
        "    pred_x = dec(pred_z).to(device)\n",
        "    pred_x = torch.reshape(pred_x, (pred_x.shape[0], pred_x.shape[1], lag+forecast))\n",
        "    pred_x_recon = pred_x[:, :, :lag]\n",
        "    pred_x_extrapol = pred_x[:, :, lag:]\n",
        "    rmse = RMSELoss()\n",
        "    # loss = torch.nn.MSELoss()(pred_x, train_X[:, :, :].to(device))\n",
        "    loss_recon = rmse(pred_x_recon, train_y_recon)\n",
        "    loss_extrapol = rmse(pred_x_extrapol, train_y_extrapol)\n",
        "    # train_loss += loss\n",
        "    # predictions.append(pred_x)\n",
        "    \n",
        "    # train_loss = torch.sqrt(train_loss)\n",
        "    # train_pred = torch.cat([x.float() for x in predictions])\n",
        "    # train_pred = torch.reshape(train_pred, (train_pred.shape[0], 1, lag))\n",
        "    with torch.no_grad():\n",
        "        print('Train: Reconstruction Loss')\n",
        "        print('Total Train Loss {:.6f}'.format(loss_recon.item()))\n",
        "        print('Train: Extrapolation Loss')\n",
        "        print('Total Train Extrapolation Loss {:.6f}'.format(loss_extrapol.item()))\n",
        "    return pred_x, pred_x_recon, pred_x_extrapol\n",
        "\n",
        "def test_loss(h, t_test):\n",
        "    # print(h.shape)\n",
        "    test_loss = 0.0\n",
        "    predictions = []\n",
        "    rmse = RMSELoss()\n",
        "    for t_r in reversed(range(test_X.shape[3])):\n",
        "        obs = test_X[:, :, :, t_r].to(device)\n",
        "        # obs = torch.reshape(obs, (1, 1)).to(device)\n",
        "        out, h = rec.forward(obs, h)\n",
        "    qz0_mean, qz0_logvar = out[:, :, :latent_dim].to(device), out[:, :, latent_dim:].to(device)\n",
        "    epsilon = torch.randn(qz0_mean.size()).to(device)\n",
        "    z0 = epsilon * torch.exp(.5 * qz0_logvar) + qz0_mean\n",
        "\n",
        "    # forward in time and solve ode for reconstructions\n",
        "    pred_z = odeint(func, z0, t_test).permute(1, 2, 0, 3).to(device)\n",
        "    pred_x = dec(pred_z).to(device)\n",
        "    pred_x = torch.reshape(pred_x, (pred_x.shape[0], pred_x.shape[1], lag+forecast))\n",
        "    pred_x_recon = pred_x[:, :, :lag]\n",
        "    pred_x_extrapol = pred_x[:, :, lag:]\n",
        "    # pred_test_X = torch.reshape(pred_test_X, (pred_test_X.shape[0], pred_test_X.shape[1]))\n",
        "    # pred_test_y = torch.reshape(pred_test_y, (1, pred_test_y.shape[0], 1, lag))\n",
        "    # print(test_pred_y.shape)\n",
        "    rmse = RMSELoss()\n",
        "    # loss = torch.nn.MSELoss()(pred_x, train_X[i, :, :].to(device))\n",
        "    loss_recon = rmse(pred_x_recon, test_y_recon)\n",
        "    loss_extrapol = rmse(pred_x_extrapol, test_y_extrapol)\n",
        "\n",
        "    # loss = torch.nn.MSELoss()(pred_x, torch.reshape(test_X[i, :, :].to(device), (1, 5))).to(device)\n",
        "    # test_loss += loss\n",
        "    # predictions.append(pred_x)\n",
        "    \n",
        "    # test_loss = torch.sqrt(test_loss)\n",
        "    # test_pred = torch.cat([x.float() for x in predictions])\n",
        "    \n",
        "    # loss = torch.nn.MSELoss()(test_pred_y[train_size-test_size:, batch_time2-2, :], label_batch_y[train_size-test_size:, batch_time2-2, :])\n",
        "    with torch.no_grad():\n",
        "        print('Test: Reconstruction Loss')\n",
        "        print('Total Loss {:.6f}'.format(loss_recon.item()))\n",
        "        print('Test: Extrapolation Loss')\n",
        "        print('Total Loss {:.6f}'.format(loss_extrapol.item()))\n",
        "    return pred_x, pred_x_recon, pred_x_extrapol\n",
        "\n",
        "def plot_train_recon(i, train_y_recon, pred_y_recon, fig, axes):\n",
        "    t_ = torch.linspace(1., train_y_recon.shape[0], train_y_recon.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), train_y_recon.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((train_y_recon.cpu().numpy()[:, :, i-1] - pred_y_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_recon.cpu().numpy()[:, :, i-1], '--', label = f\"recon_week{i}\")\n",
        "    plt.title(f\"Trial No. {trial}: Train Recon: {tickerName}'s Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Train Recon: {tickerName}'s Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_train_extrapol(i, train_y_extrapol, pred_y_extrapol, fig, axes):\n",
        "    t_ = torch.linspace(1., train_y_extrapol.shape[0], train_y_extrapol.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), train_y_extrapol.cpu().numpy()[:, :, i-1-lag], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((train_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_y_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_extrapol.cpu().numpy()[:, :, i-1-lag], '--', label = f\"extrapol_week{i}\")\n",
        "    plt.title(f\"Trial No. {trial}: Train Extrapol: {tickerName}'s Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Train Extrapol: {tickerName}'s Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_test_recon(i, test_y_recon, pred_y_recon, fig, axes):\n",
        "    t_ = torch.linspace(1., test_y_recon.shape[0], test_y_recon.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), test_y_recon.cpu().numpy()[:, :, i-1], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((test_y_recon.cpu().numpy()[:, :, i-1] - pred_y_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_recon.cpu().numpy()[:, :, i-1], '--', label = f\"recon_week{i} : RMSE {rmse}\")\n",
        "    plt.title(f\"Trial No. {trial}: Test Recon: {tickerName}'s Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Test Recon: {tickerName}'s Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_test_extrapol(i, test_y_extrapol, pred_y_extrapol, fig, axes):\n",
        "    t_ = torch.linspace(1., test_y_extrapol.shape[0], test_y_extrapol.shape[0])\n",
        "    plt.figure()\n",
        "    plt.plot(t_.numpy(), test_y_extrapol.cpu().numpy()[:, :, i-1-lag], 'g', label = f\"orig_week{i}\")\n",
        "    with torch.no_grad():\n",
        "        rmse = np.sqrt(((test_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_y_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "        plt.plot(t_.numpy(), pred_y_extrapol.cpu().numpy()[:, :, i-1-lag], '--', label = f\"extrapol_week{i} : RMSE {rmse}\")\n",
        "    plt.title(f\"Trial No. {trial}: Test Extrapol: {tickerName}'s  Median Stock price for week{i}: RMSE {rmse}\")\n",
        "    plt.legend(framealpha=1, frameon=True);\n",
        "    plt.savefig(f\"plots-latentode/Trial No. {trial}: Test Extrapol: {tickerName}'s  Median Stock price for week{i}.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def plot_loss(loss_list):\n",
        "    plt.plot(loss_list)\n",
        "    plt.title(f'{tickerName}: Train Loss (Recon+Extrapol')\n",
        "    plt.ylabel('ELBO Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.savefig(f\"plots-latentode/{tickerName}: Trial No. {trial}: ELBO Train Loss.pdf\", dpi = 150)\n",
        "    plt.show()\n",
        "\n",
        "def rmse_table(recon_rmse_filepath, extrapol_rmse_filepath, train_y_recon, pred_train_recon, train_y_extrapol, pred_train_extrapol, test_y_recon, pred_test_recon, test_y_extrapol, pred_test_extrapol):\n",
        "    recon_rmse_data = {\n",
        "        'week': list(),\n",
        "        'train_recon_rmse': list(),\n",
        "        'test_recon_rmse': list()\n",
        "    }\n",
        "\n",
        "    extrapol_rmse_data = {\n",
        "        'week': list(),\n",
        "        'train_extrapol_rmse': list(),\n",
        "        'test_extrapol_rmse': list()\n",
        "    }\n",
        "    for i in range(1, lag+forecast+1):\n",
        "        with torch.no_grad():\n",
        "            if i<=lag:\n",
        "                recon_rmse_data['week'].append(i)\n",
        "                train_recon_rmse = np.sqrt(((train_y_recon.cpu().numpy()[:, :, i-1] - pred_train_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "                recon_rmse_data['train_recon_rmse'].append(train_recon_rmse)\n",
        "                test_recon_rmse = np.sqrt(((test_y_recon.cpu().numpy()[:, :, i-1] - pred_test_recon.cpu().numpy()[:, :, i-1]) ** 2).mean())\n",
        "                recon_rmse_data['test_recon_rmse'].append(test_recon_rmse)\n",
        "            elif i>lag:\n",
        "                extrapol_rmse_data['week'].append(i)\n",
        "                train_extrapol_rmse = np.sqrt(((train_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_train_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "                extrapol_rmse_data['train_extrapol_rmse'].append(train_extrapol_rmse)\n",
        "                test_extrapol_rmse = np.sqrt(((test_y_extrapol.cpu().numpy()[:, :, i-1-lag] - pred_test_extrapol.cpu().numpy()[:, :, i-1-lag]) ** 2).mean())\n",
        "                extrapol_rmse_data['test_extrapol_rmse'].append(test_extrapol_rmse)\n",
        "    recon_rmse_df = pd.DataFrame(recon_rmse_data)\n",
        "    recon_rmse_df.to_csv(recon_rmse_filepath)\n",
        "    extrapol_rmse_df = pd.DataFrame(extrapol_rmse_data)\n",
        "    extrapol_rmse_df.to_csv(extrapol_rmse_filepath)\n",
        "\n",
        "    return recon_rmse_data, extrapol_rmse_data\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al1KZ0Wb3fvD",
        "outputId": "78d4b539-aa1f-4b0b-a907-ad6122a47056"
      },
      "source": [
        "# 1. Get Open and Close Price of asset (o, c) for each trading day.\n",
        "# libraries\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "print(f\"Get Open and Close Price of Assets\")\n",
        "def download_raw_stock_data(filepath, tickers, start, end, period = '1d'):\n",
        "    \"\"\"\n",
        "    Download Stock tickers\n",
        "    :Parameters:\n",
        "        filepath: str\n",
        "            path to store the raw data\n",
        "        tickers : str, list\n",
        "            List of tickers to download\n",
        "        period: str\n",
        "            the frequency at which to gather the data; common options would include ‘1d’ (daily), ‘1mo’ (monthly), ‘1y’ (yearly)\n",
        "        start: str\n",
        "            the date to start gathering the data. For example ‘2010–1–1’\n",
        "        end: str\n",
        "            the date to end gathering the data. For example ‘2020–1–25’\n",
        "    \n",
        "    \"\"\"\n",
        "    #define the ticker symbol\n",
        "    tickerSymbol = tickers\n",
        "\n",
        "    #get data on this ticker\n",
        "    tickerData = yf.Ticker(tickerSymbol)\n",
        "\n",
        "    #get the historical prices for this ticker\n",
        "    tickerDf = tickerData.history(period=period, start=start, end=end)\n",
        "    tickerDf.to_csv(filepath)\n",
        "\n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "path = f\"raw-stock-data/\"\n",
        "if not os.path.exists(path):\n",
        "    # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "    # Create a new directory\n",
        "    os.makedirs(path)\n",
        "    print(f\"{path} directory is created\")\n",
        "period = '1d'\n",
        "start='1970-1-1'\n",
        "end='2021-8-31'\n",
        "for tickerName, ticker in dict_tickers.items():\n",
        "    tickerName = tickerName\n",
        "    ticker = ticker\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    download_raw_stock_data(filepath, ticker, start, end, period)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(f\"The size of each asset\")\n",
        "import pandas as pd\n",
        "for tickerName in dict_tickers.keys():\n",
        "    df = pd.read_csv(f\"{path}/{tickerName}.csv\")\n",
        "    print(f\"{tickerName} size: {len(df)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get Open and Close Price of Assets\n",
            "\n",
            "\n",
            "The size of each asset\n",
            "Apple size: 10266\n",
            "Microsoft size: 8940\n",
            "Google size: 4288\n",
            "Bitcoin size: 2537\n",
            "Facebook size: 2336\n",
            "Walmart size: 12340\n",
            "Amazon size: 6114\n",
            "CVS size: 12237\n",
            "Berkshire size: 6371\n",
            "ExxonMobil size: 13030\n",
            "AtandT size: 9522\n",
            "Costco size: 8859\n",
            "Walgreens size: 10454\n",
            "Kroger size: 13030\n",
            "JPMorgan size: 10454\n",
            "Verizon size: 9522\n",
            "FordMotor size: 12420\n",
            "GeneralMotors size: 2713\n",
            "Dell size: 1268\n",
            "BankOfAmerica size: 12199\n",
            "Target size: 12238\n",
            "GeneralElectric size: 13031\n",
            "JohnsonandJohnson size: 13032\n",
            "Nvidia size: 5690\n",
            "Intel size: 10453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itQYHJeG33BS",
        "outputId": "d1da6475-7efb-4d27-81a3-8d8b3ad8bb0d"
      },
      "source": [
        "tickers = ['ExxonMobil', 'Kroger', 'GeneralElectric', 'JohnsonandJohnson']\n",
        "train_X_list = list()\n",
        "train_y_list = list()\n",
        "test_X_list = list()\n",
        "test_y_list = list()\n",
        "train_y_recon_list = list()\n",
        "train_y_extrapol_list = list()\n",
        "test_y_recon_list = list()\n",
        "test_y_extrapol_list = list()\n",
        "dataload_dict = {}\n",
        "for tickerName in tickers:\n",
        "    # Filepath\n",
        "    filepath = f\"raw-stock-data/{tickerName}.csv\"\n",
        "    # Get the data in the required format\n",
        "    data = stockDataTransformer(filepath)\n",
        "    print('Data after Data Transformation')\n",
        "    # # Total Data Size\n",
        "    data_size = data.shape[0]\n",
        "\n",
        "    print(pd.DataFrame(data))\n",
        "    print('\\n')\n",
        "\n",
        "    lag = 7\n",
        "    forecast = 3\n",
        "    data_orig = series_to_supervised(data, lag, forecast).values\n",
        "    print('Data Original after series to supervised on data')\n",
        "    print(data_orig.shape)\n",
        "    print(pd.DataFrame(data_orig))\n",
        "    print('\\n')\n",
        "\n",
        "    median_data = get_median(data)\n",
        "    print(f'Median data for {tickerName}')\n",
        "    # Median data for each week\n",
        "    print(median_data.shape)\n",
        "    print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
        "    print('\\n')\n",
        "\n",
        "    # Convert median_data to (n_samples, 5) matrix\n",
        "    data_m1 = series_to_supervised(median_data, lag, forecast).values\n",
        "    print(f'Median data after series to supervised for {tickerName}')\n",
        "    print(data_m1.shape)\n",
        "    print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n",
        "    print('\\n')\n",
        "\n",
        "    dataload = split_data(0.8, 0, lag, data_orig, data_m1, data.shape[1], 1)\n",
        "    dataload_dict[tickerName] = dataload \n",
        "\n",
        "    print(f'Get Train and Test data for {tickerName}')\n",
        "    train_X = torch.from_numpy(dataload['train_X']).to(device)\n",
        "    print(f\"train_X shape: {train_X.shape}\")  # (#training, 1, 5)\n",
        "\n",
        "    train_y = torch.from_numpy(dataload['train_y']).to(device)\n",
        "    print(f\"train_y shape: {train_y.shape}\")  # (#training, 5)\n",
        "    # train_y = torch.reshape(train_y, (train_X.shape[0], 1, train_y.shape[1])).to(device)\n",
        "    # print(f\"train_y shape: {train_y.shape}\")  # (#training, 1, 5)\n",
        "    test_X = torch.from_numpy(dataload['test_X']).to(device)\n",
        "    print(f\"test_X.shape : {test_X.shape}\")  # (#testing, 1, 5)\n",
        "    test_y = torch.from_numpy(dataload['test_y']).to(device)\n",
        "    print(f\"test_y.shape : {test_y.shape}\")  # (#testing, 5)\n",
        "    # test_y = torch.reshape(test_y, (test_X.shape[0], 1, test_y.shape[1])).to(device)\n",
        "    # print(f\"test_y.shape : {test_y.shape}\")\n",
        "\n",
        "    train_y_recon = torch.from_numpy(dataload['train_y_recon']).to(device)\n",
        "    # train_y_recon = torch.reshape(train_y_recon, (train_y_recon.shape[0], 1,train_y_recon.shape[1]))\n",
        "    train_y_extrapol = torch.from_numpy(dataload['train_y_extrapol']).to(device)\n",
        "    # train_y_extrapol = torch.reshape(train_y_extrapol, (train_y_extrapol.shape[0], 1, train_y_extrapol.shape[1]))\n",
        "    test_y_recon = torch.from_numpy(dataload['test_y_recon']).to(device)\n",
        "    # test_y_recon = torch.reshape(test_y_recon, (test_y_recon.shape[0], 1, test_y_recon.shape[1]))\n",
        "    test_y_extrapol = torch.from_numpy(dataload['test_y_extrapol']).to(device)\n",
        "    # test_y_extrapol = torch.reshape(test_y_extrapol, (test_y_extrapol.shape[0], 1, test_y_extrapol.shape[1]))\n",
        "    print(f\"train_y_recon.shape : {train_y_recon.shape}\")\n",
        "    print(f\"train_y_extrapol.shape : {train_y_extrapol.shape}\")\n",
        "    print(f\"test_y_recon.shape : {test_y_recon.shape}\")\n",
        "    print(f\"test_y_extrapol.shape : {test_y_extrapol.shape}\")\n",
        "    print('\\n')\n",
        "\n",
        "    train_X_list.append(train_X) \n",
        "    train_y_list.append(train_y)\n",
        "    test_X_list.append(test_X)\n",
        "    test_y_list.append(test_y)\n",
        "    train_y_recon_list.append(train_y_recon)\n",
        "    train_y_extrapol_list.append(train_y_extrapol)\n",
        "    test_y_recon_list.append(test_y_recon)\n",
        "    test_y_extrapol_list.append(test_y_extrapol)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after Data Transformation\n",
            "              0          1          2  ...          7          8          9\n",
            "0      0.178288   0.179010   0.179010  ...   0.180453   0.180453   0.180814\n",
            "1      0.180814   0.181536   0.181536  ...   0.179731   0.179731   0.177927\n",
            "2      0.177927   0.178649   0.178649  ...   0.177566   0.177566   0.176483\n",
            "3      0.176483   0.173957   0.173957  ...   0.167821   0.166378   0.164212\n",
            "4      0.163852   0.162769   0.162769  ...   0.164573   0.164573   0.164212\n",
            "...         ...        ...        ...  ...        ...        ...        ...\n",
            "2601  57.145067  56.967754  57.135219  ...  56.711628  56.691926  56.721481\n",
            "2602  56.711630  57.332237  56.337292  ...  56.997307  56.278189  56.347145\n",
            "2603  56.563864  57.312534  57.322381  ...  56.770000  56.020000  55.939999\n",
            "2604  55.500000  55.560001  55.419998  ...  52.740002  53.950001  54.910000\n",
            "2605  55.290001  55.360001  55.240002  ...  55.770000  56.240002  55.160000\n",
            "\n",
            "[2606 rows x 10 columns]\n",
            "\n",
            "\n",
            "Data Original after series to supervised on data\n",
            "(2597, 100)\n",
            "             0          1          2   ...         97         98         99\n",
            "0      0.178288   0.179010   0.179010  ...   0.157716   0.157716   0.157716\n",
            "1      0.180814   0.181536   0.181536  ...   0.158799   0.158799   0.159521\n",
            "2      0.177927   0.178649   0.178649  ...   0.165656   0.165656   0.166739\n",
            "3      0.176483   0.173957   0.173957  ...   0.166739   0.166378   0.165656\n",
            "4      0.163852   0.162769   0.162769  ...   0.161686   0.161686   0.161325\n",
            "...         ...        ...        ...  ...        ...        ...        ...\n",
            "2592  58.534045  58.041500  58.199118  ...  56.711628  56.691926  56.721481\n",
            "2593  57.854332  57.499699  58.563597  ...  56.997307  56.278189  56.347145\n",
            "2594  60.415568  60.139740  60.583032  ...  56.770000  56.020000  55.939999\n",
            "2595  61.371105  61.144535  61.548420  ...  52.740002  53.950001  54.910000\n",
            "2596  60.021531  61.656780  61.912903  ...  55.770000  56.240002  55.160000\n",
            "\n",
            "[2597 rows x 100 columns]\n",
            "\n",
            "\n",
            "Median data for ExxonMobil\n",
            "(2606, 1)\n",
            "   median_stockprice_week\n",
            "0                0.180634\n",
            "1                0.179912\n",
            "2                0.177927\n",
            "3                0.173416\n",
            "4                0.164032\n",
            "5                0.156814\n",
            "6                0.148332\n",
            "7                0.158438\n",
            "8                0.154468\n",
            "9                0.157355\n",
            "\n",
            "\n",
            "Median data after series to supervised for ExxonMobil\n",
            "(2597, 10)\n",
            "       week i+1   week i+2   week i+3  ...   week i+8   week i+9  week i+10\n",
            "0      0.180634   0.179912   0.177927  ...   0.158438   0.154468   0.157355\n",
            "1      0.179912   0.177927   0.173416  ...   0.154468   0.157355   0.158257\n",
            "2      0.177927   0.173416   0.164032  ...   0.157355   0.158257   0.163671\n",
            "3      0.173416   0.164032   0.156814  ...   0.158257   0.163671   0.166017\n",
            "4      0.164032   0.156814   0.148332  ...   0.163671   0.166017   0.162408\n",
            "...         ...        ...        ...  ...        ...        ...        ...\n",
            "2592  58.130158  59.745705  61.479465  ...  58.292696  56.283114  57.140143\n",
            "2593  59.745705  61.479465  61.459762  ...  56.283114  57.140143  56.347145\n",
            "2594  61.479465  61.459762  63.144270  ...  57.140143  56.347145  57.171267\n",
            "2595  61.459762  63.144270  62.183804  ...  56.347145  57.171267  54.170000\n",
            "2596  63.144270  62.183804  60.026456  ...  57.171267  54.170000  55.325001\n",
            "\n",
            "[2597 rows x 10 columns]\n",
            "\n",
            "\n",
            "(2077, 70) 2077 (2077, 10)\n",
            "(2077, 10, 7) (2077, 10) (520, 10, 7) (520, 10)\n",
            "Get Train and Test data for ExxonMobil\n",
            "train_X shape: torch.Size([2077, 10, 7])\n",
            "train_y shape: torch.Size([2077, 10])\n",
            "test_X.shape : torch.Size([520, 10, 7])\n",
            "test_y.shape : torch.Size([520, 10])\n",
            "train_y_recon.shape : torch.Size([2077, 7])\n",
            "train_y_extrapol.shape : torch.Size([2077, 3])\n",
            "test_y_recon.shape : torch.Size([520, 7])\n",
            "test_y_extrapol.shape : torch.Size([520, 3])\n",
            "\n",
            "\n",
            "Data after Data Transformation\n",
            "              0          1          2  ...          7          8          9\n",
            "0      0.000000   0.045216   0.000000  ...   0.045216   0.000000   0.045216\n",
            "1      0.000000   0.045216   0.000000  ...   0.044616   0.000000   0.045616\n",
            "2      0.000000   0.045616   0.000000  ...   0.044215   0.000000   0.044015\n",
            "3      0.000000   0.044215   0.000000  ...   0.043527   0.000000   0.042920\n",
            "4      0.000000   0.042717   0.000000  ...   0.043122   0.000000   0.043325\n",
            "...         ...        ...        ...  ...        ...        ...        ...\n",
            "2601  39.496163  39.834503  39.804651  ...  40.501232  40.550991  40.809719\n",
            "2602  40.750011  42.411854  42.292444  ...  41.546104  41.645617  42.073517\n",
            "2603  42.153128  42.571075  42.501416  ...  42.810001  42.860001  43.450001\n",
            "2604  43.750000  45.439999  45.240002  ...  46.939999  47.000000  46.740002\n",
            "2605  46.759998  45.750000  45.750000  ...  45.490002  45.520000  46.200001\n",
            "\n",
            "[2606 rows x 10 columns]\n",
            "\n",
            "\n",
            "Data Original after series to supervised on data\n",
            "(2597, 100)\n",
            "             0          1          2   ...         97         98         99\n",
            "0      0.000000   0.045216   0.000000  ...   0.050815   0.000000   0.051220\n",
            "1      0.000000   0.045216   0.000000  ...   0.050613   0.000000   0.051018\n",
            "2      0.000000   0.045616   0.000000  ...   0.050208   0.000000   0.050006\n",
            "3      0.000000   0.044215   0.000000  ...   0.051018   0.000000   0.050411\n",
            "4      0.000000   0.042717   0.000000  ...   0.051018   0.000000   0.050815\n",
            "...         ...        ...        ...  ...        ...        ...        ...\n",
            "2592  36.102817  36.411304  36.441156  ...  40.501232  40.550991  40.809719\n",
            "2593  36.331694  36.799400  37.117838  ...  41.546104  41.645617  42.073517\n",
            "2594  38.511000  38.779682  38.809537  ...  42.810001  42.860001  43.450001\n",
            "2595  38.680169  38.212467  38.212469  ...  46.939999  47.000000  46.740002\n",
            "2596  38.799585  39.386703  39.217532  ...  45.490002  45.520000  46.200001\n",
            "\n",
            "[2597 rows x 100 columns]\n",
            "\n",
            "\n",
            "Median data for Kroger\n",
            "(2606, 1)\n",
            "   median_stockprice_week\n",
            "0                0.022608\n",
            "1                0.022008\n",
            "2                0.021908\n",
            "3                0.021460\n",
            "4                0.021359\n",
            "5                0.022067\n",
            "6                0.023383\n",
            "7                0.024395\n",
            "8                0.024902\n",
            "9                0.025205\n",
            "\n",
            "\n",
            "Median data after series to supervised for Kroger\n",
            "(2597, 10)\n",
            "       week i+1   week i+2   week i+3  ...   week i+8   week i+9  week i+10\n",
            "0      0.022608   0.022008   0.021908  ...   0.024395   0.024902   0.025205\n",
            "1      0.022008   0.021908   0.021460  ...   0.024902   0.025205   0.025307\n",
            "2      0.021908   0.021460   0.021359  ...   0.025205   0.025307   0.025003\n",
            "3      0.021460   0.021359   0.022067  ...   0.025307   0.025003   0.024800\n",
            "4      0.021359   0.022067   0.023383  ...   0.025003   0.024800   0.024902\n",
            "...         ...        ...        ...  ...        ...        ...        ...\n",
            "2592  36.346622  37.436275  38.605538  ...  38.426417  39.774796  40.292258\n",
            "2593  37.436275  38.605538  38.212468  ...  39.774796  40.292258  41.725225\n",
            "2594  38.605538  38.212468  39.008558  ...  40.292258  41.725225  42.740000\n",
            "2595  38.212468  39.008558  38.421438  ...  41.725225  42.740000  46.025000\n",
            "2596  39.008558  38.421438  37.779590  ...  42.740000  46.025000  45.750000\n",
            "\n",
            "[2597 rows x 10 columns]\n",
            "\n",
            "\n",
            "(2077, 70) 2077 (2077, 10)\n",
            "(2077, 10, 7) (2077, 10) (520, 10, 7) (520, 10)\n",
            "Get Train and Test data for Kroger\n",
            "train_X shape: torch.Size([2077, 10, 7])\n",
            "train_y shape: torch.Size([2077, 10])\n",
            "test_X.shape : torch.Size([520, 10, 7])\n",
            "test_y.shape : torch.Size([520, 10])\n",
            "train_y_recon.shape : torch.Size([2077, 7])\n",
            "train_y_extrapol.shape : torch.Size([2077, 3])\n",
            "test_y_recon.shape : torch.Size([520, 7])\n",
            "test_y_extrapol.shape : torch.Size([520, 3])\n",
            "\n",
            "\n",
            "Data after Data Transformation\n",
            "               0           1           2  ...           7           8           9\n",
            "0       1.288493    1.273946    1.273946  ...    1.236538    1.240694    1.246929\n",
            "1       1.246930    1.215756    1.215757  ...    1.240694    1.240695    1.242773\n",
            "2       1.242773    1.236538    1.236539  ...    1.230304    1.230305    1.232383\n",
            "3       1.232382    1.213678    1.213678  ...    1.232383    1.232382    1.219913\n",
            "4       1.219913    1.203287    1.207444  ...    1.207443    1.182505    1.161723\n",
            "...          ...         ...         ...  ...         ...         ...         ...\n",
            "2601  101.279999  103.360001  106.639999  ...  106.320000  105.279999  103.599998\n",
            "2602  104.480003  100.599998  100.199997  ...  103.019997  103.360001  104.519997\n",
            "2603  103.559998  103.709999  103.809998  ...  106.519997  106.500000  104.919998\n",
            "2604  104.230003  103.349998  102.540001  ...   99.419998   99.589996  100.050003\n",
            "2605  100.599998  100.970001  101.290001  ...  103.379997  103.900002  106.089996\n",
            "\n",
            "[2606 rows x 10 columns]\n",
            "\n",
            "\n",
            "Data Original after series to supervised on data\n",
            "(2597, 100)\n",
            "              0           1           2   ...          97          98          99\n",
            "0       1.288493    1.273946    1.273946  ...    1.197510    1.195413    1.178635\n",
            "1       1.246930    1.215756    1.215757  ...    1.176537    1.176538    1.172343\n",
            "2       1.242773    1.236538    1.236539  ...    1.243649    1.241552    1.233163\n",
            "3       1.232382    1.213678    1.213678  ...    1.245745    1.245746    1.249940\n",
            "4       1.219913    1.203287    1.207444  ...    1.281397    1.281399    1.277204\n",
            "...          ...         ...         ...  ...         ...         ...         ...\n",
            "2592  104.640366  104.400551  104.800245  ...  106.320000  105.279999  103.599998\n",
            "2593  108.717270  114.712708  114.233067  ...  103.019997  103.360001  104.519997\n",
            "2594  113.193859  111.595078  111.914835  ...  106.519997  106.500000  104.919998\n",
            "2595  109.116958  109.436714  110.156165  ...   99.419998   99.589996  100.050003\n",
            "2596  102.641893  102.162254  103.041589  ...  103.379997  103.900002  106.089996\n",
            "\n",
            "[2597 rows x 100 columns]\n",
            "\n",
            "\n",
            "Median data for GeneralElectric\n",
            "(2606, 1)\n",
            "   median_stockprice_week\n",
            "0                1.256281\n",
            "1                1.218874\n",
            "2                1.230304\n",
            "3                1.226147\n",
            "4                1.207443\n",
            "5                1.165879\n",
            "6                1.150293\n",
            "7                1.145097\n",
            "8                1.244697\n",
            "9                1.219531\n",
            "\n",
            "\n",
            "Median data after series to supervised for GeneralElectric\n",
            "(2597, 10)\n",
            "        week i+1    week i+2    week i+3  ...    week i+8    week i+9   week i+10\n",
            "0       1.256281    1.218874    1.230304  ...    1.145097    1.244697    1.219531\n",
            "1       1.218874    1.230304    1.226147  ...    1.244697    1.219531    1.177586\n",
            "2       1.230304    1.226147    1.207443  ...    1.219531    1.177586    1.223725\n",
            "3       1.226147    1.207443    1.165879  ...    1.177586    1.223725    1.247843\n",
            "4       1.207443    1.165879    1.150293  ...    1.223725    1.247843    1.267766\n",
            "...          ...         ...         ...  ...         ...         ...         ...\n",
            "2592  105.120004  112.874100  111.155407  ...  102.760002  101.639999  105.119999\n",
            "2593  112.874100  111.155407  107.797963  ...  101.639999  105.119999  103.009998\n",
            "2594  111.155407  107.797963  104.120763  ...  105.119999  103.009998  106.060001\n",
            "2595  107.797963  104.120763  105.279999  ...  103.009998  106.060001  101.245003\n",
            "2596  104.120763  105.279999  103.959999  ...  106.060001  101.245003  103.369999\n",
            "\n",
            "[2597 rows x 10 columns]\n",
            "\n",
            "\n",
            "(2077, 70) 2077 (2077, 10)\n",
            "(2077, 10, 7) (2077, 10) (520, 10, 7) (520, 10)\n",
            "Get Train and Test data for GeneralElectric\n",
            "train_X shape: torch.Size([2077, 10, 7])\n",
            "train_y shape: torch.Size([2077, 10])\n",
            "test_X.shape : torch.Size([520, 10, 7])\n",
            "test_y.shape : torch.Size([520, 10])\n",
            "train_y_recon.shape : torch.Size([2077, 7])\n",
            "train_y_extrapol.shape : torch.Size([2077, 3])\n",
            "test_y_recon.shape : torch.Size([520, 7])\n",
            "test_y_extrapol.shape : torch.Size([520, 3])\n",
            "\n",
            "\n",
            "Data after Data Transformation\n",
            "               0           1           2  ...           7           8           9\n",
            "0       0.415588    0.405198    0.405198  ...    0.401158    0.401158    0.404044\n",
            "1       0.406353    0.406353    0.406353  ...    0.408662    0.408662    0.409239\n",
            "2       0.409239    0.408662    0.408085  ...    0.405198    0.405198    0.406353\n",
            "3       0.406353    0.404044    0.404044  ...    0.377493    0.376338    0.369411\n",
            "4       0.364794    0.364794    0.368834  ...    0.364217    0.364217    0.380955\n",
            "...          ...         ...         ...  ...         ...         ...         ...\n",
            "2601  169.989866  170.775192  170.805029  ...  171.162888  171.719592  171.162888\n",
            "2602  171.520758  171.182770  171.451181  ...  172.335922  172.823029  172.663971\n",
            "2603  172.912501  172.087402  172.693789  ...  172.773331  172.793217  174.165054\n",
            "2604  174.254511  175.208847  175.507086  ...  176.143295  175.576659  177.515152\n",
            "2605  177.833251  178.380005  178.389999  ...  174.229996  173.949997  173.300003\n",
            "\n",
            "[2606 rows x 10 columns]\n",
            "\n",
            "\n",
            "Data Original after series to supervised on data\n",
            "(2597, 100)\n",
            "              0           1           2   ...          97          98          99\n",
            "0       0.415588    0.405198    0.405198  ...    0.362579    0.360844    0.358531\n",
            "1       0.406353    0.406353    0.406353  ...    0.355640    0.355640    0.355640\n",
            "2       0.409239    0.408662    0.408085  ...    0.362001    0.362001    0.360266\n",
            "3       0.406353    0.404044    0.404044  ...    0.350435    0.350435    0.341183\n",
            "4       0.364794    0.364794    0.368834  ...    0.337713    0.337713    0.335400\n",
            "...          ...         ...         ...  ...         ...         ...         ...\n",
            "2592  167.859047  168.026993  167.888671  ...  171.162888  171.719592  171.162888\n",
            "2593  168.995773  168.071274  168.260129  ...  172.335922  172.823029  172.663971\n",
            "2594  164.910050  165.059158  166.023428  ...  172.773331  172.793217  174.165054\n",
            "2595  165.019396  166.093018  166.083086  ...  176.143295  175.576659  177.515152\n",
            "2596  163.220089  164.244003  162.305536  ...  174.229996  173.949997  173.300003\n",
            "\n",
            "[2597 rows x 100 columns]\n",
            "\n",
            "\n",
            "Median data for JohnsonandJohnson\n",
            "(2606, 1)\n",
            "   median_stockprice_week\n",
            "0                0.402890\n",
            "1                0.406353\n",
            "2                0.405198\n",
            "3                0.400869\n",
            "4                0.372875\n",
            "5                0.395097\n",
            "6                0.371687\n",
            "7                0.360411\n",
            "8                0.367350\n",
            "9                0.361134\n",
            "\n",
            "\n",
            "Median data after series to supervised for JohnsonandJohnson\n",
            "(2597, 10)\n",
            "        week i+1    week i+2    week i+3  ...    week i+8    week i+9   week i+10\n",
            "0       0.402890    0.406353    0.405198  ...    0.360411    0.367350    0.361134\n",
            "1       0.406353    0.405198    0.400869  ...    0.367350    0.361134    0.355062\n",
            "2       0.405198    0.400869    0.372875  ...    0.361134    0.355062    0.357664\n",
            "3       0.400869    0.372875    0.395097  ...    0.355062    0.357664    0.351592\n",
            "4       0.372875    0.395097    0.371687  ...    0.357664    0.351592    0.336557\n",
            "...          ...         ...         ...  ...         ...         ...         ...\n",
            "2592  169.040169  168.160736  164.760933  ...  168.424165  167.400246  171.008804\n",
            "2593  168.160736  164.760933  164.119747  ...  167.400246  171.008804  172.022779\n",
            "2594  164.760933  164.119747  162.688253  ...  171.008804  172.022779  172.758415\n",
            "2595  164.119747  162.688253  163.041158  ...  172.022779  172.758415  176.202944\n",
            "2596  162.688253  163.041158  166.947946  ...  172.758415  176.202944  176.340004\n",
            "\n",
            "[2597 rows x 10 columns]\n",
            "\n",
            "\n",
            "(2077, 70) 2077 (2077, 10)\n",
            "(2077, 10, 7) (2077, 10) (520, 10, 7) (520, 10)\n",
            "Get Train and Test data for JohnsonandJohnson\n",
            "train_X shape: torch.Size([2077, 10, 7])\n",
            "train_y shape: torch.Size([2077, 10])\n",
            "test_X.shape : torch.Size([520, 10, 7])\n",
            "test_y.shape : torch.Size([520, 10])\n",
            "train_y_recon.shape : torch.Size([2077, 7])\n",
            "train_y_extrapol.shape : torch.Size([2077, 3])\n",
            "test_y_recon.shape : torch.Size([520, 7])\n",
            "test_y_extrapol.shape : torch.Size([520, 3])\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK7uEAK-8-B_",
        "outputId": "d2c0495f-e4c4-4620-cdb1-b447fdca7ee2"
      },
      "source": [
        "train_X = torch.stack(train_X_list).to(device)\n",
        "print(f\"train_X shape: {train_X.shape}\")  # (#training, 1, 5)\n",
        "\n",
        "train_y = torch.stack(train_y_list).to(device)\n",
        "print(f\"train_y shape: {train_y.shape}\")  # (#training, 5)\n",
        "\n",
        "test_X = torch.stack(test_X_list).to(device)\n",
        "print(f\"test_X.shape : {test_X.shape}\")  # (#testing, 1, 5)\n",
        "test_y = torch.stack(test_y_list).to(device)\n",
        "print(f\"test_y.shape : {test_y.shape}\")  # (#testing, 5)\n",
        "\n",
        "train_y_recon = torch.stack(train_y_recon_list).to(device)\n",
        "# train_y_recon = torch.reshape(train_y_recon, (train_y_recon.shape[0], 1,train_y_recon.shape[1]))\n",
        "train_y_extrapol = torch.stack(train_y_extrapol_list).to(device)\n",
        "# train_y_extrapol = torch.reshape(train_y_extrapol, (train_y_extrapol.shape[0], 1, train_y_extrapol.shape[1]))\n",
        "test_y_recon = torch.stack(test_y_recon_list).to(device)\n",
        "# test_y_recon = torch.reshape(test_y_recon, (test_y_recon.shape[0], 1, test_y_recon.shape[1]))\n",
        "test_y_extrapol = torch.stack(test_y_extrapol_list).to(device)\n",
        "# test_y_extrapol = torch.reshape(test_y_extrapol, (test_y_extrapol.shape[0], 1, test_y_extrapol.shape[1]))\n",
        "print(f\"train_y_recon.shape : {train_y_recon.shape}\")\n",
        "print(f\"train_y_extrapol.shape : {train_y_extrapol.shape}\")\n",
        "print(f\"test_y_recon.shape : {test_y_recon.shape}\")\n",
        "print(f\"test_y_extrapol.shape : {test_y_extrapol.shape}\")\n",
        "print('\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_X shape: torch.Size([4, 2077, 10, 7])\n",
            "train_y shape: torch.Size([4, 2077, 10])\n",
            "test_X.shape : torch.Size([4, 520, 10, 7])\n",
            "test_y.shape : torch.Size([4, 520, 10])\n",
            "train_y_recon.shape : torch.Size([4, 2077, 7])\n",
            "train_y_extrapol.shape : torch.Size([4, 2077, 3])\n",
            "test_y_recon.shape : torch.Size([4, 520, 7])\n",
            "test_y_extrapol.shape : torch.Size([4, 520, 3])\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOyz6nfr-q6T",
        "outputId": "ac50044a-431f-4ae5-b05d-c1ba376a024e"
      },
      "source": [
        "trial = 1\n",
        "# predictions on test_data's time size (test_size-batch_time, 1)\n",
        "# Time steps\n",
        "t = torch.linspace(0, lag+forecast-1, lag+forecast).to(device)\n",
        "print(f\"t.shape : {t.shape}\")\n",
        "\n",
        "latent_dim = 4\n",
        "nhidden = 20\n",
        "rnn_nhidden = 25\n",
        "obs_dim = data.shape[1]\n",
        "out_dim = 1\n",
        "noise_std = .3\n",
        "\n",
        "lr = 0.001\n",
        "loss_str = 'elbo'\n",
        "# all_values = True\n",
        "niters = 300  # training epochs"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t.shape : torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "UmGzKOGk-17_",
        "outputId": "c5eaca27-213d-40ac-f242-7ddbf68d406f"
      },
      "source": [
        "## Training\n",
        "# model\n",
        "nbatches_train = train_X.shape[1]\n",
        "func = LatentODEfunc(latent_dim, nhidden).to(device)\n",
        "rec = RecognitionRNN(latent_dim, obs_dim, rnn_nhidden, nbatches_train).to(device)\n",
        "dec = Decoder(latent_dim, out_dim, nhidden).to(device)\n",
        "params = (list(func.parameters()) + list(dec.parameters()) + list(rec.parameters()))\n",
        "optimizer = optim.Adam(params, lr=lr)\n",
        "loss_list = train(loss_str, niters)\n",
        "\n",
        "# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html\n",
        "torch.save(func.state_dict(), f\"model-latentode/{tickerName}-trial{trial}-func-moresequences.pth\")\n",
        "torch.save(rec.state_dict(), f\"model-latentode/{tickerName}-trial{trial}-rec-moresequences.pth\")\n",
        "torch.save(dec.state_dict(), f\"model-latentode/{tickerName}-trial{trial}-dec-moresequences.pth\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-17241aa794a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mloss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-a6226622bf09>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(loss_str, niters)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m# loss_ = torch.mean(torch.cat([x.float() for x in loss_list])).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# loss_meter.update(loss.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQjJzA2eewsQ"
      },
      "source": [
        "# FileName\n",
        "tickerName = 'ExxonMobil'\n",
        "# Filepath\n",
        "filepath = f\"raw-stock-data/{tickerName}.csv\"\n",
        "# Get the data in the required format\n",
        "data = stockDataTransformer(filepath)\n",
        "print('Data after Data Transformation')\n",
        "# # Total Data Size\n",
        "data_size = data.shape[0]\n",
        "\n",
        "print(pd.DataFrame(data))\n",
        "print('\\n')\n",
        "\n",
        "lag = 7\n",
        "forecast = 3\n",
        "data_orig = series_to_supervised(data, lag, forecast).values\n",
        "print('Data Original after series to supervised on data')\n",
        "print(data_orig.shape)\n",
        "print(pd.DataFrame(data_orig))\n",
        "print('\\n')\n",
        "\n",
        "median_data = get_median(data)\n",
        "print('Median data')\n",
        "# Median data for each week\n",
        "print(median_data.shape)\n",
        "print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
        "print('\\n')\n",
        "\n",
        "# Convert median_data to (n_samples, 5) matrix\n",
        "data_m1 = series_to_supervised(median_data, lag, forecast).values\n",
        "print('Median data after series to supervised')\n",
        "print(data_m1.shape)\n",
        "print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n",
        "print('\\n')\n",
        "\n",
        "dataload = split_data(0.8, 0, lag, data_orig, data_m1, data.shape[1], 1)\n",
        "\n",
        "print('Get Train and Test data')\n",
        "train_X = torch.from_numpy(dataload['train_X']).to(device)\n",
        "print(f\"train_X shape: {train_X.shape}\")  # (#training, 1, 5)\n",
        "\n",
        "train_y = torch.from_numpy(dataload['train_y']).to(device)\n",
        "print(f\"train_y shape: {train_y.shape}\")  # (#training, 5)\n",
        "train_y = torch.reshape(train_y, (train_X.shape[0], 1, train_y.shape[1])).to(device)\n",
        "print(f\"train_y shape: {train_y.shape}\")  # (#training, 1, 5)\n",
        "test_X = torch.from_numpy(dataload['test_X']).to(device)\n",
        "print(f\"test_X.shape : {test_X.shape}\")  # (#testing, 1, 5)\n",
        "test_y = torch.from_numpy(dataload['test_y']).to(device)\n",
        "print(f\"test_y.shape : {test_y.shape}\")  # (#testing, 5)\n",
        "test_y = torch.reshape(test_y, (test_X.shape[0], 1, test_y.shape[1])).to(device)\n",
        "print(f\"test_y.shape : {test_y.shape}\")\n",
        "\n",
        "train_y_recon = torch.from_numpy(dataload['train_y_recon']).to(device)\n",
        "train_y_recon = torch.reshape(train_y_recon, (train_y_recon.shape[0], 1,train_y_recon.shape[1]))\n",
        "train_y_extrapol = torch.from_numpy(dataload['train_y_extrapol']).to(device)\n",
        "train_y_extrapol = torch.reshape(train_y_extrapol, (train_y_extrapol.shape[0], 1, train_y_extrapol.shape[1]))\n",
        "test_y_recon = torch.from_numpy(dataload['test_y_recon']).to(device)\n",
        "test_y_recon = torch.reshape(test_y_recon, (test_y_recon.shape[0], 1, test_y_recon.shape[1]))\n",
        "test_y_extrapol = torch.from_numpy(dataload['test_y_extrapol']).to(device)\n",
        "test_y_extrapol = torch.reshape(test_y_extrapol, (test_y_extrapol.shape[0], 1, test_y_extrapol.shape[1]))\n",
        "print(f\"train_y_recon.shape : {train_y_recon.shape}\")\n",
        "print(f\"train_y_extrapol.shape : {train_y_extrapol.shape}\")\n",
        "print(f\"test_y_recon.shape : {test_y_recon.shape}\")\n",
        "print(f\"test_y_extrapol.shape : {test_y_extrapol.shape}\")\n",
        "print('\\n')\n",
        "\n",
        "trial = 2\n",
        "# predictions on test_data's time size (test_size-batch_time, 1)\n",
        "# Time steps\n",
        "t = torch.linspace(0, lag+forecast-1, lag+forecast).to(device)\n",
        "print(f\"t.shape : {t.shape}\")\n",
        "\n",
        "latent_dim = 50\n",
        "nhidden = 100\n",
        "rnn_nhidden = 125\n",
        "obs_dim = data.shape[1]\n",
        "out_dim = 1\n",
        "noise_std = .3\n",
        "\n",
        "lr = 0.001\n",
        "loss_str = 'elbo'\n",
        "# all_values = True\n",
        "niters = 2000  # training epochs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}