{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVJodaa_8mQs"
      },
      "source": [
        "# Import this (important)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhfJy5RA8pHw",
        "outputId": "4ac85703-72dc-4985-a314-30232fe5e657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'torchdiffeq'...\n",
            "remote: Enumerating objects: 1138, done.\u001b[K\n",
            "remote: Counting objects: 100% (434/434), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 1138 (delta 256), reused 400 (delta 240), pack-reused 704\u001b[K\n",
            "Receiving objects: 100% (1138/1138), 8.29 MiB | 8.71 MiB/s, done.\n",
            "Resolving deltas: 100% (682/682), done.\n",
            "Obtaining file:///content/torchdiffeq\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu111)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "  Running setup.py develop for torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n",
            "_impl  __init__.py\n"
          ]
        }
      ],
      "source": [
        "# Install the latest version of author's repo neural ode implementation\n",
        "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "!cd torchdiffeq && pip install -e .\n",
        "!ls torchdiffeq/torchdiffeq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnIz6ghoAA9H"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf82UlO58phm"
      },
      "source": [
        "# Rough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4d8700-2uyv"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8U0VQjD2hH8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import SystemRandom\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fDkAwSb2-N2"
      },
      "outputs": [],
      "source": [
        "import lib.utils as utils\n",
        "from lib.plotting import *\n",
        "\n",
        "from lib.rnn_baselines import *\n",
        "from lib.ode_rnn import *\n",
        "from lib.create_latent_ode_model import create_LatentODE_model\n",
        "from lib.parse_datasets import parse_datasets\n",
        "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
        "from lib.diffeq_solver import DiffeqSolver\n",
        "from mujoco_physics import HopperPhysics\n",
        "\n",
        "from lib.utils import compute_loss_all_batches\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import lib.utils as utils\n",
        "from lib.diffeq_solver import DiffeqSolver\n",
        "from generate_timeseries import Periodic_1d\n",
        "from torch.distributions import uniform\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from mujoco_physics import HopperPhysics\n",
        "from physionet import PhysioNet, variable_time_collate_fn, get_data_min_max\n",
        "from person_activity import PersonActivity, variable_time_collate_fn_activity\n",
        "\n",
        "from sklearn import model_selection\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2w5cfx1I8uz2"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NljGxF4I3YGF"
      },
      "outputs": [],
      "source": [
        "n = 1000  # size of dataset\n",
        "niters = 500  # number of iterations\n",
        "lr = 1e-3  # Starting Learning Rate\n",
        "b = 50  # batch_size\n",
        "viz = True  # show plots while training\n",
        "# save: path for saving checkpoints\n",
        "# load: ID of the experiment to load for evaluation\n",
        "random_seed = 1991 # random seed\n",
        "# dataset = Dataset to load. Available: physionet, activity, hopper, periodic\n",
        "sample_tp = None # number of time points to sub-sample.  If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\n",
        "c = None # Cut out the section of the timeline of the specified length (in number of points\n",
        "# quantization: Quantization on the physionet dataset.\" \"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\n",
        "latent_ode = None # latent ode with ode-rnn encoder\n",
        "# z0-encoder = odernn # Type of encoder for Latent ODE model: odernn or rnn\n",
        "# classic-rnn\n",
        "# rnn-cell\n",
        "# input-decay\n",
        "# ode-rnn # Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\n",
        "# rnn-vae  # Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\n",
        "latents = 10 # size of the latent state\n",
        "rec_dims = 20 # dimensionality of the recognition model (ODE or RNN)\n",
        "rec_layers = 1 # Number of layers in ODE func in recognition ODE\n",
        "gen_layers = 1 # Number of layers in ODE func in generative ODE\n",
        "units = 100 # Number of units per layer in ODE func\n",
        "gru_units = 100 # Number of units per layer in each of GRU update networks\n",
        "poisson = 'store_true' # Model poisson-process likelihood for the density of events in addition to reconstruction\n",
        "classif = 'store_true' # Include binary classification loss -- used for Physionet dataset for hospiral mortality\n",
        "linear_classif = 'store_true' # If using a classifier, use a linear classifier instead of 1-layer NN\n",
        "extrap = 'store_true' # Set extrapolation mode. If this flag is not set, run interpolation mode\n",
        "timepoints = 100 # total number of timepoints\n",
        "max_tp = 5. # We subsample points in the interval [0, args.max_tp]\n",
        "noise_weight = 0.01 # Noise amplitude for generated traejctories\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4nhr13w7k3h",
        "outputId": "48146850-bf94-433b-fab8-049704dcebb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f03000d14b0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkVaoUde_8Py"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb7DmLfLASpy"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import uniform\n",
        "n_total_tp = 100\n",
        "max_t_extrap = max_tp/timepoints * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj5lZq4Y7x2V",
        "outputId": "29a9f073-c73a-457c-e6d2-8cf9473a1999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([3.7851, 3.3443, 0.1417, 1.3734, 3.0917, 0.3541, 2.0763, 0.4173, 3.6215,\n",
            "        3.9683, 3.6055, 3.8517, 1.5262, 1.1137, 3.9579, 1.0563, 3.9331, 4.4630,\n",
            "        3.0525, 4.4004, 2.8379, 1.0336, 0.1935, 2.1825, 0.0623, 0.8038, 0.1491,\n",
            "        2.8308, 4.4699, 4.7802, 0.0876, 2.9323, 0.6113, 4.8144, 0.1380, 0.2273,\n",
            "        0.5352, 1.8769, 0.4140, 0.9900, 1.5855, 2.3460, 0.2538, 1.5439, 4.2902,\n",
            "        3.2616, 2.5595, 4.6672, 3.3973, 2.9913, 1.8209, 4.3530, 3.2576, 3.6419,\n",
            "        2.3678, 1.2531, 1.5027, 0.8094, 2.2725, 3.1943, 0.9322, 0.1816, 3.5082,\n",
            "        1.3179, 3.5005, 3.7536, 4.8195, 0.2850, 4.3183, 3.9178, 1.1724, 2.6610,\n",
            "        1.8388, 0.0661, 2.3603, 1.7746, 4.1492, 0.1078, 4.0930, 2.2240, 2.0680,\n",
            "        1.2015, 3.2396, 1.0108, 2.0369, 0.2006, 1.9468, 0.9870, 0.1718, 2.8426,\n",
            "        3.3702, 4.5436, 1.2397, 1.8489, 4.0469, 2.2679, 2.1030, 1.8386, 1.4328])\n",
            "torch.Size([99])\n",
            "torch.Size([100])\n",
            "torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "distribution = uniform.Uniform(torch.Tensor([0.0]),torch.Tensor([max_t_extrap]))\n",
        "time_steps_extrap =  distribution.sample(torch.Size([n_total_tp-1]))[:,0]\n",
        "print(time_steps_extrap)\n",
        "print(time_steps_extrap.shape)\n",
        "time_steps_extrap = torch.cat((torch.Tensor([0.0]), time_steps_extrap))\n",
        "print(time_steps_extrap.shape)\n",
        "time_steps_extrap = torch.sort(time_steps_extrap)[0]\n",
        "print(time_steps_extrap.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8UO_7BmEDal"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKVYI0ng-B6d"
      },
      "source": [
        "# Run Model of the author"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "eBYSErWB-E6H",
        "outputId": "4ba946bc-21c4-483f-a64c-4a8f09aca5a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: Latent ODE [-h] [-n N] [--niters NITERS] [--lr LR] [-b BATCH_SIZE]\n",
            "                  [--viz] [--save SAVE] [--load LOAD] [-r RANDOM_SEED]\n",
            "                  [--dataset DATASET] [-s SAMPLE_TP] [-c CUT_TP]\n",
            "                  [--quantization QUANTIZATION] [--latent-ode]\n",
            "                  [--z0-encoder Z0_ENCODER] [--classic-rnn]\n",
            "                  [--rnn-cell RNN_CELL] [--input-decay] [--ode-rnn]\n",
            "                  [--rnn-vae] [-l LATENTS] [--rec-dims REC_DIMS]\n",
            "                  [--rec-layers REC_LAYERS] [--gen-layers GEN_LAYERS]\n",
            "                  [-u UNITS] [-g GRU_UNITS] [--poisson] [--classif]\n",
            "                  [--linear-classif] [--extrap] [-t TIMEPOINTS]\n",
            "                  [--max-t MAX_T] [--noise-weight NOISE_WEIGHT]\n",
            "Latent ODE: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-b3795be3-3477-4bee-97ed-b38edd553d61.json\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "###########################\n",
        "# Latent ODEs for Irregularly-Sampled Time Series\n",
        "# Author: Yulia Rubanova\n",
        "###########################\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import SystemRandom\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "import torch.optim as optim\n",
        "\n",
        "import lib.utils as utils\n",
        "from lib.plotting import *\n",
        "\n",
        "from lib.rnn_baselines import *\n",
        "from lib.ode_rnn import *\n",
        "from lib.create_latent_ode_model import create_LatentODE_model\n",
        "from lib.parse_datasets import parse_datasets\n",
        "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
        "from lib.diffeq_solver import DiffeqSolver\n",
        "from mujoco_physics import HopperPhysics\n",
        "\n",
        "from lib.utils import compute_loss_all_batches\n",
        "\n",
        "# Generative model for noisy data based on ODE\n",
        "parser = argparse.ArgumentParser('Latent ODE')\n",
        "parser.add_argument('-n',  type=int, default=100, help=\"Size of the dataset\")\n",
        "parser.add_argument('--niters', type=int, default=300)\n",
        "parser.add_argument('--lr',  type=float, default=1e-2, help=\"Starting learning rate.\")\n",
        "parser.add_argument('-b', '--batch-size', type=int, default=50)\n",
        "parser.add_argument('--viz', action='store_true', help=\"Show plots while training\")\n",
        "\n",
        "parser.add_argument('--save', type=str, default='experiments/', help=\"Path for save checkpoints\")\n",
        "parser.add_argument('--load', type=str, default=None, help=\"ID of the experiment to load for evaluation. If None, run a new experiment.\")\n",
        "parser.add_argument('-r', '--random-seed', type=int, default=1991, help=\"Random_seed\")\n",
        "\n",
        "parser.add_argument('--dataset', type=str, default='periodic', help=\"Dataset to load. Available: physionet, activity, hopper, periodic\")\n",
        "parser.add_argument('-s', '--sample-tp', type=float, default=None, help=\"Number of time points to sub-sample.\"\n",
        "\t\"If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\")\n",
        "\n",
        "parser.add_argument('-c', '--cut-tp', type=int, default=None, help=\"Cut out the section of the timeline of the specified length (in number of points).\"\n",
        "\t\"Used for periodic function demo.\")\n",
        "\n",
        "parser.add_argument('--quantization', type=float, default=0.1, help=\"Quantization on the physionet dataset.\"\n",
        "\t\"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\")\n",
        "\n",
        "parser.add_argument('--latent-ode', action='store_true', help=\"Run Latent ODE seq2seq model\")\n",
        "parser.add_argument('--z0-encoder', type=str, default='odernn', help=\"Type of encoder for Latent ODE model: odernn or rnn\")\n",
        "\n",
        "parser.add_argument('--classic-rnn', action='store_true', help=\"Run RNN baseline: classic RNN that sees true points at every point. Used for interpolation only.\")\n",
        "parser.add_argument('--rnn-cell', default=\"gru\", help=\"RNN Cell type. Available: gru (default), expdecay\")\n",
        "parser.add_argument('--input-decay', action='store_true', help=\"For RNN: use the input that is the weighted average of impirical mean and previous value (like in GRU-D)\")\n",
        "\n",
        "parser.add_argument('--ode-rnn', action='store_true', help=\"Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\")\n",
        "\n",
        "parser.add_argument('--rnn-vae', action='store_true', help=\"Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\")\n",
        "\n",
        "parser.add_argument('-l', '--latents', type=int, default=6, help=\"Size of the latent state\")\n",
        "parser.add_argument('--rec-dims', type=int, default=20, help=\"Dimensionality of the recognition model (ODE or RNN).\")\n",
        "\n",
        "parser.add_argument('--rec-layers', type=int, default=1, help=\"Number of layers in ODE func in recognition ODE\")\n",
        "parser.add_argument('--gen-layers', type=int, default=1, help=\"Number of layers in ODE func in generative ODE\")\n",
        "\n",
        "parser.add_argument('-u', '--units', type=int, default=100, help=\"Number of units per layer in ODE func\")\n",
        "parser.add_argument('-g', '--gru-units', type=int, default=100, help=\"Number of units per layer in each of GRU update networks\")\n",
        "\n",
        "parser.add_argument('--poisson', action='store_true', help=\"Model poisson-process likelihood for the density of events in addition to reconstruction.\")\n",
        "parser.add_argument('--classif', action='store_true', help=\"Include binary classification loss -- used for Physionet dataset for hospiral mortality\")\n",
        "\n",
        "parser.add_argument('--linear-classif', action='store_true', help=\"If using a classifier, use a linear classifier instead of 1-layer NN\")\n",
        "parser.add_argument('--extrap', action='store_true', help=\"Set extrapolation mode. If this flag is not set, run interpolation mode.\")\n",
        "\n",
        "parser.add_argument('-t', '--timepoints', type=int, default=100, help=\"Total number of time-points\")\n",
        "parser.add_argument('--max-t',  type=float, default=5., help=\"We subsample points in the interval [0, args.max_tp]\")\n",
        "parser.add_argument('--noise-weight', type=float, default=0.01, help=\"Noise amplitude for generated traejctories\")\n",
        "\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "file_name = os.path.basename(__file__)[:-3]\n",
        "utils.makedirs(args.save)\n",
        "\n",
        "#####################################################################################################\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\ttorch.manual_seed(args.random_seed)\n",
        "\tnp.random.seed(args.random_seed)\n",
        "\n",
        "\texperimentID = args.load\n",
        "\tif experimentID is None:\n",
        "\t\t# Make a new experiment ID\n",
        "\t\texperimentID = int(SystemRandom().random()*100000)\n",
        "\tckpt_path = os.path.join(args.save, \"experiment_\" + str(experimentID) + '.ckpt')\n",
        "\n",
        "\tstart = time.time()\n",
        "\tprint(\"Sampling dataset of {} training examples\".format(args.n))\n",
        "\t\n",
        "\tinput_command = sys.argv\n",
        "\tind = [i for i in range(len(input_command)) if input_command[i] == \"--load\"]\n",
        "\tif len(ind) == 1:\n",
        "\t\tind = ind[0]\n",
        "\t\tinput_command = input_command[:ind] + input_command[(ind+2):]\n",
        "\tinput_command = \" \".join(input_command)\n",
        "\n",
        "\tutils.makedirs(\"results/\")\n",
        "\n",
        "\t##################################################################\n",
        "\tdata_obj = parse_datasets(args, device)\n",
        "\tinput_dim = data_obj[\"input_dim\"]\n",
        "\n",
        "\tclassif_per_tp = False\n",
        "\tif (\"classif_per_tp\" in data_obj):\n",
        "\t\t# do classification per time point rather than on a time series as a whole\n",
        "\t\tclassif_per_tp = data_obj[\"classif_per_tp\"]\n",
        "\n",
        "\tif args.classif and (args.dataset == \"hopper\" or args.dataset == \"periodic\"):\n",
        "\t\traise Exception(\"Classification task is not available for MuJoCo and 1d datasets\")\n",
        "\n",
        "\tn_labels = 1\n",
        "\tif args.classif:\n",
        "\t\tif (\"n_labels\" in data_obj):\n",
        "\t\t\tn_labels = data_obj[\"n_labels\"]\n",
        "\t\telse:\n",
        "\t\t\traise Exception(\"Please provide number of labels for classification task\")\n",
        "\n",
        "\t##################################################################\n",
        "\t# Create the model\n",
        "\tobsrv_std = 0.01\n",
        "\tif args.dataset == \"hopper\":\n",
        "\t\tobsrv_std = 1e-3 \n",
        "\n",
        "\tobsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
        "\n",
        "\tz0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
        "\n",
        "\tif args.rnn_vae:\n",
        "\t\tif args.poisson:\n",
        "\t\t\tprint(\"Poisson process likelihood not implemented for RNN-VAE: ignoring --poisson\")\n",
        "\n",
        "\t\t# Create RNN-VAE model\n",
        "\t\tmodel = RNN_VAE(input_dim, args.latents, \n",
        "\t\t\tdevice = device, \n",
        "\t\t\trec_dims = args.rec_dims, \n",
        "\t\t\tconcat_mask = True, \n",
        "\t\t\tobsrv_std = obsrv_std,\n",
        "\t\t\tz0_prior = z0_prior,\n",
        "\t\t\tuse_binary_classif = args.classif,\n",
        "\t\t\tclassif_per_tp = classif_per_tp,\n",
        "\t\t\tlinear_classifier = args.linear_classif,\n",
        "\t\t\tn_units = args.units,\n",
        "\t\t\tinput_space_decay = args.input_decay,\n",
        "\t\t\tcell = args.rnn_cell,\n",
        "\t\t\tn_labels = n_labels,\n",
        "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
        "\t\t\t).to(device)\n",
        "\n",
        "\n",
        "\telif args.classic_rnn:\n",
        "\t\tif args.poisson:\n",
        "\t\t\tprint(\"Poisson process likelihood not implemented for RNN: ignoring --poisson\")\n",
        "\n",
        "\t\tif args.extrap:\n",
        "\t\t\traise Exception(\"Extrapolation for standard RNN not implemented\")\n",
        "\t\t# Create RNN model\n",
        "\t\tmodel = Classic_RNN(input_dim, args.latents, device, \n",
        "\t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
        "\t\t\tn_units = args.units,\n",
        "\t\t\tuse_binary_classif = args.classif,\n",
        "\t\t\tclassif_per_tp = classif_per_tp,\n",
        "\t\t\tlinear_classifier = args.linear_classif,\n",
        "\t\t\tinput_space_decay = args.input_decay,\n",
        "\t\t\tcell = args.rnn_cell,\n",
        "\t\t\tn_labels = n_labels,\n",
        "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
        "\t\t\t).to(device)\n",
        "\telif args.ode_rnn:\n",
        "\t\t# Create ODE-GRU model\n",
        "\t\tn_ode_gru_dims = args.latents\n",
        "\t\t\t\t\n",
        "\t\tif args.poisson:\n",
        "\t\t\tprint(\"Poisson process likelihood not implemented for ODE-RNN: ignoring --poisson\")\n",
        "\n",
        "\t\tif args.extrap:\n",
        "\t\t\traise Exception(\"Extrapolation for ODE-RNN not implemented\")\n",
        "\n",
        "\t\tode_func_net = utils.create_net(n_ode_gru_dims, n_ode_gru_dims, \n",
        "\t\t\tn_layers = args.rec_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
        "\n",
        "\t\trec_ode_func = ODEFunc(\n",
        "\t\t\tinput_dim = input_dim, \n",
        "\t\t\tlatent_dim = n_ode_gru_dims,\n",
        "\t\t\tode_func_net = ode_func_net,\n",
        "\t\t\tdevice = device).to(device)\n",
        "\n",
        "\t\tz0_diffeq_solver = DiffeqSolver(input_dim, rec_ode_func, \"euler\", args.latents, \n",
        "\t\t\todeint_rtol = 1e-3, odeint_atol = 1e-4, device = device)\n",
        "\t\n",
        "\t\tmodel = ODE_RNN(input_dim, n_ode_gru_dims, device = device, \n",
        "\t\t\tz0_diffeq_solver = z0_diffeq_solver, n_gru_units = args.gru_units,\n",
        "\t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
        "\t\t\tuse_binary_classif = args.classif,\n",
        "\t\t\tclassif_per_tp = classif_per_tp,\n",
        "\t\t\tn_labels = n_labels,\n",
        "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
        "\t\t\t).to(device)\n",
        "\telif args.latent_ode:\n",
        "\t\tmodel = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
        "\t\t\tclassif_per_tp = classif_per_tp,\n",
        "\t\t\tn_labels = n_labels)\n",
        "\telse:\n",
        "\t\traise Exception(\"Model not specified\")\n",
        "\n",
        "\t##################################################################\n",
        "\n",
        "\tif args.viz:\n",
        "\t\tviz = Visualizations(device)\n",
        "\n",
        "\t##################################################################\n",
        "\t\n",
        "\t#Load checkpoint and evaluate the model\n",
        "\tif args.load is not None:\n",
        "\t\tutils.get_ckpt_model(ckpt_path, model, device)\n",
        "\t\texit()\n",
        "\n",
        "\t##################################################################\n",
        "\t# Training\n",
        "\n",
        "\tlog_path = \"logs/\" + file_name + \"_\" + str(experimentID) + \".log\"\n",
        "\tif not os.path.exists(\"logs/\"):\n",
        "\t\tutils.makedirs(\"logs/\")\n",
        "\tlogger = utils.get_logger(logpath=log_path, filepath=os.path.abspath(__file__))\n",
        "\tlogger.info(input_command)\n",
        "\n",
        "\toptimizer = optim.Adamax(model.parameters(), lr=args.lr)\n",
        "\n",
        "\tnum_batches = data_obj[\"n_train_batches\"]\n",
        "\n",
        "\tfor itr in range(1, num_batches * (args.niters + 1)):\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tutils.update_learning_rate(optimizer, decay_rate = 0.999, lowest = args.lr / 10)\n",
        "\n",
        "\t\twait_until_kl_inc = 10\n",
        "\t\tif itr // num_batches < wait_until_kl_inc:\n",
        "\t\t\tkl_coef = 0.\n",
        "\t\telse:\n",
        "\t\t\tkl_coef = (1-0.99** (itr // num_batches - wait_until_kl_inc))\n",
        "\n",
        "\t\tbatch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
        "\t\ttrain_res = model.compute_all_losses(batch_dict, n_traj_samples = 3, kl_coef = kl_coef)\n",
        "\t\ttrain_res[\"loss\"].backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\tn_iters_to_viz = 1\n",
        "\t\tif itr % (n_iters_to_viz * num_batches) == 0:\n",
        "\t\t\twith torch.no_grad():\n",
        "\n",
        "\t\t\t\ttest_res = compute_loss_all_batches(model, \n",
        "\t\t\t\t\tdata_obj[\"test_dataloader\"], args,\n",
        "\t\t\t\t\tn_batches = data_obj[\"n_test_batches\"],\n",
        "\t\t\t\t\texperimentID = experimentID,\n",
        "\t\t\t\t\tdevice = device,\n",
        "\t\t\t\t\tn_traj_samples = 3, kl_coef = kl_coef)\n",
        "\n",
        "\t\t\t\tmessage = 'Epoch {:04d} [Test seq (cond on sampled tp)] | Loss {:.6f} | Likelihood {:.6f} | KL fp {:.4f} | FP STD {:.4f}|'.format(\n",
        "\t\t\t\t\titr//num_batches, \n",
        "\t\t\t\t\ttest_res[\"loss\"].detach(), test_res[\"likelihood\"].detach(), \n",
        "\t\t\t\t\ttest_res[\"kl_first_p\"], test_res[\"std_first_p\"])\n",
        "\t\t \t\n",
        "\t\t\t\tlogger.info(\"Experiment \" + str(experimentID))\n",
        "\t\t\t\tlogger.info(message)\n",
        "\t\t\t\tlogger.info(\"KL coef: {}\".format(kl_coef))\n",
        "\t\t\t\tlogger.info(\"Train loss (one batch): {}\".format(train_res[\"loss\"].detach()))\n",
        "\t\t\t\tlogger.info(\"Train CE loss (one batch): {}\".format(train_res[\"ce_loss\"].detach()))\n",
        "\t\t\t\t\n",
        "\t\t\t\tif \"auc\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Classification AUC (TEST): {:.4f}\".format(test_res[\"auc\"]))\n",
        "\n",
        "\t\t\t\tif \"mse\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Test MSE: {:.4f}\".format(test_res[\"mse\"]))\n",
        "\n",
        "\t\t\t\tif \"accuracy\" in train_res:\n",
        "\t\t\t\t\tlogger.info(\"Classification accuracy (TRAIN): {:.4f}\".format(train_res[\"accuracy\"]))\n",
        "\n",
        "\t\t\t\tif \"accuracy\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Classification accuracy (TEST): {:.4f}\".format(test_res[\"accuracy\"]))\n",
        "\n",
        "\t\t\t\tif \"pois_likelihood\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"Poisson likelihood: {}\".format(test_res[\"pois_likelihood\"]))\n",
        "\n",
        "\t\t\t\tif \"ce_loss\" in test_res:\n",
        "\t\t\t\t\tlogger.info(\"CE loss: {}\".format(test_res[\"ce_loss\"]))\n",
        "\n",
        "\t\t\ttorch.save({\n",
        "\t\t\t\t'args': args,\n",
        "\t\t\t\t'state_dict': model.state_dict(),\n",
        "\t\t\t}, ckpt_path)\n",
        "\n",
        "\n",
        "\t\t\t# Plotting\n",
        "\t\t\tif args.viz:\n",
        "\t\t\t\twith torch.no_grad():\n",
        "\t\t\t\t\ttest_dict = utils.get_next_batch(data_obj[\"test_dataloader\"])\n",
        "\n",
        "\t\t\t\t\tprint(\"plotting....\")\n",
        "\t\t\t\t\tif isinstance(model, LatentODE) and (args.dataset == \"periodic\"): #and not args.classic_rnn and not args.ode_rnn:\n",
        "\t\t\t\t\t\tplot_id = itr // num_batches // n_iters_to_viz\n",
        "\t\t\t\t\t\tviz.draw_all_plots_one_dim(test_dict, model, \n",
        "\t\t\t\t\t\t\tplot_name = file_name + \"_\" + str(experimentID) + \"_{:03d}\".format(plot_id) + \".png\",\n",
        "\t\t\t\t\t\t \texperimentID = experimentID, save=True)\n",
        "\t\t\t\t\t\tplt.pause(0.01)\n",
        "\ttorch.save({\n",
        "\t\t'args': args,\n",
        "\t\t'state_dict': model.state_dict(),\n",
        "\t}, ckpt_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG4mctCz-Fv7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juPmjjMF_7oA"
      },
      "source": [
        "# Break it down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0fP80LJ_928",
        "outputId": "54ffaba5-1862-46ae-aed0-990773a9f517"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'torchdiffeq' already exists and is not an empty directory.\n",
            "Obtaining file:///content/torchdiffeq\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu111)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "  Attempting uninstall: torchdiffeq\n",
            "    Found existing installation: torchdiffeq 0.2.2\n",
            "    Can't uninstall 'torchdiffeq'. No files were found to uninstall.\n",
            "  Running setup.py develop for torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n",
            "_impl  __init__.py\n"
          ]
        }
      ],
      "source": [
        "# Install the latest version of author's repo neural ode implementation\n",
        "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "!cd torchdiffeq && pip install -e .\n",
        "!ls torchdiffeq/torchdiffeq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFjl3iKBAECB"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "mHuQH-oTADRa"
      },
      "outputs": [],
      "source": [
        "# run_models.py\n",
        "import os\n",
        "import sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import SystemRandom\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "import torch.optim as optim\n",
        "\n",
        "import lib.utils as utils\n",
        "from lib.plotting import *\n",
        "\n",
        "from lib.rnn_baselines import *\n",
        "from lib.ode_rnn import *\n",
        "from lib.create_latent_ode_model import create_LatentODE_model\n",
        "from lib.parse_datasets import parse_datasets\n",
        "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
        "from lib.diffeq_solver import DiffeqSolver\n",
        "from mujoco_physics import HopperPhysics\n",
        "\n",
        "from lib.utils import compute_loss_all_batches\n",
        "\n",
        "import sys\n",
        "# print(sys.argv[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNCXSGE0OSjP"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "xaYoHqr9AX-P"
      },
      "outputs": [],
      "source": [
        "# Generative model for noisy data based on ODE\n",
        "parser = argparse.ArgumentParser('Latent ODE')\n",
        "parser.add_argument('-n',  type=int, default=100, help=\"Size of the dataset\")\n",
        "parser.add_argument('--niters', type=int, default=300)\n",
        "parser.add_argument('--lr',  type=float, default=1e-2, help=\"Starting learning rate.\")\n",
        "parser.add_argument('-b', '--batch-size', type=int, default=50)\n",
        "parser.add_argument('--viz', action='store_true', help=\"Show plots while training\")\n",
        "\n",
        "parser.add_argument('--save', type=str, default='experiments/', help=\"Path for save checkpoints\")\n",
        "parser.add_argument('--load', type=str, default=None, help=\"ID of the experiment to load for evaluation. If None, run a new experiment.\")\n",
        "parser.add_argument('-r', '--random-seed', type=int, default=1991, help=\"Random_seed\")\n",
        "\n",
        "parser.add_argument('--dataset', type=str, default='hopper', help=\"Dataset to load. Available: physionet, activity, hopper, periodic\")\n",
        "parser.add_argument('-s', '--sample-tp', type=float, default=None, help=\"Number of time points to sub-sample.\"\n",
        "\t\"If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\")\n",
        "\n",
        "parser.add_argument('-c', '--cut-tp', type=int, default=None, help=\"Cut out the section of the timeline of the specified length (in number of points).\"\n",
        "\t\"Used for periodic function demo.\")\n",
        "\n",
        "parser.add_argument('--quantization', type=float, default=0.1, help=\"Quantization on the physionet dataset.\"\n",
        "\t\"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\")\n",
        "\n",
        "parser.add_argument('--latent-ode', default = True, action='store_true', help=\"Run Latent ODE seq2seq model\")\n",
        "parser.add_argument('--z0-encoder', type=str, default='odernn', help=\"Type of encoder for Latent ODE model: odernn or rnn\")\n",
        "\n",
        "parser.add_argument('--classic-rnn', action='store_true', help=\"Run RNN baseline: classic RNN that sees true points at every point. Used for interpolation only.\")\n",
        "parser.add_argument('--rnn-cell', default=\"gru\", help=\"RNN Cell type. Available: gru (default), expdecay\")\n",
        "parser.add_argument('--input-decay', action='store_true', help=\"For RNN: use the input that is the weighted average of impirical mean and previous value (like in GRU-D)\")\n",
        "\n",
        "parser.add_argument('--ode-rnn', action='store_true', help=\"Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\")\n",
        "\n",
        "parser.add_argument('--rnn-vae', action='store_true', help=\"Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\")\n",
        "\n",
        "parser.add_argument('-l', '--latents', type=int, default=6, help=\"Size of the latent state\")\n",
        "parser.add_argument('--rec-dims', type=int, default=20, help=\"Dimensionality of the recognition model (ODE or RNN).\")\n",
        "\n",
        "parser.add_argument('--rec-layers', type=int, default=1, help=\"Number of layers in ODE func in recognition ODE\")\n",
        "parser.add_argument('--gen-layers', type=int, default=1, help=\"Number of layers in ODE func in generative ODE\")\n",
        "\n",
        "parser.add_argument('-u', '--units', type=int, default=100, help=\"Number of units per layer in ODE func\")\n",
        "parser.add_argument('-g', '--gru-units', type=int, default=100, help=\"Number of units per layer in each of GRU update networks\")\n",
        "\n",
        "parser.add_argument('--poisson', action='store_true', help=\"Model poisson-process likelihood for the density of events in addition to reconstruction.\")\n",
        "parser.add_argument('--classif', action='store_true', help=\"Include binary classification loss -- used for Physionet dataset for hospiral mortality\")\n",
        "\n",
        "parser.add_argument('--linear-classif', action='store_true', help=\"If using a classifier, use a linear classifier instead of 1-layer NN\")\n",
        "parser.add_argument('--extrap', action='store_true', help=\"Set extrapolation mode. If this flag is not set, run interpolation mode.\")\n",
        "\n",
        "parser.add_argument('-t', '--timepoints', type=int, default=100, help=\"Total number of time-points\")\n",
        "parser.add_argument('--max-t',  type=float, default=5., help=\"We subsample points in the interval [0, args.max_tp]\")\n",
        "parser.add_argument('--noise-weight', type=float, default=0.01, help=\"Noise amplitude for generated traejctories\")\n",
        "\n",
        "sys.argv = ['-f']\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# file_name = os.path.basename(__file__)[:-3]\n",
        "utils.makedirs(args.save)  # saves in 'experiments/' folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdatPwwYOVx5"
      },
      "source": [
        "## Manual seed, experimentID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu69paaHCAS4",
        "outputId": "dccfcb7c-025b-4332-8ee4-ed128c09deb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "experimentID is None\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(args.random_seed)\n",
        "np.random.seed(args.random_seed)\n",
        "\n",
        "experimentID = args.load  # None\n",
        "print(f\"experimentID is {experimentID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYDzsdK9B-NF",
        "outputId": "70bee1e6-0e36-4373-fe9c-342939c6d196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "experimentID is 70226\n",
            "ckpt_path is experiments/experiment_70226.ckpt\n"
          ]
        }
      ],
      "source": [
        "if experimentID is None:\n",
        "\t\t# Make a new experiment ID\n",
        "\t\texperimentID = int(SystemRandom().random()*100000) # from random import SystemRandom\n",
        "print(f\"experimentID is {experimentID}\")\n",
        "ckpt_path = os.path.join(args.save, \"experiment_\" + str(experimentID) + '.ckpt') \n",
        "print(f\"ckpt_path is {ckpt_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3hevLCAC2tS",
        "outputId": "cf808625-cc83-4421-9012-fd5a25239e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling dataset of 100 training examples\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "print(\"Sampling dataset of {} training examples\".format(args.n))  # n is size of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84rCv2MoGjKL",
        "outputId": "aef23af5-1728-4f50-d647-ca2ddb4786d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type of args is <class 'argparse.Namespace'>\n",
            "args is Namespace(batch_size=50, classic_rnn=False, classif=False, cut_tp=None, dataset='hopper', extrap=False, gen_layers=1, gru_units=100, input_decay=False, latent_ode=True, latents=6, linear_classif=False, load=None, lr=0.01, max_t=5.0, n=100, niters=300, noise_weight=0.01, ode_rnn=False, poisson=False, quantization=0.1, random_seed=1991, rec_dims=20, rec_layers=1, rnn_cell='gru', rnn_vae=False, sample_tp=None, save='experiments/', timepoints=100, units=100, viz=False, z0_encoder='odernn')\n"
          ]
        }
      ],
      "source": [
        "print(f\"type of args is {type(args)}\")\n",
        "print(f\"args is {str(args)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d7TpUY3BLIP",
        "outputId": "774d7be4-f4bc-4bab-b1e9-32551a37808a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_command is ['-f']\n"
          ]
        }
      ],
      "source": [
        "input_command = sys.argv\n",
        "print(f\"input_command is {input_command}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcTvI3I2GVlj",
        "outputId": "6965414c-f2dc-4cd2-e118-e1384c3986a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ind is []\n",
            "len(ind) is 0\n"
          ]
        }
      ],
      "source": [
        "ind = [i for i in range(len(input_command)) if input_command[i] == \"--load\"]\n",
        "print(f\"ind is {ind}\")\n",
        "print(f\"len(ind) is {len(ind)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvrwritSGX56",
        "outputId": "38c3c91e-f570-44b5-86aa-6f3bf26d1444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_command is -f\n"
          ]
        }
      ],
      "source": [
        "if len(ind) == 1:\n",
        "\t\tind = ind[0]\n",
        "\t\tinput_command = input_command[:ind] + input_command[(ind+2):]\n",
        "input_command = \" \".join(input_command)\n",
        "print(f\"input_command is {input_command}\")\n",
        "utils.makedirs(\"results/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNxDMKpNObYB"
      },
      "source": [
        "## parse_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsSKx1IeG4Wn",
        "outputId": "fc2a7108-3d5a-4250-f3d4-718053c313a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using the function parse_datasets from parse_datasets.py\n",
            "Inputs are (args, device)\n",
            "dataset_name is hopper\n",
            "\n",
            "\n",
            "args.timepoints is 100\n",
            "args.extrap is False\n",
            "n_total_tp = args.timepoints + args.extrap  --> is 100\n",
            "\n",
            "\n",
            "args.max_t is 5.0\n",
            "max_t_extrap = args.max_t / args.timepoints * n_total_tp --> 5.0\n",
            "\n",
            "\n",
            "if dataset_name == 'hopper'\n",
            "root is data\n",
            "Let's download\n",
            "we will check os.path.exists(os.path.join(self.data_folder, self.training_file)). If it exists, return\n",
            "we will check os.path.exists(os.path.join(self.data_folder, self.training_file)). If it exists, return\n",
            "data_file is data/HopperPhysics/training.pt\n",
            "Using torch.Tensor(torch.load(data_file)).to(device) to get self.data\n",
            "self.data.shape is torch.Size([10000, 200, 14])\n",
            "\n",
            "\n",
            "Using normalize_data()\n",
            "data.shape is torch.Size([10000, 200, 14])\n",
            "reshaped = data.reshape(-1, data.size(-1))\n",
            "reshaped.shape is torch.Size([2000000, 14])\n",
            "att_min = torch.min(reshaped, 0)[0]\n",
            "att_min.shape is torch.Size([14])\n",
            "att_max = torch.max(reshaped, 0)[0]\n",
            "att_max.shape is torch.Size([14])\n",
            "data_norm = (data - att_min) / att_max\n",
            "dataset.shape is torch.Size([100, 200, 14])\n",
            "\n",
            "\n",
            "n_tp_data = dataset[:].shape[1]\n",
            "n_tp_data is 200\n",
            "\n",
            "\n",
            "time_steps.shape is torch.Size([200])\n",
            "time_steps = time_steps / len(time_steps)\n",
            "time_steps.shape is torch.Size([200])\n",
            "\n",
            "\n",
            "if not args.extrap\n",
            "# Creating dataset for interpolation\n",
            "# sample time points from different parts of the timeline, \n",
            "# so that the model learns from different parts of hopper trajectory\n",
            "n_traj = len(dataset)\n",
            "n_traj is 100\n",
            "n_tp_data = dataset.shape[1]\n",
            "n_tp_data is 200\n",
            "n_reduced_tp = args.timepoints\n",
            "n_reduced_tp is 100\n",
            "\n",
            "\n",
            "start_ind = np.random.randint(0, high=n_tp_data - n_reduced_tp +1, size=n_traj)\n",
            "start_ind.shape is (100,)\n",
            "end_ind = start_ind + n_reduced_tp\n",
            "end_ind.shape is (100,)\n",
            "for i in range(n_traj):\n",
            "   sliced.append(dataset[i, start_ind[i] : end_ind[i], :])\n",
            "dataset = torch.stack(sliced).to(device)\n",
            "dataset.shape is torch.Size([100, 100, 14])\n",
            "time_steps = time_steps[:n_reduced_tp]\n",
            "time_steps.shape is torch.Size([100])\n",
            "\n",
            "\n",
            "Using split_train_test()\n",
            "train_y.shape is torch.Size([80, 100, 14])\n",
            "test_y.shape is torch.Size([20, 100, 14])\n",
            "n_samples is 100\n",
            "input_dim is 14\n",
            "batch_size = min(args.batch_size, args.n)\n",
            "batch_size is 50\n",
            "\n",
            "\n",
            "Using DataLoader from from torch.utils.data import DataLoader: https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
            "collate_fn (callable, optional)  merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.\n",
            "data_objects has following keys: dict_keys(['dataset_obj', 'train_dataloader', 'test_dataloader', 'input_dim', 'n_train_batches', 'n_test_batches'])\n"
          ]
        }
      ],
      "source": [
        "data_obj = parse_datasets(args, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVOzGMhYqRlo",
        "outputId": "6ce3d4b1-8d9d-4b93-d602-695738792b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_dim is 14\n",
            "n_train_batches is 2\n",
            "n_test_batches is 1\n"
          ]
        }
      ],
      "source": [
        "input_dim = data_obj[\"input_dim\"]\n",
        "n_train_batches = data_obj['n_train_batches']\n",
        "n_test_batches = data_obj['n_test_batches']\n",
        "print(f\"input_dim is {input_dim}\")\n",
        "print(f\"n_train_batches is {n_train_batches}\")\n",
        "print(f\"n_test_batches is {n_test_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW_Hbre8Hs8z",
        "outputId": "ae0ed782-4ed7-447b-b7e9-530fdd61183a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['dataset_obj', 'train_dataloader', 'test_dataloader', 'input_dim', 'n_train_batches', 'n_test_batches'])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_obj.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG8UQWqRytd"
      },
      "source": [
        "## Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "9acAJ4xhH_0b"
      },
      "outputs": [],
      "source": [
        "classif_per_tp = False\n",
        "if (\"classif_per_tp\" in data_obj):\n",
        "  # do classification per time point rather than on a time series as a whole\n",
        "  classif_per_tp = data_obj[\"classif_per_tp\"]\n",
        "\n",
        "if args.classif and (args.dataset == \"hopper\" or args.dataset == \"periodic\"):\n",
        "  raise Exception(\"Classification task is not available for MuJoCo and 1d datasets\")\n",
        "\n",
        "n_labels = 1\n",
        "if args.classif:\n",
        "  if (\"n_labels\" in data_obj):\n",
        "    n_labels = data_obj[\"n_labels\"]\n",
        "  else:\n",
        "    raise Exception(\"Please provide number of labels for classification task\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmrRsoTWI2Z4",
        "outputId": "3bb07f31-5c6d-4125-8948-146ee28529c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_labels is 1\n"
          ]
        }
      ],
      "source": [
        "print(f\"n_labels is {n_labels}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGFg8VmZIt3N",
        "outputId": "b772b91a-adf3-4db5-d587-b4c3f7d1cd5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "obsrv_std is tensor([0.0010])\n",
            "z0_prior is Normal(loc: tensor([0.]), scale: tensor([1.]))\n"
          ]
        }
      ],
      "source": [
        "obsrv_std = 0.01\n",
        "if args.dataset == \"hopper\":\n",
        "  obsrv_std = 1e-3 \n",
        "\n",
        "obsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
        "print(f\"obsrv_std is {obsrv_std}\")\n",
        "z0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))\n",
        "print(f\"z0_prior is {z0_prior}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Es2YTFkSI0_D",
        "outputId": "35a1d951-5ce2-4359-cd5f-0b41fcae8c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inside create_LatentODE_model\n",
            "dim is 6\n",
            "\n",
            "\n",
            "Making ode_func_net\n",
            "Inside create_net function\n",
            "n_input for layers is 6\n",
            "n_units for layers is 100\n",
            "n_layers is 1\n",
            "nonlinear is <class 'torch.nn.modules.activation.Tanh'>\n",
            "for i in range(n_layers):\n",
            "    layers.append(nonlinear())\n",
            "    layers.append(nn.Linear(n_units, n_units))\n",
            "layers.append(nonlinear())\n",
            "layers.append(nn.Linear(n_units, n_outputs))\n",
            "n_outputs is 6\n",
            "Making gen_ode_func\n",
            "Inside ODEFunc class\n",
            "input_dim is 14\n",
            "latent_dim is 6\n",
            "Inside init_network_weights function\n",
            "std is 0.1\n",
            "Making ode_func_net for odernn\n",
            "Inside create_net function\n",
            "n_input for layers is 20\n",
            "n_units for layers is 100\n",
            "n_layers is 1\n",
            "nonlinear is <class 'torch.nn.modules.activation.Tanh'>\n",
            "for i in range(n_layers):\n",
            "    layers.append(nonlinear())\n",
            "    layers.append(nn.Linear(n_units, n_units))\n",
            "layers.append(nonlinear())\n",
            "layers.append(nn.Linear(n_units, n_outputs))\n",
            "n_outputs is 20\n",
            "Making rec_ode_func using ODEFunc\n",
            "Inside ODEFunc class\n",
            "input_dim is 28\n",
            "latent_dim is 20\n",
            "Inside init_network_weights function\n",
            "std is 0.1\n",
            "Inside DiffeqSolver\n",
            "Inside GRU unit\n",
            "\n",
            "\n",
            "self.update_gate = nn.Sequential( nn.Linear(latent_dim * 2 + input_dim, n_units), nn.Tanh(), nn.Linear(n_units, latent_dim), nn.Sigmoid())\n",
            "self.reset_gate = nn.Sequential( nn.Linear(latent_dim * 2 + input_dim, n_units), nn.Tanh(), nn.Linear(n_units, latent_dim), nn.Sigmoid())\n",
            "self.new_state_net = nn.Sequential( nn.Linear(latent_dim * 2 + input_dim, n_units), nn.Tanh(), nn.Linear(n_units, latent_dim), nn.Sigmoid())\n",
            "Inside init_network_weights function\n",
            "std is 0.1\n",
            "Inside init_network_weights function\n",
            "std is 0.1\n",
            "Inside init_network_weights function\n",
            "std is 0.1\n",
            "Inside init_network_weights function\n",
            "std is 0.1\n",
            "Inside init_network_weights function\n",
            "std is 0.1\n",
            "Inside DiffeqSolver\n"
          ]
        }
      ],
      "source": [
        "if args.latent_ode:\n",
        "    model = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
        "            classif_per_tp = classif_per_tp,\n",
        "            n_labels = n_labels)\n",
        "else:\n",
        "\traise Exception(\"Model not specified\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "braJwCR6RTrz"
      },
      "outputs": [],
      "source": [
        "t1 = torch.tensor([1,2,3,4]) \n",
        "t2 = torch.tensor([5,6,7,8]) \n",
        "t3 = torch.tensor([9,10,11,12])\n",
        "\n",
        "t = torch.stack(\n",
        "    (t1,t2,t3)\n",
        "    ,dim=-1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRFpJMNPRTnl",
        "outputId": "5287c4c9-fd93-4b38-ea7e-3102413e31fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[1],\n",
              "         [2],\n",
              "         [3],\n",
              "         [4]]), tensor([[ 5,  9],\n",
              "         [ 6, 10],\n",
              "         [ 7, 11],\n",
              "         [ 8, 12]]))"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.split_last_dim(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "7Srlw66gRTlC"
      },
      "outputs": [],
      "source": [
        "from lib import utils as utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UulIfhotRTim",
        "outputId": "eadc9d8e-b958-49f9-afe1-cb2a403c3491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[1],\n",
              "         [2],\n",
              "         [3],\n",
              "         [4]]), tensor([[ 5,  9],\n",
              "         [ 6, 10],\n",
              "         [ 7, 11],\n",
              "         [ 8, 12]]))"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "utils.split_last_dim(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRVi3J2POA-d"
      },
      "source": [
        "# For Stock Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdhXnHBitsXf",
        "outputId": "f06eac26-5f3c-47a2-c536-9c2dcd6852b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'torchdiffeq' already exists and is not an empty directory.\n",
            "Obtaining file:///content/torchdiffeq\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu111)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "  Attempting uninstall: torchdiffeq\n",
            "    Found existing installation: torchdiffeq 0.2.2\n",
            "    Can't uninstall 'torchdiffeq'. No files were found to uninstall.\n",
            "  Running setup.py develop for torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n",
            "_impl  __init__.py  __pycache__\n"
          ]
        }
      ],
      "source": [
        "# Install the latest version of author's repo neural ode implementation\n",
        "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "!cd torchdiffeq && pip install -e .\n",
        "!ls torchdiffeq/torchdiffeq\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CE5l_KMEuJeR"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9_9gEN7qtxuC"
      },
      "outputs": [],
      "source": [
        "# run_models.py\n",
        "import os\n",
        "import sys\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import SystemRandom\n",
        "from sklearn import model_selection\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import relu\n",
        "import torch.optim as optim\n",
        "\n",
        "import lib.utils as utils\n",
        "from lib.plotting import *\n",
        "\n",
        "from lib.rnn_baselines import *\n",
        "from lib.ode_rnn import *\n",
        "from lib.create_latent_ode_model import create_LatentODE_model\n",
        "from lib.parse_datasets import parse_datasets\n",
        "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
        "from lib.diffeq_solver import DiffeqSolver\n",
        "from mujoco_physics import HopperPhysics\n",
        "\n",
        "from lib.utils import compute_loss_all_batches\n",
        "\n",
        "import sys\n",
        "# print(sys.argv[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8wEQAF0uHYF"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "m5RUn7XROEik"
      },
      "outputs": [],
      "source": [
        "# Generative model for noisy data based on ODE\n",
        "parser = argparse.ArgumentParser('Latent ODE')\n",
        "parser.add_argument('-n',  type=int, default=100, help=\"Size of the dataset\")\n",
        "parser.add_argument('--niters', type=int, default=300)\n",
        "parser.add_argument('--lr',  type=float, default=1e-2, help=\"Starting learning rate.\")\n",
        "parser.add_argument('-b', '--batch-size', type=int, default=50)\n",
        "parser.add_argument('--viz', action='store_true', help=\"Show plots while training\")\n",
        "\n",
        "parser.add_argument('--save', type=str, default='experiments/', help=\"Path for save checkpoints\")\n",
        "parser.add_argument('--load', type=str, default=None, help=\"ID of the experiment to load for evaluation. If None, run a new experiment.\")\n",
        "parser.add_argument('-r', '--random-seed', type=int, default=1991, help=\"Random_seed\")\n",
        "\n",
        "parser.add_argument('--dataset', type=str, default='hopper', help=\"Dataset to load. Available: physionet, activity, hopper, periodic\")\n",
        "parser.add_argument('-s', '--sample-tp', type=float, default=None, help=\"Number of time points to sub-sample.\"\n",
        "\t\"If > 1, subsample exact number of points. If the number is in [0,1], take a percentage of available points per time series. If None, do not subsample\")\n",
        "\n",
        "parser.add_argument('-c', '--cut-tp', type=int, default=None, help=\"Cut out the section of the timeline of the specified length (in number of points).\"\n",
        "\t\"Used for periodic function demo.\")\n",
        "\n",
        "parser.add_argument('--quantization', type=float, default=0.1, help=\"Quantization on the physionet dataset.\"\n",
        "\t\"Value 1 means quantization by 1 hour, value 0.1 means quantization by 0.1 hour = 6 min\")\n",
        "\n",
        "parser.add_argument('--latent-ode', default = True, action='store_true', help=\"Run Latent ODE seq2seq model\")\n",
        "parser.add_argument('--z0-encoder', type=str, default='odernn', help=\"Type of encoder for Latent ODE model: odernn or rnn\")\n",
        "\n",
        "parser.add_argument('--classic-rnn', action='store_true', help=\"Run RNN baseline: classic RNN that sees true points at every point. Used for interpolation only.\")\n",
        "parser.add_argument('--rnn-cell', default=\"gru\", help=\"RNN Cell type. Available: gru (default), expdecay\")\n",
        "parser.add_argument('--input-decay', action='store_true', help=\"For RNN: use the input that is the weighted average of impirical mean and previous value (like in GRU-D)\")\n",
        "\n",
        "parser.add_argument('--ode-rnn', action='store_true', help=\"Run ODE-RNN baseline: RNN-style that sees true points at every point. Used for interpolation only.\")\n",
        "\n",
        "parser.add_argument('--rnn-vae', action='store_true', help=\"Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss.\")\n",
        "\n",
        "parser.add_argument('-l', '--latents', type=int, default=6, help=\"Size of the latent state\")\n",
        "parser.add_argument('--rec-dims', type=int, default=20, help=\"Dimensionality of the recognition model (ODE or RNN).\")\n",
        "\n",
        "parser.add_argument('--rec-layers', type=int, default=1, help=\"Number of layers in ODE func in recognition ODE\")\n",
        "parser.add_argument('--gen-layers', type=int, default=1, help=\"Number of layers in ODE func in generative ODE\")\n",
        "\n",
        "parser.add_argument('-u', '--units', type=int, default=100, help=\"Number of units per layer in ODE func\")\n",
        "parser.add_argument('-g', '--gru-units', type=int, default=100, help=\"Number of units per layer in each of GRU update networks\")\n",
        "\n",
        "parser.add_argument('--poisson', action='store_true', help=\"Model poisson-process likelihood for the density of events in addition to reconstruction.\")\n",
        "parser.add_argument('--classif', action='store_true', help=\"Include binary classification loss -- used for Physionet dataset for hospiral mortality\")\n",
        "\n",
        "parser.add_argument('--linear-classif', action='store_true', help=\"If using a classifier, use a linear classifier instead of 1-layer NN\")\n",
        "parser.add_argument('--extrap', action='store_true', help=\"Set extrapolation mode. If this flag is not set, run interpolation mode.\")\n",
        "\n",
        "parser.add_argument('-t', '--timepoints', type=int, default=100, help=\"Total number of time-points\")\n",
        "parser.add_argument('--max-t',  type=float, default=5., help=\"We subsample points in the interval [0, args.max_tp]\")\n",
        "parser.add_argument('--noise-weight', type=float, default=0.01, help=\"Noise amplitude for generated traejctories\")\n",
        "\n",
        "sys.argv = ['-f']\n",
        "args = parser.parse_args()\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# file_name = os.path.basename(__file__)[:-3]\n",
        "utils.makedirs(args.save)  # saves in 'experiments/' folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNEj_b_nuDHe"
      },
      "source": [
        "## Manual seed, experimentID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AklWRbrvOgYp",
        "outputId": "fd4778cc-de9f-4021-aded-5f245905db39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "experimentID is None\n",
            "experimentID is 13863\n",
            "ckpt_path is experiments/experiment_13863.ckpt\n",
            "Sampling dataset of 100 training examples\n",
            "args is Namespace(batch_size=50, classic_rnn=False, classif=False, cut_tp=None, dataset='hopper', extrap=False, gen_layers=1, gru_units=100, input_decay=False, latent_ode=True, latents=6, linear_classif=False, load=None, lr=0.01, max_t=5.0, n=100, niters=300, noise_weight=0.01, ode_rnn=False, poisson=False, quantization=0.1, random_seed=1991, rec_dims=20, rec_layers=1, rnn_cell='gru', rnn_vae=False, sample_tp=None, save='experiments/', timepoints=100, units=100, viz=False, z0_encoder='odernn')\n",
            "input_command is ['-f']\n",
            "ind is []\n",
            "len(ind) is 0\n",
            "input_command is -f\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(args.random_seed)\n",
        "np.random.seed(args.random_seed)\n",
        "\n",
        "experimentID = args.load  # None\n",
        "print(f\"experimentID is {experimentID}\")\n",
        "\n",
        "if experimentID is None:\n",
        "\t\t# Make a new experiment ID\n",
        "\t\texperimentID = int(SystemRandom().random()*100000) # from random import SystemRandom\n",
        "print(f\"experimentID is {experimentID}\")\n",
        "ckpt_path = os.path.join(args.save, \"experiment_\" + str(experimentID) + '.ckpt') \n",
        "print(f\"ckpt_path is {ckpt_path}\")\n",
        "\n",
        "start = time.time()\n",
        "print(\"Sampling dataset of {} training examples\".format(args.n))  # n is size of the dataset\n",
        "\n",
        "print(f\"args is {str(args)}\")\n",
        "\n",
        "input_command = sys.argv\n",
        "print(f\"input_command is {input_command}\")\n",
        "\n",
        "ind = [i for i in range(len(input_command)) if input_command[i] == \"--load\"]\n",
        "print(f\"ind is {ind}\")\n",
        "print(f\"len(ind) is {len(ind)}\")\n",
        "\n",
        "if len(ind) == 1:\n",
        "\t\tind = ind[0]\n",
        "\t\tinput_command = input_command[:ind] + input_command[(ind+2):]\n",
        "input_command = \" \".join(input_command)\n",
        "print(f\"input_command is {input_command}\")\n",
        "utils.makedirs(\"results/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6eEFp6q2onU"
      },
      "source": [
        "## Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3Fl56I3uVGJ",
        "outputId": "a320fbed-0c90-4bd8-e0dc-bd8773912836"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.64.tar.gz (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     || 6.3 MB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.64-py2.py3-none-any.whl size=24109 sha256=4065379c39b442292edaa407b1da8a9042a909b5238476d9a910eb394f3430b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/fe/9b/a4d3d78796b699e37065e5b6c27b75cff448ddb8b24943c288\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.3 yfinance-0.1.64\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTD8BTd02qeX",
        "outputId": "7fd98c1b-4863-4a41-ea4e-13b2401ebb1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get Open and Close Price of Assets\n",
            "raw-stock-data/data-2000-2021 directory is created\n",
            "\n",
            "\n",
            "The size of each asset\n",
            "Apple size: 5450\n",
            "Microsoft size: 5450\n",
            "Google size: 4288\n",
            "Bitcoin size: 2537\n",
            "Facebook size: 2336\n",
            "Walmart size: 5450\n",
            "Amazon size: 5450\n",
            "CVS size: 5450\n",
            "Berkshire size: 5450\n",
            "ExxonMobil size: 5450\n",
            "AtandT size: 5450\n",
            "Costco size: 5450\n",
            "Walgreens size: 5450\n",
            "Kroger size: 5450\n",
            "JPMorgan size: 5450\n",
            "Verizon size: 5450\n",
            "FordMotor size: 5450\n",
            "GeneralMotors size: 2713\n",
            "Dell size: 1268\n",
            "BankOfAmerica size: 5450\n",
            "Target size: 5450\n",
            "GeneralElectric size: 5450\n",
            "JohnsonandJohnson size: 5450\n",
            "Nvidia size: 5451\n",
            "Intel size: 5450\n",
            "Apple data.shape (1090, 10)\n",
            "Apple Data Original after series to supervised on data (1085, 60)\n",
            "Apple Median data\n",
            "Apple median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Apple Median data after series to supervised\n",
            "Apple data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Microsoft data.shape (1090, 10)\n",
            "Microsoft Data Original after series to supervised on data (1085, 60)\n",
            "Microsoft Median data\n",
            "Microsoft median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Microsoft Median data after series to supervised\n",
            "Microsoft data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Google data.shape (856, 10)\n",
            "Google Data Original after series to supervised on data (851, 60)\n",
            "Google Median data\n",
            "Google median_data.shape (856, 1)\n",
            "\n",
            "\n",
            "Google Median data after series to supervised\n",
            "Google data_m1.shape (851, 6)\n",
            "\n",
            "\n",
            "Bitcoin data.shape (506, 10)\n",
            "Bitcoin Data Original after series to supervised on data (501, 60)\n",
            "Bitcoin Median data\n",
            "Bitcoin median_data.shape (506, 1)\n",
            "\n",
            "\n",
            "Bitcoin Median data after series to supervised\n",
            "Bitcoin data_m1.shape (501, 6)\n",
            "\n",
            "\n",
            "Facebook data.shape (466, 10)\n",
            "Facebook Data Original after series to supervised on data (461, 60)\n",
            "Facebook Median data\n",
            "Facebook median_data.shape (466, 1)\n",
            "\n",
            "\n",
            "Facebook Median data after series to supervised\n",
            "Facebook data_m1.shape (461, 6)\n",
            "\n",
            "\n",
            "Walmart data.shape (1090, 10)\n",
            "Walmart Data Original after series to supervised on data (1085, 60)\n",
            "Walmart Median data\n",
            "Walmart median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Walmart Median data after series to supervised\n",
            "Walmart data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Amazon data.shape (1090, 10)\n",
            "Amazon Data Original after series to supervised on data (1085, 60)\n",
            "Amazon Median data\n",
            "Amazon median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Amazon Median data after series to supervised\n",
            "Amazon data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "CVS data.shape (1090, 10)\n",
            "CVS Data Original after series to supervised on data (1085, 60)\n",
            "CVS Median data\n",
            "CVS median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "CVS Median data after series to supervised\n",
            "CVS data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Berkshire data.shape (1090, 10)\n",
            "Berkshire Data Original after series to supervised on data (1085, 60)\n",
            "Berkshire Median data\n",
            "Berkshire median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Berkshire Median data after series to supervised\n",
            "Berkshire data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "ExxonMobil data.shape (1090, 10)\n",
            "ExxonMobil Data Original after series to supervised on data (1085, 60)\n",
            "ExxonMobil Median data\n",
            "ExxonMobil median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "ExxonMobil Median data after series to supervised\n",
            "ExxonMobil data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "AtandT data.shape (1090, 10)\n",
            "AtandT Data Original after series to supervised on data (1085, 60)\n",
            "AtandT Median data\n",
            "AtandT median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "AtandT Median data after series to supervised\n",
            "AtandT data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Costco data.shape (1090, 10)\n",
            "Costco Data Original after series to supervised on data (1085, 60)\n",
            "Costco Median data\n",
            "Costco median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Costco Median data after series to supervised\n",
            "Costco data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Walgreens data.shape (1090, 10)\n",
            "Walgreens Data Original after series to supervised on data (1085, 60)\n",
            "Walgreens Median data\n",
            "Walgreens median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Walgreens Median data after series to supervised\n",
            "Walgreens data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Kroger data.shape (1090, 10)\n",
            "Kroger Data Original after series to supervised on data (1085, 60)\n",
            "Kroger Median data\n",
            "Kroger median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Kroger Median data after series to supervised\n",
            "Kroger data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "JPMorgan data.shape (1090, 10)\n",
            "JPMorgan Data Original after series to supervised on data (1085, 60)\n",
            "JPMorgan Median data\n",
            "JPMorgan median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "JPMorgan Median data after series to supervised\n",
            "JPMorgan data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Verizon data.shape (1090, 10)\n",
            "Verizon Data Original after series to supervised on data (1085, 60)\n",
            "Verizon Median data\n",
            "Verizon median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Verizon Median data after series to supervised\n",
            "Verizon data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "FordMotor data.shape (1090, 10)\n",
            "FordMotor Data Original after series to supervised on data (1085, 60)\n",
            "FordMotor Median data\n",
            "FordMotor median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "FordMotor Median data after series to supervised\n",
            "FordMotor data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "GeneralMotors data.shape (542, 10)\n",
            "GeneralMotors Data Original after series to supervised on data (537, 60)\n",
            "GeneralMotors Median data\n",
            "GeneralMotors median_data.shape (542, 1)\n",
            "\n",
            "\n",
            "GeneralMotors Median data after series to supervised\n",
            "GeneralMotors data_m1.shape (537, 6)\n",
            "\n",
            "\n",
            "Dell data.shape (252, 10)\n",
            "Dell Data Original after series to supervised on data (247, 60)\n",
            "Dell Median data\n",
            "Dell median_data.shape (252, 1)\n",
            "\n",
            "\n",
            "Dell Median data after series to supervised\n",
            "Dell data_m1.shape (247, 6)\n",
            "\n",
            "\n",
            "BankOfAmerica data.shape (1090, 10)\n",
            "BankOfAmerica Data Original after series to supervised on data (1085, 60)\n",
            "BankOfAmerica Median data\n",
            "BankOfAmerica median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "BankOfAmerica Median data after series to supervised\n",
            "BankOfAmerica data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Target data.shape (1090, 10)\n",
            "Target Data Original after series to supervised on data (1085, 60)\n",
            "Target Median data\n",
            "Target median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Target Median data after series to supervised\n",
            "Target data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "GeneralElectric data.shape (1090, 10)\n",
            "GeneralElectric Data Original after series to supervised on data (1085, 60)\n",
            "GeneralElectric Median data\n",
            "GeneralElectric median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "GeneralElectric Median data after series to supervised\n",
            "GeneralElectric data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "JohnsonandJohnson data.shape (1090, 10)\n",
            "JohnsonandJohnson Data Original after series to supervised on data (1085, 60)\n",
            "JohnsonandJohnson Median data\n",
            "JohnsonandJohnson median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "JohnsonandJohnson Median data after series to supervised\n",
            "JohnsonandJohnson data_m1.shape (1085, 6)\n",
            "\n",
            "\n",
            "Nvidia data.shape (1090, 10)\n",
            "Nvidia Data Original after series to supervised on data (1079, 60)\n",
            "Nvidia Median data\n",
            "Nvidia median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Nvidia Median data after series to supervised\n",
            "Nvidia data_m1.shape (1079, 6)\n",
            "\n",
            "\n",
            "Intel data.shape (1090, 10)\n",
            "Intel Data Original after series to supervised on data (1085, 60)\n",
            "Intel Median data\n",
            "Intel median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Intel Median data after series to supervised\n",
            "Intel data_m1.shape (1085, 6)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1. Get Open and Close Price of asset (o, c) for each trading day.\n",
        "# libraries\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "print(f\"Get Open and Close Price of Assets\")\n",
        "def download_raw_stock_data(filepath, tickers, start, end, period = '1d'):\n",
        "    \"\"\"\n",
        "    Download Stock tickers\n",
        "    :Parameters:\n",
        "        filepath: str\n",
        "            path to store the raw data\n",
        "        tickers : str, list\n",
        "            List of tickers to download\n",
        "        period: str\n",
        "            the frequency at which to gather the data; common options would include 1d (daily), 1mo (monthly), 1y (yearly)\n",
        "        start: str\n",
        "            the date to start gathering the data. For example 201011\n",
        "        end: str\n",
        "            the date to end gathering the data. For example 2020125\n",
        "    \n",
        "    \"\"\"\n",
        "    #define the ticker symbol\n",
        "    tickerSymbol = tickers\n",
        "\n",
        "    #get data on this ticker\n",
        "    tickerData = yf.Ticker(tickerSymbol)\n",
        "\n",
        "    #get the historical prices for this ticker\n",
        "    tickerDf = tickerData.history(period=period, start=start, end=end)\n",
        "    tickerDf.to_csv(filepath)\n",
        "\n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "path = f\"raw-stock-data/data-2000-2021\"\n",
        "if not os.path.exists(path):\n",
        "    # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "    # Create a new directory\n",
        "    os.makedirs(path)\n",
        "    print(f\"{path} directory is created\")\n",
        "period = '1d'\n",
        "start='2000-1-1'\n",
        "end='2021-8-31'\n",
        "for tickerName, ticker in dict_tickers.items():\n",
        "    tickerName = tickerName\n",
        "    ticker = ticker\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    download_raw_stock_data(filepath, ticker, start, end, period)\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(f\"The size of each asset\")\n",
        "import pandas as pd\n",
        "for tickerName in dict_tickers.keys():\n",
        "    df = pd.read_csv(f\"{path}/{tickerName}.csv\")\n",
        "    print(f\"{tickerName} size: {len(df)}\")\n",
        "\n",
        "# 2. Get weekly data.\n",
        "# 3. Transform $d_{i}$ to sequences of lag * len($d_{i}$) length.\n",
        "\n",
        "def stockDataTransformer(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    df1 = df[['Open', 'Close']].copy()\n",
        "    data = df1.values\n",
        "    n_samples = data.shape[0]//10*10\n",
        "    reshape_number = n_samples*data.shape[1]//10\n",
        "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
        "    return data1\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "def split_data(perc_train, perc_valid, lag, data_orig, data_m1, n_features_orig, n_features_median):\n",
        "    values = data_m1\n",
        "    \n",
        "    sizeOfReframed = len(data_m1)\n",
        "    len_train = int(perc_train*sizeOfReframed) # int(sizeOfReframed - len_test) # - len_valid)\n",
        "    train_data_orig = data_orig[:len_train, :]\n",
        "    # valid = values[len_train:len_valid+len_train, :]\n",
        "    test_data_orig = data_orig[len_train:, :]  # [len_valid+len_train:, :]\n",
        "    # n_features = n_features\n",
        "    \n",
        "    train_data_ml = values[:len_train, :]\n",
        "    test_data_ml = values[len_train:, :] \n",
        "    # split into input and outputs\n",
        "    n_obs = lag * n_features_orig\n",
        "    n_obs_median = (lag+forecast) * n_features_median\n",
        "    train_X, train_y = train_data_orig[:, :n_obs], train_data_ml[:, :n_obs_median]\n",
        "    test_X, test_y = test_data_orig[:, :n_obs], test_data_ml[:, :n_obs_median]\n",
        "    # valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
        "    print(train_X.shape, len(train_X), train_y.shape)\n",
        "    \n",
        "    # reshape input to be 3D [samples, features, lag]\n",
        "    train_X = train_X.reshape((train_X.shape[0], n_features_orig, lag))\n",
        "    test_X = test_X.reshape((test_X.shape[0], n_features_orig, lag))\n",
        "    # valid_X = valid_X.reshape((valid_X.shape[0], lag, n_features))\n",
        "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # , valid_X.shape, valid_y.shape)\n",
        "    \n",
        "    # Get the reconstruction train_y, test_y and extrapolated train_y, test_y\n",
        "    train_y_recon, train_y_extrapol = train_y[:, :lag], train_y[:, lag:]\n",
        "    test_y_recon, test_y_extrapol = test_y[:, :lag], test_y[:, lag:]\n",
        "    dataload = {\n",
        "        'train_data_orig': train_data_orig,\n",
        "        'test_data_orig': test_data_orig,\n",
        "        'train_data_ml': train_data_ml,\n",
        "        'test_data_ml': test_data_ml,\n",
        "        # 'valid': valid,\n",
        "        'train_X': train_X,\n",
        "        'train_y': train_y,\n",
        "        'test_X': test_X,\n",
        "        'test_y': test_y,\n",
        "        'n_features_orig': n_features_orig,\n",
        "        'n_features_median': n_features_median,\n",
        "        'n_obs': n_obs,\n",
        "        'n_obs_median': n_obs_median,\n",
        "        # 'valid_X': valid_X,\n",
        "        # 'valid_y': valid_y,\n",
        "        'train_y_recon': train_y_recon,\n",
        "        'train_y_extrapol': train_y_extrapol,\n",
        "        'test_y_recon': test_y_recon,\n",
        "        'test_y_extrapol': test_y_extrapol\n",
        "    }\n",
        "    \n",
        "    return dataload\n",
        "\n",
        "def get_median(array, axis = 1):\n",
        "    # https://numpy.org/doc/stable/reference/generated/numpy.median.html\n",
        "    return np.median(array, axis = axis).reshape(data_size, 1)  #, keepdims=True)\n",
        "\n",
        "from pandas import concat\n",
        "import numpy as np\n",
        "week_sequence = {}\n",
        "median_data_dict = {}\n",
        "lag = 5\n",
        "for tickerName in dict_tickers.keys():\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    # Get the data in the required format\n",
        "    data = stockDataTransformer(filepath)\n",
        "    # # Total Data Size\n",
        "    data_size = data.shape[0]\n",
        "    print(f\"{tickerName} data.shape {data.shape}\")\n",
        "    data_orig = series_to_supervised(data, lag).values\n",
        "    print(f'{tickerName} Data Original after series to supervised on data {data_orig.shape}')\n",
        "    week_sequence[tickerName] = data_orig\n",
        "\n",
        "    median_data = get_median(data)\n",
        "    print(f'{tickerName} Median data')\n",
        "    # Median data for each week\n",
        "    print(f\"{tickerName} median_data.shape {median_data.shape}\")\n",
        "    # print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
        "    print('\\n')\n",
        "\n",
        "    # Convert median_data to (n_samples, 5) matrix\n",
        "    data_m1 = series_to_supervised(median_data, lag).values\n",
        "    median_data_dict[tickerName] = data_m1\n",
        "    print(f'{tickerName} Median data after series to supervised')\n",
        "    print(f\"{tickerName} data_m1.shape {data_m1.shape}\")\n",
        "    # print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n",
        "    print('\\n')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s-4blJV2sss",
        "outputId": "44e12c63-f366-4173-a21b-f935441ddf5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data.shape (2170, 60)\n",
            "data.shape (3021, 60)\n",
            "data.shape (3522, 60)\n",
            "data.shape (3983, 60)\n",
            "data.shape (5068, 60)\n",
            "data.shape (6153, 60)\n",
            "data.shape (7238, 60)\n",
            "data.shape (8323, 60)\n",
            "data.shape (9408, 60)\n",
            "data.shape (10493, 60)\n",
            "data.shape (11578, 60)\n",
            "data.shape (12663, 60)\n",
            "data.shape (13748, 60)\n",
            "data.shape (14833, 60)\n",
            "data.shape (15918, 60)\n",
            "data.shape (17003, 60)\n",
            "data.shape (17540, 60)\n",
            "data.shape (17787, 60)\n",
            "data.shape (18872, 60)\n",
            "data.shape (19957, 60)\n",
            "data.shape (21042, 60)\n",
            "data.shape (22127, 60)\n",
            "data.shape (23206, 60)\n",
            "data.shape (24291, 60)\n",
            "data.shape (2170, 6)\n",
            "data.shape (3021, 6)\n",
            "data.shape (3522, 6)\n",
            "data.shape (3983, 6)\n",
            "data.shape (5068, 6)\n",
            "data.shape (6153, 6)\n",
            "data.shape (7238, 6)\n",
            "data.shape (8323, 6)\n",
            "data.shape (9408, 6)\n",
            "data.shape (10493, 6)\n",
            "data.shape (11578, 6)\n",
            "data.shape (12663, 6)\n",
            "data.shape (13748, 6)\n",
            "data.shape (14833, 6)\n",
            "data.shape (15918, 6)\n",
            "data.shape (17003, 6)\n",
            "data.shape (17540, 6)\n",
            "data.shape (17787, 6)\n",
            "data.shape (18872, 6)\n",
            "data.shape (19957, 6)\n",
            "data.shape (21042, 6)\n",
            "data.shape (22127, 6)\n",
            "data.shape (23206, 6)\n",
            "data.shape (24291, 6)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "data = week_sequence['Apple']\n",
        "# 4. Bundle all sequences together\n",
        "for tickerName in week_sequence.keys():\n",
        "    if tickerName != 'Apple':\n",
        "        data1 = week_sequence[tickerName]\n",
        "        data = np.concatenate((data, data1))\n",
        "        print(f\"data.shape {data.shape}\")\n",
        "\n",
        "import numpy as np\n",
        "data_m = median_data_dict['Apple']\n",
        "# 4. Bundle all sequences together\n",
        "for tickerName in median_data_dict.keys():\n",
        "    if tickerName != 'Apple':\n",
        "        data1 = median_data_dict[tickerName]\n",
        "        data_m = np.concatenate((data_m, data1))\n",
        "        print(f\"data.shape {data_m.shape}\")\n",
        "\n",
        "data_df = pd.DataFrame(data)\n",
        "data_df.to_csv(f\"all_assets_sequences.csv\")\n",
        "\n",
        "data_m_df = pd.DataFrame(data_m)\n",
        "data_m_df.to_csv(f\"median_sequences.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnHegSSq3QHK"
      },
      "source": [
        "## Parse Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGdaFPy53TdP"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arCRMwnw3Adz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import lib.utils as utils\n",
        "from lib.diffeq_solver import DiffeqSolver\n",
        "from generate_timeseries import Periodic_1d\n",
        "from torch.distributions import uniform\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "# from mujoco_physics import HopperPhysics\n",
        "\n",
        "from sklearn import model_selection\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o3MTXbx3ctF"
      },
      "source": [
        "### HopperPhysics class for our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5_OW_ir3mhW"
      },
      "source": [
        "#### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Rf17d12X3iHr"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from lib.utils import get_dict_template\n",
        "import lib.utils as utils\n",
        "from torchvision.datasets.utils import download_url\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wfk8BWz33vt"
      },
      "source": [
        "#### Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "404lfySk33Vj"
      },
      "outputs": [],
      "source": [
        "###########################\n",
        "# Latent ODEs for Irregularly-Sampled Time Series\n",
        "# Authors: Yulia Rubanova and Ricky Chen\n",
        "###########################\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from lib.utils import get_dict_template\n",
        "import lib.utils as utils\n",
        "from torchvision.datasets.utils import download_url\n",
        "import pandas as pd\n",
        "\n",
        "class HopperPhysics(object):\n",
        "\n",
        "    T = 5\n",
        "    print(f\"We are inside HopperPhysics class\")\n",
        "    print(f\"T is {T}\")\n",
        "    D = 10\n",
        "    print(\"dim is {D}\")\n",
        "\n",
        "    median_dim = 1\n",
        "    print(\"median_dim is {median_dim}\")\n",
        "\n",
        "    # n_training_samples = 10000\n",
        "    # print(f\"n_training_samples is {n_training_samples}\")\n",
        "\n",
        "    training_file = 'training.csv'\n",
        "    print(f\"training_file name is {training_file}\")\n",
        "\n",
        "    def __init__(self, root, trainfile, medianfile, download = True, generate=False, device = torch.device(\"cpu\")):\n",
        "        print(f\"root is {root}\")\n",
        "        self.root = root\n",
        "        if download:\n",
        "            print(f\"Let's load dataset\")\n",
        "            X = pd.read_csv(os.path.join(root, trainfile))\n",
        "            y = pd.read_csv(os.path.join(root, medianfile))\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
        "\n",
        "        data_file = os.path.join(self.data_folder, self.training_file)\n",
        "        median_file = os.path.join(self.data_folder, 'median_sequences.csv')\n",
        "        print(f\"data_file is {data_file}\")\n",
        "        # print(f\"Using torch.Tensor(torch.load(data_file)).to(device) to get self.data\")\n",
        "        self.data = torch.Tensor(X.values).to(device) # https://pytorch.org/docs/stable/generated/torch.load.html\n",
        "        self.median_data = torch.Tensor(y.values).to(device)\n",
        "\n",
        "        self.data = torch.reshape(self.data, (self.data.shape[0], T, D))\n",
        "        self.median_data = torch.reshape(self.median_data, (self.median_data.shape[0], T, D))\n",
        "        \n",
        "\n",
        "        print(f\"self.data.shape is {self.data.shape}\")\n",
        "        print(f\"self.median_data.shape is {self.median_data.shape}\")\n",
        "        print(\"\\n\")\n",
        "        self.data, self.data_min, self.data_max = utils.normalize_data(self.data)\n",
        "        self.median_data, self.median_data_min, self.median_data_max = utils.normalize_data(self.median_data)\n",
        "        self.dataset = list(zip(self.data, self.median_data))\n",
        "\n",
        "        self.device =device\n",
        "        self.n_training_samples = self.data.shape[0]\n",
        "        print(f\"self.n_training_samples is {self.n_training_samples}\")\n",
        "\n",
        "    def _check_exists(self):\n",
        "        print(f\"we will check os.path.exists(os.path.join(self.data_folder, self.training_file)). If it exists, return\")\n",
        "        return os.path.exists(os.path.join(self.data_folder, self.training_file))\n",
        "\n",
        "    @property\n",
        "    def data_folder(self):\n",
        "        return self.root\n",
        "\n",
        "        # def __getitem__(self, index):\n",
        "        #     return self.data[index]\n",
        "\n",
        "    def get_dataset(self):\n",
        "        return self.dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def size(self, ind = None):\n",
        "        if ind is not None:\n",
        "            return self.data.shape[ind]\n",
        "            return self.data.shape\n",
        "                \n",
        "    def __repr__(self):\n",
        "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
        "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
        "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
        "        return fmt_str\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### parse_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/xg/7kkkscn565q970f5fprf7jjm0000gn/T/ipykernel_12910/3070103915.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import lib.utils as utils\n",
        "from lib.diffeq_solver import DiffeqSolver\n",
        "from generate_timeseries import Periodic_1d\n",
        "from torch.distributions import uniform\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn import model_selection\n",
        "import random\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_datasets(args, device):\n",
        "\n",
        "\t# batch the data and median_data together and don't shuffle\n",
        "\tdef basic_collate_fn(batch, time_steps, args = args, device = device, data_type = \"train\"):\n",
        "    \t# batch looks like [(x0,y0), (x4,y4), (x2,y2)... ]\n",
        "\t\t\n",
        "    \tdata = torch.stack([item[0] for item in batch])\n",
        "\t\tmedian_data = torch.stack([item[1] for item in batch])\n",
        "\t\tdata_dict = {\n",
        "\t\t\t\"data\": data, \n",
        "\t\t\t\"median_data\": median_data,\n",
        "\t\t\t\"time_steps\": time_steps}\n",
        "\t\treturn data, median_data\n",
        "\t\t\n",
        "\t\n",
        "\n",
        "\tdef basic_collate_fn(batch, time_steps, args = args, device = device, data_type = \"train\"):\n",
        "\t\tbatch = torch.stack(batch)\n",
        "\t\tdata_dict = {\n",
        "\t\t\t\"data\": batch, \n",
        "\t\t\t\"time_steps\": time_steps}\n",
        "\n",
        "\t\tdata_dict = utils.split_and_subsample_batch(data_dict, args, data_type = data_type)\n",
        "\t\treturn data_dict\n",
        "\n",
        "\n",
        "\tdataset_name = args.dataset\n",
        "\n",
        "\tn_total_tp = args.timepoints + args.extrap\n",
        "\tmax_t_extrap = args.max_t / args.timepoints * n_total_tp\n",
        "\n",
        "\t##################################################################\n",
        "\t# MuJoCo dataset\n",
        "\tif dataset_name == \"hopper\":\n",
        "\t\tdataset_obj = HopperPhysics(root='data', download=True, generate=False, device = device)\n",
        "\t\tdataset = dataset_obj.get_dataset()[:args.n]\n",
        "\t\tdataset = dataset.to(device)\n",
        "\n",
        "\n",
        "\t\tn_tp_data = dataset[:].shape[1]\n",
        "\n",
        "\t\t# Time steps that are used later on for exrapolation\n",
        "\t\ttime_steps = torch.arange(start=0, end = n_tp_data, step=1).float().to(device)\n",
        "\t\ttime_steps = time_steps / len(time_steps)\n",
        "\n",
        "\t\tdataset = dataset.to(device)\n",
        "\t\ttime_steps = time_steps.to(device)\n",
        "\n",
        "\t\tif not args.extrap:\n",
        "\t\t\t# Creating dataset for interpolation\n",
        "\t\t\t# sample time points from different parts of the timeline, \n",
        "\t\t\t# so that the model learns from different parts of hopper trajectory\n",
        "\t\t\tn_traj = len(dataset)\n",
        "\t\t\tn_tp_data = dataset.shape[1]\n",
        "\t\t\tn_reduced_tp = args.timepoints\n",
        "\n",
        "\t\t\t# sample time points from different parts of the timeline, \n",
        "\t\t\t# so that the model learns from different parts of hopper trajectory\n",
        "\t\t\tstart_ind = np.random.randint(0, high=n_tp_data - n_reduced_tp +1, size=n_traj)\n",
        "\t\t\tend_ind = start_ind + n_reduced_tp\n",
        "\t\t\tsliced = []\n",
        "\t\t\tfor i in range(n_traj):\n",
        "\t\t\t\t  sliced.append(dataset[i, start_ind[i] : end_ind[i], :])\n",
        "\t\t\tdataset = torch.stack(sliced).to(device)\n",
        "\t\t\ttime_steps = time_steps[:n_reduced_tp]\n",
        "\n",
        "\t\t# Split into train and test by the time sequences\n",
        "\t\ttrain_y, test_y = utils.split_train_test(dataset, train_fraq = 0.8)\n",
        "\n",
        "\t\tn_samples = len(dataset)\n",
        "\t\tinput_dim = dataset.size(-1)\n",
        "\n",
        "\t\tbatch_size = min(args.batch_size, args.n)\n",
        "\t\ttrain_dataloader = DataLoader(train_y, batch_size = batch_size, shuffle=False,\n",
        "\t\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps, data_type = \"train\"))\n",
        "\t\ttest_dataloader = DataLoader(test_y, batch_size = n_samples, shuffle=False,\n",
        "\t\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps, data_type = \"test\"))\n",
        "\t\t\n",
        "\t\tdata_objects = {\"dataset_obj\": dataset_obj, \n",
        "\t\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\t\"n_test_batches\": len(test_dataloader)}\n",
        "\t\treturn data_objects\n",
        "\n",
        "\t##################################################################\n",
        "\t# Physionet dataset\n",
        "\n",
        "\tif dataset_name == \"physionet\":\n",
        "\t\ttrain_dataset_obj = PhysioNet('data/physionet', train=True, \n",
        "\t\t\t\t\t\t\t\t\t\tquantization = args.quantization,\n",
        "\t\t\t\t\t\t\t\t\t\tdownload=True, n_samples = min(10000, args.n), \n",
        "\t\t\t\t\t\t\t\t\t\tdevice = device)\n",
        "\t\t# Use custom collate_fn to combine samples with arbitrary time observations.\n",
        "\t\t# Returns the dataset along with mask and time steps\n",
        "\t\ttest_dataset_obj = PhysioNet('data/physionet', train=False, \n",
        "\t\t\t\t\t\t\t\t\t\tquantization = args.quantization,\n",
        "\t\t\t\t\t\t\t\t\t\tdownload=True, n_samples = min(10000, args.n), \n",
        "\t\t\t\t\t\t\t\t\t\tdevice = device)\n",
        "\n",
        "\t\t# Combine and shuffle samples from physionet Train and physionet Test\n",
        "\t\ttotal_dataset = train_dataset_obj[:len(train_dataset_obj)]\n",
        "\n",
        "\t\tif not args.classif:\n",
        "\t\t\t# Concatenate samples from original Train and Test sets\n",
        "\t\t\t# Only 'training' physionet samples are have labels. Therefore, if we do classifiction task, we don't need physionet 'test' samples.\n",
        "\t\t\ttotal_dataset = total_dataset + test_dataset_obj[:len(test_dataset_obj)]\n",
        "\n",
        "\t\t# Shuffle and split\n",
        "\t\ttrain_data, test_data = model_selection.train_test_split(total_dataset, train_size= 0.8, \n",
        "\t\t\trandom_state = 42, shuffle = True)\n",
        "\n",
        "\t\trecord_id, tt, vals, mask, labels = train_data[0]\n",
        "\n",
        "\t\tn_samples = len(total_dataset)\n",
        "\t\tinput_dim = vals.size(-1)\n",
        "\n",
        "\t\tbatch_size = min(min(len(train_dataset_obj), args.batch_size), args.n)\n",
        "\t\tdata_min, data_max = get_data_min_max(total_dataset)\n",
        "\n",
        "\t\ttrain_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"train\",\n",
        "\t\t\t\tdata_min = data_min, data_max = data_max))\n",
        "\t\ttest_dataloader = DataLoader(test_data, batch_size = n_samples, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn(batch, args, device, data_type = \"test\",\n",
        "\t\t\t\tdata_min = data_min, data_max = data_max))\n",
        "\n",
        "\t\tattr_names = train_dataset_obj.params\n",
        "\t\tdata_objects = {\"dataset_obj\": train_dataset_obj, \n",
        "\t\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\t\"n_test_batches\": len(test_dataloader),\n",
        "\t\t\t\t\t\"attr\": attr_names, #optional\n",
        "\t\t\t\t\t\"classif_per_tp\": False, #optional\n",
        "\t\t\t\t\t\"n_labels\": 1} #optional\n",
        "\t\treturn data_objects\n",
        "\n",
        "\t##################################################################\n",
        "\t# Human activity dataset\n",
        "\n",
        "\tif dataset_name == \"activity\":\n",
        "\t\tn_samples =  min(10000, args.n)\n",
        "\t\tdataset_obj = PersonActivity('data/PersonActivity', \n",
        "\t\t\t\t\t\t\tdownload=True, n_samples =  n_samples, device = device)\n",
        "\t\tprint(dataset_obj)\n",
        "\t\t# Use custom collate_fn to combine samples with arbitrary time observations.\n",
        "\t\t# Returns the dataset along with mask and time steps\n",
        "\n",
        "\t\t# Shuffle and split\n",
        "\t\ttrain_data, test_data = model_selection.train_test_split(dataset_obj, train_size= 0.8, \n",
        "\t\t\trandom_state = 42, shuffle = True)\n",
        "\n",
        "\t\ttrain_data = [train_data[i] for i in np.random.choice(len(train_data), len(train_data))]\n",
        "\t\ttest_data = [test_data[i] for i in np.random.choice(len(test_data), len(test_data))]\n",
        "\n",
        "\t\trecord_id, tt, vals, mask, labels = train_data[0]\n",
        "\t\tinput_dim = vals.size(-1)\n",
        "\n",
        "\t\tbatch_size = min(min(len(dataset_obj), args.batch_size), args.n)\n",
        "\t\ttrain_dataloader = DataLoader(train_data, batch_size= batch_size, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn_activity(batch, args, device, data_type = \"train\"))\n",
        "\t\ttest_dataloader = DataLoader(test_data, batch_size=n_samples, shuffle=False, \n",
        "\t\t\tcollate_fn= lambda batch: variable_time_collate_fn_activity(batch, args, device, data_type = \"test\"))\n",
        "\n",
        "\t\tdata_objects = {\"dataset_obj\": dataset_obj, \n",
        "\t\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\t\"n_test_batches\": len(test_dataloader),\n",
        "\t\t\t\t\t\"classif_per_tp\": True, #optional\n",
        "\t\t\t\t\t\"n_labels\": labels.size(-1)}\n",
        "\n",
        "\t\treturn data_objects\n",
        "\n",
        "\t########### 1d datasets ###########\n",
        "\n",
        "\t# Sampling args.timepoints time points in the interval [0, args.max_t]\n",
        "\t# Sample points for both training sequence and explapolation (test)\n",
        "\tdistribution = uniform.Uniform(torch.Tensor([0.0]),torch.Tensor([max_t_extrap]))\n",
        "\ttime_steps_extrap =  distribution.sample(torch.Size([n_total_tp-1]))[:,0]\n",
        "\ttime_steps_extrap = torch.cat((torch.Tensor([0.0]), time_steps_extrap))\n",
        "\ttime_steps_extrap = torch.sort(time_steps_extrap)[0]\n",
        "\n",
        "\tdataset_obj = None\n",
        "\t##################################################################\n",
        "\t# Sample a periodic function\n",
        "\tif dataset_name == \"periodic\":\n",
        "\t\tdataset_obj = Periodic_1d(\n",
        "\t\t\tinit_freq = None, init_amplitude = 1.,\n",
        "\t\t\tfinal_amplitude = 1., final_freq = None, \n",
        "\t\t\tz0 = 1.)\n",
        "\n",
        "\t##################################################################\n",
        "\n",
        "\tif dataset_obj is None:\n",
        "\t\traise Exception(\"Unknown dataset: {}\".format(dataset_name))\n",
        "\n",
        "\tdataset = dataset_obj.sample_traj(time_steps_extrap, n_samples = args.n, \n",
        "\t\tnoise_weight = args.noise_weight)\n",
        "\n",
        "\t# Process small datasets\n",
        "\tdataset = dataset.to(device)\n",
        "\ttime_steps_extrap = time_steps_extrap.to(device)\n",
        "\n",
        "\ttrain_y, test_y = utils.split_train_test(dataset, train_fraq = 0.8)\n",
        "\n",
        "\tn_samples = len(dataset)\n",
        "\tinput_dim = dataset.size(-1)\n",
        "\n",
        "\tbatch_size = min(args.batch_size, args.n)\n",
        "\ttrain_dataloader = DataLoader(train_y, batch_size = batch_size, shuffle=False,\n",
        "\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps_extrap, data_type = \"train\"))\n",
        "\ttest_dataloader = DataLoader(test_y, batch_size = args.n, shuffle=False,\n",
        "\t\tcollate_fn= lambda batch: basic_collate_fn(batch, time_steps_extrap, data_type = \"test\"))\n",
        "\t\n",
        "\tdata_objects = {#\"dataset_obj\": dataset_obj, \n",
        "\t\t\t\t\"train_dataloader\": utils.inf_generator(train_dataloader), \n",
        "\t\t\t\t\"test_dataloader\": utils.inf_generator(test_dataloader),\n",
        "\t\t\t\t\"input_dim\": input_dim,\n",
        "\t\t\t\t\"n_train_batches\": len(train_dataloader),\n",
        "\t\t\t\t\"n_test_batches\": len(test_dataloader)}\n",
        "\n",
        "\treturn data_objects\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "latentode_irregular_understanding.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
