{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "detsec_understanding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3BReyTTydHb"
      },
      "source": [
        "# DETSEC on EGC Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZW4m4HKyLqD"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QeSJ5ADyLqF"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from operator import itemgetter, attrgetter, methodcaller\n",
        "import tensorflow as tf\n",
        "# from tensorflow import keras\n",
        "# from tensorflow.keras import layers\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "# from tensorflow.contrib.rnn import DropoutWrapper\n",
        "import time\n",
        "import calendar\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import normalized_mutual_info_score\n",
        "# from tensorflow.contrib import rnn\n",
        "from scipy.spatial import distance\n",
        "from operator import itemgetter\n",
        "import random as rand\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lab5G36o2732"
      },
      "source": [
        "from tensorflow.compat.v1.nn.rnn_cell import DropoutWrapper"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vIHKCQ63Kku"
      },
      "source": [
        "from tensorflow.compat.v1.nn import rnn_cell as rnn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ36Y8ml4WoZ"
      },
      "source": [
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea0GohUn6DXx"
      },
      "source": [
        "tf.compat.v1.reset_default_graph()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWX99TmdyLqF"
      },
      "source": [
        "## buildMaskBatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WlSE01IyLqG"
      },
      "source": [
        "def buildMaskBatch(batch_seql, max_size):\n",
        "\tmask_batch = []\n",
        "\tfor el in batch_seql:\n",
        "\t\tmask_batch.append(  np.concatenate( (np.ones(el), np.zeros(max_size - el)) ) )\n",
        "\treturn np.array(mask_batch)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyj0BiMpyLqG"
      },
      "source": [
        "## extractFeatures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS-_o0B-yLqG"
      },
      "source": [
        "def extractFeatures(ts_data, seq_length, mask_val):\n",
        "\tbatchsz = 1024\n",
        "\titerations = int(ts_data.shape[0] / batchsz)\n",
        "\tif ts_data.shape[0] % batchsz != 0:\n",
        "\t\titerations+=1\n",
        "\tfeatures = None\n",
        "\n",
        "\tfor ibatch in range(iterations):\n",
        "\t\tbatch_data, batch_seqL = getBatch(ts_data, seq_length, ibatch, batchsz)\n",
        "\t\tbatch_mask, _ = getBatch(mask_val, mask_val, ibatch, batchsz)\n",
        "\t\tpartial_features = sess.run(embedding,feed_dict={input_t:batch_data, seqL:batch_seqL, mask: batch_mask})\n",
        "\t\tif features is None:\n",
        "\t\t\tfeatures = partial_features\n",
        "\t\telse:\n",
        "\t\t\tfeatures = np.vstack((features, partial_features))\n",
        "\n",
        "\t\tdel batch_data\n",
        "\t\tdel batch_mask\n",
        "\treturn features"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lPqnChyyLqH"
      },
      "source": [
        "## gate and gating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lx5P24byLqH"
      },
      "source": [
        "def gate(vec):\n",
        "    mask = tf.compat.v1.layers.dense(vec, vec.get_shape()[1], activation=tf.sigmoid)\n",
        "    return mask"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGdwRXS7yLqH"
      },
      "source": [
        "def gating(outputs_list, mask):\n",
        "\tgating_results = None\n",
        "\tif mask is None:\n",
        "\t\tfor i in range(len(outputs_list)):\n",
        "\t\t\tval = outputs_list[i]\n",
        "\t\t\tmultiplication = val * gate(val)\n",
        "\t\t\tif gating_results is None:\n",
        "\t\t\t\tgating_results = multiplication\n",
        "\t\t\telse:\n",
        "\t\t\t\tgating_results = gating_results + multiplication\n",
        "\t\treturn gating_results\n",
        "\n",
        "\tfor i in range(len(outputs_list)):\n",
        "\t\tval = outputs_list[i]\n",
        "\t\tmultiplication = val * gate(val)\n",
        "\t\tmultiplication = tf.compat.v1.transpose( multiplication )\n",
        "\t\tmultiplication = multiplication * mask[:,i]\n",
        "\t\tmultiplication = tf.compat.v1.transpose( multiplication )\n",
        "\t\tif gating_results is None:\n",
        "\t\t\tgating_results = multiplication\n",
        "\t\telse:\n",
        "\t\t\tgating_results = gating_results +multiplication\n",
        "\n",
        "\treturn gating_results"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfS0KgxKyLqI"
      },
      "source": [
        "## attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbwI4oPiyLqI"
      },
      "source": [
        "def attention(outputs_list, nunits, attention_size):\n",
        "\toutputs = tf.stack(outputs_list, axis=1)\n",
        "\n",
        "\t# Trainable parameters\n",
        "\tW_omega = tf.compat.v1.Variable(tf.compat.v1.random_normal([nunits, attention_size], stddev=0.1))\n",
        "\tb_omega = tf.compat.v1.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))\n",
        "\tu_omega = tf.compat.v1.Variable(tf.compat.v1.random_normal([attention_size], stddev=0.1))\n",
        "\n",
        "\t# Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
        "\t#  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
        "\tv = tf.compat.v1.tanh(tf.compat.v1.tensordot(outputs, W_omega, axes=1) + b_omega)\n",
        "\t# For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
        "\tvu = tf.compat.v1.tensordot(v, u_omega, axes=1)   # (B,T) shape\n",
        "\talphas = tf.compat.v1.nn.softmax(vu)              # (B,T) shape also\n",
        "\n",
        "\toutput = tf.compat.v1.reduce_sum(outputs * tf.compat.v1.expand_dims(alphas, -1), 1)\n",
        "\toutput = tf.compat.v1.reshape(output, [-1, nunits])\n",
        "\treturn output\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnYT3FyYyLqJ"
      },
      "source": [
        "## getBatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO8fLwQbyLqJ"
      },
      "source": [
        "def getBatch(X, Y, i, batch_size):\n",
        "    start_id = i*batch_size\n",
        "    end_id = min( (i+1) * batch_size, X.shape[0])\n",
        "    batch_x = X[start_id:end_id]\n",
        "    batch_y = Y[start_id:end_id]\n",
        "    return batch_x, batch_y\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-vyE_ylyLqJ"
      },
      "source": [
        "## AE3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFWCXcOqyLqJ"
      },
      "source": [
        "\n",
        "def AE3(x, b_size, n_dim, seqL, mask, toReuse):\n",
        "\twith tf.compat.v1.variable_scope(\"ENCDEC\",reuse=toReuse):\n",
        "\t\tn_splits = x.get_shape()[1] / n_dim\n",
        "\t\tn_splits = int(n_splits)\n",
        "\t\tx_list = tf.compat.v1.split(x, n_splits, axis=1)\n",
        "\t\tx_list_bw = tf.compat.v1.stack(x_list[::-1],axis=1)\n",
        "\t\tx_list = tf.compat.v1.stack(x_list,axis=1)\n",
        "\t\t#FIXED TO 512 for big dataset\n",
        "\t\t#FOR SMALL DATASET WE RECOMMEND 64 OR 32\n",
        "\t\tnunits = 512\n",
        "\t\toutputsEncLFW = None\n",
        "\t\toutputsEncLBW = None\n",
        "\n",
        "\t\twith tf.compat.v1.variable_scope(\"encoderFWL\", reuse=toReuse):\n",
        "\t\t\tcellEncoderFW = rnn.GRUCell(nunits)\n",
        "\t\t\toutputsEncLFW,_=tf.compat.v1.nn.dynamic_rnn(cellEncoderFW, x_list, sequence_length = seqL, dtype=\"float32\")\n",
        "\n",
        "\t\twith tf.compat.v1.variable_scope(\"encoderBWL\", reuse=toReuse):\n",
        "\t\t\tcellEncoderBW = rnn.GRUCell(nunits)\n",
        "\t\t\toutputsEncLBW,_=tf.compat.v1.nn.dynamic_rnn(cellEncoderBW, x_list_bw, sequence_length = seqL, dtype=\"float32\")\n",
        "\n",
        "\t\tfinal_list_fw = []\n",
        "\t\tfor i in range( n_splits ):\n",
        "\t\t\tfinal_list_fw.append( outputsEncLFW[:,i,:] )\n",
        "\n",
        "\t\tfinal_list_bw = []\n",
        "\t\tfor i in range( n_splits ):\n",
        "\t\t\tfinal_list_bw.append( outputsEncLBW[:,i,:] )\n",
        "\n",
        "\t\tencoder_fw = attention(final_list_fw, nunits, nunits)\n",
        "\t\tencoder_bw = attention(final_list_bw, nunits, nunits)\n",
        "\t\tencoder = gate(encoder_fw) * encoder_fw + gate(encoder_bw) * encoder_bw\n",
        "\n",
        "\t\tx_list2decode = []\n",
        "\t\tx_list2decode_bw = []\n",
        "\t\tfor i in range(n_splits):\n",
        "\t\t\tx_list2decode.append( tf.compat.v1.identity(encoder) )\n",
        "\t\t\tx_list2decode_bw.append( tf.compat.v1.identity(encoder) )\n",
        "\n",
        "\t\tx_list2decode = tf.compat.v1.stack(x_list2decode, axis=1)\n",
        "\t\tx_list2decode_bw = tf.compat.v1.stack(x_list2decode_bw, axis=1)\n",
        "\n",
        "\t\twith tf.compat.v1.variable_scope(\"decoderG\", reuse=toReuse):\n",
        "\t\t\tcellDecoder = rnn.GRUCell(nunits)\n",
        "\t\t\toutputsDecG,_=tf.compat.v1.nn.dynamic_rnn(cellDecoder, x_list2decode, sequence_length = seqL, dtype=\"float32\")\n",
        "\n",
        "\t\twith tf.compat.v1.variable_scope(\"decoderGFW\", reuse=toReuse):\n",
        "\t\t\tcellDecoder = rnn.GRUCell(nunits)\n",
        "\t\t\toutputsDecGFW,_=tf.compat.v1.nn.dynamic_rnn(cellDecoder, x_list2decode_bw, sequence_length = seqL, dtype=\"float32\")\n",
        "\n",
        "\t\tout_list = []\n",
        "\t\tout_list_bw = []\n",
        "\t\tfor i in range(n_splits):\n",
        "\t\t\ttemp_cell = outputsDecG[:,i,:]\n",
        "\t\t\ttt = tf.compat.v1.layers.dense(temp_cell, n_dim, activation=None)\n",
        "\t\t\tout_list.append( tt )\n",
        "\n",
        "\t\t\ttemp_cell2 = outputsDecGFW[:,i,:]\n",
        "\t\t\ttt2 = tf.compat.v1.layers.dense(temp_cell, n_dim, activation=None)\n",
        "\t\t\tout_list_bw.append(tt2)\n",
        "\n",
        "\t\treconstruct = tf.compat.v1.concat(out_list, axis=1)\n",
        "\t\treconstruct2 = tf.compat.v1.concat(out_list_bw[::1], axis=1)\n",
        "\n",
        "\t\treturn reconstruct, reconstruct2, encoder"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q8UU5JKyLqK"
      },
      "source": [
        "## dirName, n_dims, n_clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_1nPLpsyLqL"
      },
      "source": [
        "dirName = \"ECG\"\n",
        "n_dims = 2\n",
        "n_clusters = 6\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV8cTnZ0yLqL"
      },
      "source": [
        "## output_dir, dataFileName, seqLFileName"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ww8iQt4yLqL"
      },
      "source": [
        "output_dir = 'output'\n",
        "#DATA FILE with size:\t(nSamples, (n_dims * max_length) )\n",
        "dataFileName = dirName+\"/data.npy\"\n",
        "#SEQUENCE LENGTH FILE with size: ( nSamples, )\n",
        "#It contains the sequence length (multiplied by n_dims) for each sequence with positional reference to the data.npy file\n",
        "#This means that, if a time series has 4 attributes and it has a lenght equal to 20, the corresponding values in the seq_length.npy file will be 80\n",
        "seqLFileName = dirName+\"/seq_length.npy\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zAOLQaVyLqL"
      },
      "source": [
        "## data, n_row, n_col, seqLength, orig_data, orig_seqLength, n_feat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-pJQxkByLqM"
      },
      "source": [
        "data = np.load(dataFileName)\n",
        "n_row = data.shape[0]\n",
        "n_col = data.shape[1]\n",
        "\n",
        "seqLength = np.load(seqLFileName)\n",
        "\n",
        "orig_data = data\n",
        "orig_seqLength = seqLength\n",
        "n_feat = data.shape[1]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqqtpvVpyLqM"
      },
      "source": [
        "## b_size, dropOut, seqL, input_t, mask, target_t, sess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3HqyXSb3nNk",
        "outputId": "473931ce-9909-4738-e574-047656f33b8c"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxdgjbGfyLqM"
      },
      "source": [
        "b_size = tf.compat.v1.placeholder(tf.float32, () , name=\"b_size\")\n",
        "dropOut = tf.compat.v1.placeholder(tf.float32, () , name=\"dropOut\")\n",
        "seqL = tf.compat.v1.placeholder(tf.float32, (None) , name=\"seqL\")\n",
        "input_t = tf.compat.v1.placeholder(tf.float32, (None, n_feat), name='inputs')\n",
        "mask = tf.compat.v1.placeholder(tf.float32, (None, n_feat), name='mask')\n",
        "target_t = tf.compat.v1.placeholder(tf.float32, (None, n_feat), name='target_t')\n",
        "\n",
        "sess = tf.compat.v1.InteractiveSession()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "49jSh-_P5MgV",
        "outputId": "e68a6cd9-8246-4634-80eb-62a486445478"
      },
      "source": [
        "input_t.get_shape()[1].value"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-95dee7e06d91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'value'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUeJxXeHyLqM"
      },
      "source": [
        "## reconstruction (forward), reconstruction2 (backward), embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA7CShLWyLqN",
        "outputId": "637c91b6-10ae-4de7-eee5-ffc53962599c"
      },
      "source": [
        "\n",
        "reconstruction, reconstruction2, embedding = AE3(input_t, b_size, n_dims, seqL, mask, False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From <ipython-input-12-ac80ad27e55f>:17: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:572: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:582: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:524: UserWarning: `tf.nn.rnn_cell.GRUCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.GRUCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.GRUCell` is deprecated and will be removed \"\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:236: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer_v1.py:1676: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJmIOCctyLqN"
      },
      "source": [
        "## b_centroids, loss_fw, loss_bw, cost, opt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97kl9ATByLqN"
      },
      "source": [
        "b_centroids = tf.compat.v1.placeholder(tf.float32, (None, embedding.get_shape()[1]), name='b_centroids')\n",
        "loss_fw = tf.compat.v1.square( (target_t - reconstruction) * mask )\n",
        "loss_fw = tf.compat.v1.reduce_sum(loss_fw, axis=1)\n",
        "\n",
        "loss_bw = tf.compat.v1.square( (target_t - reconstruction2) * mask )\n",
        "loss_bw = tf.compat.v1.reduce_sum(loss_bw, axis=1)\n",
        "\n",
        "cost = tf.compat.v1.reduce_mean(loss_fw) + tf.compat.v1.reduce_mean(loss_bw)  #+ latent_loss\n",
        "opt = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTBkWYIjyLqN"
      },
      "source": [
        "# CLUSTERING REFINEMENT CENTROIDS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j4dYUSuyLqN"
      },
      "source": [
        "## loss_crc, cost_crc, opt_crc, batchsz, hm_epochs, iterations, max_length, best_loss, noise_factor, th, new_centroids, kmeans_labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vskI_Ch9yLqN"
      },
      "source": [
        "loss_crc = tf.compat.v1.reduce_sum(tf.square( embedding - b_centroids ), axis=1)\n",
        "loss_crc = tf.compat.v1.reduce_mean(loss_crc)\n",
        "\n",
        "cost_crc = loss_crc + cost\n",
        "opt_crc = tf.compat.v1.train.AdamOptimizer(learning_rate=0.0001).minimize(cost_crc)\n",
        "\n",
        "tf.compat.v1.global_variables_initializer().run()\n",
        "\n",
        "batchsz = 16\n",
        "hm_epochs = 300\n",
        "\n",
        "iterations = int(data.shape[0] / batchsz)\n",
        "max_length = data.shape[1]\n",
        "\n",
        "if data.shape[0] % batchsz != 0:\n",
        "    iterations+=1\n",
        "\n",
        "best_loss = sys.float_info.max\n",
        "noise_factor = 0.01\n",
        "\n",
        "th = 50#number of epochs for the autoencoder pretraining step\n",
        "new_centroids = None\n",
        "kmeans_labels = None\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ_wjyC8yLqN"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6n4soZWyLqN",
        "outputId": "767cbea7-e541-4d24-e9c8-dc46ed367d73"
      },
      "source": [
        "for e in range(hm_epochs):\n",
        "\tstart = time.time()\n",
        "\tlossi = 0\n",
        "\tdata, seqLength = shuffle(data, seqLength, random_state=0)\n",
        "\tcostT = 0\n",
        "\tcostT2 = 0\n",
        "\tif e < th:\n",
        "\t\tdata, seqLength = shuffle(data, seqLength, random_state=0)\n",
        "\telse:\n",
        "\t\tmask_val = buildMaskBatch(seqLength, max_length)\n",
        "\t\tfeatures = extractFeatures(data, seqLength, mask_val)\n",
        "\t\tkmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=random.randint(1,10000000)).fit(features)\n",
        "\t\tnew_centroids = kmeans.cluster_centers_\n",
        "\t\tkmeans_labels = kmeans.labels_\n",
        "\t\tembeddings_data = extractFeatures(data, seqLength, mask_val)\n",
        "\t\tdata, seqLength, kmeans_labels = shuffle(data, seqLength, kmeans_labels, random_state=0)\n",
        "\n",
        "\tfor ibatch in range(iterations):\n",
        "\t\tbatch_data, batch_seql = getBatch(data, seqLength, ibatch, batchsz)\n",
        "\t\tmask_batch = buildMaskBatch(batch_seql, batch_data.shape[1])\n",
        "\t\tcost_L = 0\n",
        "\n",
        "\t\t#PRETRAINING ENCODER for 50 EPOCHS\n",
        "\t\tif e < th:\n",
        "\t\t\t_, cost_L= sess.run([opt, cost],feed_dict={ input_t:batch_data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_t:batch_data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tb_size: batch_data.shape[0],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tseqL: batch_seql,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tmask: mask_batch\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t})\n",
        "\t\t\tcost_C=0\n",
        "\t\t#COMBINED TRAINING WITH ENCO/DEC + CLUSTERING REFINEMENT\n",
        "\t\telse:\n",
        "\t\t\tbatch_km_labels, _ = getBatch(kmeans_labels, kmeans_labels, ibatch, batchsz)\n",
        "\t\t\tbatch_centroids = []\n",
        "\t\t\tfor el in batch_km_labels:\n",
        "\t\t\t\tbatch_centroids.append( new_centroids[el]  )\n",
        "\t\t\tbatch_centroids = np.array(batch_centroids)\n",
        "\t\t\t_, cost_L, cost_C = sess.run([opt_crc, cost, loss_crc], feed_dict={\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tinput_t:batch_data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_t:batch_data,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t#centroids: centroids_val,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tb_size: int(batch_data.shape[0]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tseqL: batch_seql,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tmask: mask_batch,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tb_centroids: batch_centroids\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t})\n",
        "\n",
        "\t\tcostT+=cost_L\n",
        "\t\tcostT2+=cost_C\n",
        "\t\tdel batch_data\n",
        "\t\tdel batch_seql\n",
        "\t\tdel mask_batch\n",
        "\n",
        "\tmask_val = buildMaskBatch(seqLength, max_length)\n",
        "\tembedd = extractFeatures(data, seqLength, mask_val)\n",
        "\tkmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embedd)\n",
        "\tprint(\"Epoch:\",e,\"| COST_EMB:\",costT/iterations,\" | COST_CRC: \", costT2/iterations)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | COST_EMB: 73.77821731567383  | COST_CRC:  0.0\n",
            "Epoch: 1 | COST_EMB: 29.887936225304237  | COST_CRC:  0.0\n",
            "Epoch: 2 | COST_EMB: 9.16568899154663  | COST_CRC:  0.0\n",
            "Epoch: 3 | COST_EMB: 4.033777236938477  | COST_CRC:  0.0\n",
            "Epoch: 4 | COST_EMB: 2.8778927142803488  | COST_CRC:  0.0\n",
            "Epoch: 5 | COST_EMB: 2.295543936582712  | COST_CRC:  0.0\n",
            "Epoch: 6 | COST_EMB: 1.9883648065420299  | COST_CRC:  0.0\n",
            "Epoch: 7 | COST_EMB: 1.9273168307084303  | COST_CRC:  0.0\n",
            "Epoch: 8 | COST_EMB: 1.772748406116779  | COST_CRC:  0.0\n",
            "Epoch: 9 | COST_EMB: 1.7467064674084003  | COST_CRC:  0.0\n",
            "Epoch: 10 | COST_EMB: 1.6977952627035289  | COST_CRC:  0.0\n",
            "Epoch: 11 | COST_EMB: 1.6853362780350905  | COST_CRC:  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pPYaubhyLqO"
      },
      "source": [
        "## output_dir, mask_val, embedd, kmeans, data representation, clustering assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGu1GKPyyLqO"
      },
      "source": [
        "output_dir = output_dir+\"_detsec512\"\n",
        "if not os.path.exists(output_dir):\n",
        "\tos.mkdir(output_dir)\n",
        "\n",
        "mask_val = buildMaskBatch(orig_seqLength, max_length)\n",
        "embedd = extractFeatures(orig_data, orig_seqLength, mask_val)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(embedd)\n",
        "\n",
        "#SAVE THE DATA REPRESENTATION\n",
        "np.save(\"detsec_features.npy\", embedd )\n",
        "#SAVE THE CLUSTERING ASSIGNMENT\n",
        "np.save(\"detsec_clust_assignment.npy\", np.array(kmeans.labels_) )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}