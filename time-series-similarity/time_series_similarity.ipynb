{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "time_series_similarity.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i_oESKqElmfU",
        "kYItHghUnjBn"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA-p-8kPspwH"
      },
      "source": [
        "# Install this (important)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mModaxDbsFH9",
        "outputId": "497607a0-68c9-4433-b49e-c97141eeaf88"
      },
      "source": [
        "# Install the latest version of author's repo neural ode implementation\n",
        "!git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "!cd torchdiffeq && pip install -e .\n",
        "!ls torchdiffeq/torchdiffeq"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdiffeq'...\n",
            "remote: Enumerating objects: 1138, done.\u001b[K\n",
            "remote: Counting objects: 100% (434/434), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 1138 (delta 256), reused 400 (delta 240), pack-reused 704\u001b[K\n",
            "Receiving objects: 100% (1138/1138), 8.29 MiB | 19.88 MiB/s, done.\n",
            "Resolving deltas: 100% (682/682), done.\n",
            "Obtaining file:///content/torchdiffeq\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu102)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "  Running setup.py develop for torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n",
            "_impl  __init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsnxSsoMzQrp",
        "outputId": "e63ea542-217c-46e7-c58e-52014be23e4e"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.63.tar.gz (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 21.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.63-py2.py3-none-any.whl size=23918 sha256=6ae8b41fa9d58fcfa8f06229453cbab263f482aa047c891093b1cca18471f5c2\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/87/8b/7ec24486e001d3926537f5f7801f57a74d181be25b11157983\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.3 yfinance-0.1.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmW7-fNXvpjK"
      },
      "source": [
        "# Data Collection and Clustering Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwJslKhovvNW"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcAd1jnYvt7H"
      },
      "source": [
        "## Rough of Data Collection\n",
        "\n",
        "1. Get Open and Close Price of asset (o, c) for each trading day.\n",
        "2. Transform it into sequences.\n",
        "    - $d_{i} : {o_{1}c_{1} ... o_{5}c_{5}}$\n",
        "    - where $d_{i}$ is a sequence of o and c for the week `i`.\n",
        "3. Transform $d_{i}$ to sequences of lag * len($d_{i}$) length.\n",
        "4. Normalize these sequences to a range (0, 1).\n",
        "5. Clustering algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYHu8FG1w9E9",
        "outputId": "4efb9a36-e4a9-462e-c695-0ff1a81dcac9"
      },
      "source": [
        "# 1. Get Open and Close Price of asset (o, c) for each trading day.\n",
        "# libraries\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "print(f\"Get Open and Close Price of Assets\")\n",
        "def download_raw_stock_data(filepath, tickers, start, end, period = '1d'):\n",
        "    \"\"\"\n",
        "    Download Stock tickers\n",
        "    :Parameters:\n",
        "        filepath: str\n",
        "            path to store the raw data\n",
        "        tickers : str, list\n",
        "            List of tickers to download\n",
        "        period: str\n",
        "            the frequency at which to gather the data; common options would include ‘1d’ (daily), ‘1mo’ (monthly), ‘1y’ (yearly)\n",
        "        start: str\n",
        "            the date to start gathering the data. For example ‘2010–1–1’\n",
        "        end: str\n",
        "            the date to end gathering the data. For example ‘2020–1–25’\n",
        "    \n",
        "    \"\"\"\n",
        "    #define the ticker symbol\n",
        "    tickerSymbol = tickers\n",
        "\n",
        "    #get data on this ticker\n",
        "    tickerData = yf.Ticker(tickerSymbol)\n",
        "\n",
        "    #get the historical prices for this ticker\n",
        "    tickerDf = tickerData.history(period=period, start=start, end=end)\n",
        "    tickerDf.to_csv(filepath)\n",
        "\n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "path = f\"raw-stock-data/data-2000-2021\"\n",
        "if not os.path.exists(path):\n",
        "    # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "    # Create a new directory\n",
        "    os.makedirs(path)\n",
        "    print(f\"{path} directory is created\")\n",
        "period = '1d'\n",
        "start='2000-1-1'\n",
        "end='2021-8-31'\n",
        "for tickerName, ticker in dict_tickers.items():\n",
        "    tickerName = tickerName\n",
        "    ticker = ticker\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    download_raw_stock_data(filepath, ticker, start, end, period)\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(f\"The size of each asset\")\n",
        "import pandas as pd\n",
        "for tickerName in dict_tickers.keys():\n",
        "    df = pd.read_csv(f\"{path}/{tickerName}.csv\")\n",
        "    print(f\"{tickerName} size: {len(df)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get Open and Close Price of Assets\n",
            "raw-stock-data/data-2000-2021 directory is created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0mxiP_J4LvW"
      },
      "source": [
        "len(dict_tickers.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PslCRoOlBIEc"
      },
      "source": [
        "### 2. Transform it into sequences.\n",
        "    - $d_{i} : {o_{1}c_{1} ... o_{5}c_{5}}$\n",
        "    - where $d_{i}$ is a sequence of o and c for the week `i`.\n",
        "3. Transform $d_{i}$ to sequences of lag * len($d_{i}$) length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL0nozG8ynms"
      },
      "source": [
        "# 2. Get weekly data.\n",
        "# 3. Transform $d_{i}$ to sequences of lag * len($d_{i}$) length.\n",
        "\n",
        "def stockDataTransformer(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    df1 = df[['Open', 'Close']].copy()\n",
        "    data = df1.values\n",
        "    n_samples = data.shape[0]//10*10\n",
        "    reshape_number = n_samples*data.shape[1]//10\n",
        "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
        "    return data1\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "def split_data(perc_train, perc_valid, lag, data_orig, data_m1, n_features_orig, n_features_median):\n",
        "    values = data_m1\n",
        "    \n",
        "    sizeOfReframed = len(data_m1)\n",
        "    len_train = int(perc_train*sizeOfReframed) # int(sizeOfReframed - len_test) # - len_valid)\n",
        "    train_data_orig = data_orig[:len_train, :]\n",
        "    # valid = values[len_train:len_valid+len_train, :]\n",
        "    test_data_orig = data_orig[len_train:, :]  # [len_valid+len_train:, :]\n",
        "    # n_features = n_features\n",
        "    \n",
        "    train_data_ml = values[:len_train, :]\n",
        "    test_data_ml = values[len_train:, :] \n",
        "    # split into input and outputs\n",
        "    n_obs = lag * n_features_orig\n",
        "    n_obs_median = (lag+forecast) * n_features_median\n",
        "    train_X, train_y = train_data_orig[:, :n_obs], train_data_ml[:, :n_obs_median]\n",
        "    test_X, test_y = test_data_orig[:, :n_obs], test_data_ml[:, :n_obs_median]\n",
        "    # valid_X, valid_y = valid[:, :n_obs], valid[:, -1]\n",
        "    print(train_X.shape, len(train_X), train_y.shape)\n",
        "    \n",
        "    # reshape input to be 3D [samples, features, lag]\n",
        "    train_X = train_X.reshape((train_X.shape[0], n_features_orig, lag))\n",
        "    test_X = test_X.reshape((test_X.shape[0], n_features_orig, lag))\n",
        "    # valid_X = valid_X.reshape((valid_X.shape[0], lag, n_features))\n",
        "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)  # , valid_X.shape, valid_y.shape)\n",
        "    \n",
        "    # Get the reconstruction train_y, test_y and extrapolated train_y, test_y\n",
        "    train_y_recon, train_y_extrapol = train_y[:, :lag], train_y[:, lag:]\n",
        "    test_y_recon, test_y_extrapol = test_y[:, :lag], test_y[:, lag:]\n",
        "    dataload = {\n",
        "        'train_data_orig': train_data_orig,\n",
        "        'test_data_orig': test_data_orig,\n",
        "        'train_data_ml': train_data_ml,\n",
        "        'test_data_ml': test_data_ml,\n",
        "        # 'valid': valid,\n",
        "        'train_X': train_X,\n",
        "        'train_y': train_y,\n",
        "        'test_X': test_X,\n",
        "        'test_y': test_y,\n",
        "        'n_features_orig': n_features_orig,\n",
        "        'n_features_median': n_features_median,\n",
        "        'n_obs': n_obs,\n",
        "        'n_obs_median': n_obs_median,\n",
        "        # 'valid_X': valid_X,\n",
        "        # 'valid_y': valid_y,\n",
        "        'train_y_recon': train_y_recon,\n",
        "        'train_y_extrapol': train_y_extrapol,\n",
        "        'test_y_recon': test_y_recon,\n",
        "        'test_y_extrapol': test_y_extrapol\n",
        "    }\n",
        "    \n",
        "    return dataload\n",
        "\n",
        "def get_median(array, axis = 1):\n",
        "    # https://numpy.org/doc/stable/reference/generated/numpy.median.html\n",
        "    return np.median(array, axis = axis).reshape(data_size, 1)  #, keepdims=True)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RLOgCZV_oUf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8q5WFNK0eDZ"
      },
      "source": [
        "from pandas import concat\n",
        "import numpy as np\n",
        "week_sequence = {}\n",
        "median_data_dict = {}\n",
        "lag = 5\n",
        "for tickerName in dict_tickers.keys():\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    # Get the data in the required format\n",
        "    data = stockDataTransformer(filepath)\n",
        "    # # Total Data Size\n",
        "    data_size = data.shape[0]\n",
        "    print(f\"{tickerName} data.shape {data.shape}\")\n",
        "    data_orig = series_to_supervised(data, lag).values\n",
        "    print(f'{tickerName} Data Original after series to supervised on data {data_orig.shape}')\n",
        "    week_sequence[tickerName] = data_orig\n",
        "\n",
        "    median_data = get_median(data)\n",
        "    print(f'{tickerName} Median data')\n",
        "    # Median data for each week\n",
        "    print(f\"{tickerName} median_data.shape {median_data.shape}\")\n",
        "    # print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
        "    print('\\n')\n",
        "\n",
        "    # Convert median_data to (n_samples, 5) matrix\n",
        "    data_m1 = series_to_supervised(median_data, lag).values\n",
        "    median_data_dict[tickerName] = data_m1\n",
        "    print(f'{tickerName} Median data after series to supervised')\n",
        "    print(f\"{tickerName} data_m1.shape {data_m1.shape}\")\n",
        "    # print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvIdGj4r1chm"
      },
      "source": [
        "import numpy as np\n",
        "data = week_sequence['Apple']\n",
        "# 4. Bundle all sequences together\n",
        "for tickerName in week_sequence.keys():\n",
        "    if tickerName != 'Apple':\n",
        "        data1 = week_sequence[tickerName]\n",
        "        data = np.concatenate((data, data1))\n",
        "        print(f\"data.shape {data.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_wWh2OJFJaB"
      },
      "source": [
        "data_df = pd.DataFrame(data)\n",
        "data_df.to_csv(f\"all_assets_sequences.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tgcw0tj54F6g"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbdkOC5Uzde6"
      },
      "source": [
        "# import numpy as np\n",
        "# a1 = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "# a2 = np.array([[7, 8, 9], [10, 11, 12], [13, 14, 15]])\n",
        "# a3 = np.array([[7, 8, 9], [10, 11, 12], [13, 14, 15]])\n",
        "# np.concatenate((a1, a2, a3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5MG5n5AG2u6"
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6ci3UIjCNPv"
      },
      "source": [
        "data_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5IdKSsxBO1H"
      },
      "source": [
        "### 4. Normalize these sequences to a range (0, 1).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoi5Czkuyr9C"
      },
      "source": [
        "# 4. Normalize these sequences to a range (0, 1).\n",
        "from sklearn import preprocessing \n",
        "# https://www.journaldev.com/45109/normalize-data-in-python\n",
        "# Normalizes the sample\n",
        "data_normalized = preprocessing.normalize(data_df)\n",
        "data_normalized_df = pd.DataFrame(data_normalized)\n",
        "data_normalized_df.to_csv(f\"all_assets_sequences_lag{lag+1}_normalized.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur_S8-Dk7yX-"
      },
      "source": [
        "### 5. Clustering algorithm. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1Be3yIbFqbi"
      },
      "source": [
        "#### k-means clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZLxTLr6GcBt"
      },
      "source": [
        "# 5. Clustering algorithm. \n",
        "# K Means\n",
        "\n",
        "# K-Means Clustering\n",
        "\n",
        "# One of the most commonly used methods of clustering is K-means Clustering \n",
        "# K Means allows us to define the required number of clusters\n",
        "# Now we first start by taking an arbitrary number of k. Let’s say we take k=3 \n",
        "# Now to form three groups from the set of data\n",
        "# Algorithm chooses three random points as the centroid \n",
        "# It also computes Euclidean distances from the centroid to all other data points\n",
        "# The algorithm after measuring the distances of all the data points \n",
        "# from the centroid associates each data point with a centroid based on its proximity\n",
        "\n",
        "# Silhouette Score\n",
        "# Clusters are well apart from each other as the silhouette score is closer to 1\n",
        "# Silhouette Coefficient score is a metric used to calculate the goodness of a clustering technique \n",
        "# Its value ranges from -1 to 1.\n",
        "# 1: Means clusters are well apart from each other and clearly distinguished.\n",
        "# 0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
        "# -1: Means clusters are assigned in the wrong way.\n",
        "\n",
        "# Deciding Value of K\n",
        "# The most crucial aspect of K-Means clustering is deciding the value of K\n",
        "# We do this by performing Elbow Analysis \n",
        "\n",
        "# Library\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VlMi0yZyr0n"
      },
      "source": [
        "\n",
        "wcss=[]\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300,\n",
        "                    n_init = 10, random_state = 0)\n",
        "    kmeans.fit(data_normalized)\n",
        "    print(f\"Inertia for clusters = {i}: {kmeans.inertia_}\")\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "print('Within Cluster Sum of Square (WCSS) for 10 Clusters')\n",
        "\n",
        "# Inference : \n",
        "# Cluster 1 means only one cluster, inshort variance of the dataset\n",
        "# Variance in Cluster 1 = 1515\n",
        "# Cluster 2 means two cluster i.e, Variance in Cluster 2 = 104\n",
        "# Same way for Variance in Cluster 3 = 79\n",
        "# Vairance in Cluster is also knows as Within Cluster Sum of Square (WCSS)\n",
        "# Normally as we increase number of clusters, within Sum of Square will decrease "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waK-0qDRF1z7"
      },
      "source": [
        "# We now plot the WCSS obtained \n",
        "\n",
        "# WCSS or within-cluster sum of squares is a measure of \n",
        "# how internally coherent the clusters are K-Means tries to minimize this criterion\n",
        "\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('The elbow method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "\n",
        "# Inference : \n",
        "# In the elbow graph, we look for the points where the drop falls and the line smoothens out\n",
        "# In the above graph, this happens for k=2. \n",
        "# Another way of understanding this is that we note the point at which the WCSS is less \n",
        "# and try to find the number of clusters for our dataset. \n",
        "# We see that at the number of clusters = 2, WCSS is less than 100, which is good for us. \n",
        "# So we take k =2.\n",
        "# We will check Silhouette Coefficient also for 2 and 3 Clusters respectively"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAPxJ-hZGW68"
      },
      "source": [
        "# Running K-Means Model\n",
        "\n",
        "# We now run K-Means clustering for obtaining a 2 cluster solution.\n",
        "cluster_Kmeans = KMeans(n_clusters=3)\n",
        "model_kmeans = cluster_Kmeans.fit(data_normalized)\n",
        "pred_kmeans = model_kmeans.labels_\n",
        "# print(pred_kmeans)\n",
        "\n",
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(pred_kmeans, return_counts=True)\n",
        "dict(zip(unique, counts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7P-7WdFHKyD"
      },
      "source": [
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(pred_kmeans, return_counts=True)\n",
        "print('K Means Output Cluster Frequency')\n",
        "print(dict(zip(unique, counts)))\n",
        "# Silhouette Score\n",
        "print('Silhouette Score for 8 Clusters')\n",
        "print(silhouette_score(data_normalized,pred_kmeans))\n",
        "print(\"\\n\")\n",
        "\n",
        "# In the above output we got value labels: ‘0’, ‘1’\n",
        "\n",
        "# Inference :   \n",
        "# Silhouette Score = 0.91 which is great (should be tend to 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5trhgeeHdwk"
      },
      "source": [
        "# If we take 4 Cluster to Check Silhouetter Score\n",
        "cluster_Kmeans = KMeans(n_clusters=3)\n",
        "model_kmeans = cluster_Kmeans.fit(data_normalized)\n",
        "labels_1 = model_kmeans.labels_\n",
        "print(\"Silhouette Score for 3 Cluster\")\n",
        "print(silhouette_score(data_normalized,labels_1))\n",
        "\n",
        "# Inference : \n",
        "# As we can observe Score is 0.29 (decrease so reject 3 Cluster)\n",
        "# Optimal number of cluster=2 as its silhouette score is greater than that of 3 clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMFr8HnOLWz5"
      },
      "source": [
        "#### Hierarchial Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH5JUeFzLFbj"
      },
      "source": [
        "# Hierarchical Clustering \n",
        "\n",
        "# Agglomerative Clustering which is a method of clustering \n",
        "# which builds a hierarchy of clusters by merging together small clusters\n",
        "\n",
        "# Silhouette Score\n",
        "# Clusters are well apart from each other as the silhouette score is closer to 1\n",
        "# Silhouette Coefficient score is a metric used to calculate the goodness of a clustering technique \n",
        "# Its value ranges from -1 to 1.\n",
        "# 1: Means clusters are well apart from each other and clearly distinguished.\n",
        "# 0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
        "# -1: Means clusters are assigned in the wrong way.\n",
        "\n",
        "# Importing Libraries\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV-80u_YLtq_"
      },
      "source": [
        "# Plotting of Dendrogram\n",
        "\n",
        "# We make use of dendrogram to decide the number of clusters required for our dataset\n",
        "# A dendrogram is a tree diagram which illustrates the arrangement of clusters.\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import os\n",
        "# We finally plot a Dendrogram \n",
        "# which helps us in deciding what the threshold values should be for the clustering algorithm\n",
        "# Basically, we decide the number of clusters by using this dendrogram.\n",
        "path = f\"plots-similarity\"\n",
        "if not os.path.exists(path):\n",
        "    # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "    # Create a new directory\n",
        "    os.makedirs(path)\n",
        "    print(f\"{path} directory is created\")\n",
        "Z = sch.linkage(data_normalized, method = 'median')\n",
        "plt.figure(figsize=(20,7))\n",
        "den = sch.dendrogram(Z)\n",
        "plt.title('Dendrogram for the clustering of the data')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Euclidean distance in the space with other variables')\n",
        "plt.savefig(f\"{path}/Dendrogram for the clustering of the data (median).pdf\", dpi = 150)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvHsqqeWPZfl"
      },
      "source": [
        "# Single Linkage - Nearest Point\n",
        "\n",
        "Z = sch.linkage(data_normalized, method = 'single')\n",
        "plt.figure(figsize=(20,7))\n",
        "den = sch.dendrogram(Z)\n",
        "plt.title('Dendrogram for the clustering of the data')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Euclidean distance in the space with other variables')\n",
        "plt.savefig(f\"{path}/Dendrogram for the clustering of the data (single linkage - nearest point).pdf\", dpi = 150)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4pITkRoPhb_"
      },
      "source": [
        "# Complete Linkage - Farthest Point \n",
        "\n",
        "Z = sch.linkage(iris_X_1, method = 'complete')\n",
        "plt.figure(figsize=(20,7))\n",
        "den = sch.dendrogram(Z)\n",
        "plt.title('Dendrogram for the clustering of the dataset iris)')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Euclidean distance in the space with other variables')\n",
        "plt.savefig(f\"{path}/Dendrogram for the clustering of the data (complete linkage - farthest point).pdf\", dpi = 150)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxOFuf5zQubY"
      },
      "source": [
        "# Average Linkage - Average Distance between all points\n",
        "\n",
        "Z = sch.linkage(iris_X_1, method = 'average')\n",
        "plt.figure(figsize=(20,7))\n",
        "den = sch.dendrogram(Z)\n",
        "plt.title('Dendrogram for the clustering of the dataset iris)')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Euclidean distance in the space with other variables')\n",
        "plt.savefig(f\"{path}/Dendrogram for the clustering of the data (average linkage).pdf\", dpi = 150)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zj9ZC0tQ9OB"
      },
      "source": [
        "# Building an Agglomerative Clustering Model\n",
        "# Initialise Model\n",
        "# We analyse the above-created dendrogram \n",
        "# decide that we will be making 2 clusters for this data\n",
        "\n",
        "cluster_H = AgglomerativeClustering(n_clusters=2)\n",
        "cluster_H"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MHBIWsSRC2y"
      },
      "source": [
        "# Fitting Model\n",
        "# After building Agglomerative clustering, we will fit our data \n",
        "# Note that only the independent variables from the data \n",
        "# are taken into account for the purpose of clustering\n",
        "\n",
        "model_clt = cluster_H.fit(data_normalized)\n",
        "model_clt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeK_wKw9RKCx"
      },
      "source": [
        "# Predicting Output Class\n",
        "\n",
        "print('Output Clusters are')\n",
        "pred1 = model_clt.labels_\n",
        "print(pred1)\n",
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(pred1, return_counts=True)\n",
        "print(dict(zip(unique, counts)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6BZvGrqRO9-"
      },
      "source": [
        "print('Hierarchical Clustering Output Cluster')\n",
        "print(dict(zip(unique, counts)))\n",
        "# Silhouette Score\n",
        "print('Silhouette Score for 2 Clusters')\n",
        "print(silhouette_score(data_normalized,pred1))\n",
        "print('\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF-OVlo1SuLp"
      },
      "source": [
        "#### DBSCAN Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5KtfrQDStDP"
      },
      "source": [
        "# DBSCAN\n",
        "\n",
        "# Density-based spatial clustering of applications with noise, or DBSCAN \n",
        "# It is a popular clustering algorithm used as a replacement for k-means clustering\n",
        "# To run it doesn’t require an input for the number of clusters \n",
        "# But DBSCAN does need to tune two other parameters\n",
        "# 'eps' parameter is the maximum distance between two data points \n",
        "# to be considered in the same neighborhood. \n",
        "# 'min_samples' parameter is the minimum amount of data points \n",
        "# in a neighborhood to be considered a cluster\n",
        "\n",
        "# Silhouette Score\n",
        "# Clusters are well apart from each other as the silhouette score is closer to 1\n",
        "# Silhouette Coefficient score is a metric used to calculate the goodness of a clustering technique \n",
        "# Its value ranges from -1 to 1.\n",
        "# 1: Means clusters are well apart from each other and clearly distinguished.\n",
        "# 0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
        "# -1: Means clusters are assigned in the wrong way.\n",
        "\n",
        "from sklearn.cluster import DBSCAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvw7DhVNS-yQ"
      },
      "source": [
        "# DBSCAN Clustering\n",
        "\n",
        "# Declaring Model\n",
        "clt_DB = DBSCAN()\n",
        "\n",
        "# By Default Parameters in dbscan\n",
        "clt_DB\n",
        "\n",
        "# Inference :\n",
        "# By Default \n",
        "# eps = 0.5, if we increase this size of cluster will increase\n",
        "# distance metric = euclidean\n",
        "# min_samples = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MujPpSykT1Oi"
      },
      "source": [
        "# Fitting the model\n",
        "model_dbscan = clt_DB.fit(data_normalized)\n",
        "pred_dbscan = model_dbscan.labels_\n",
        "print(\"DBScan Output Cluster v1\")\n",
        "# print(pred_dbscan)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghcmdag2UI-6"
      },
      "source": [
        "#Frequency count of the Output clusters\n",
        "unique, counts = np.unique(pred_dbscan, return_counts=True)\n",
        "freq_1 = dict(zip(unique, counts))\n",
        "print(\"Frequency of DBScan Output Cluster v1\")\n",
        "print(freq_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVVdv6z2USvg"
      },
      "source": [
        "# Optimize value of eps through Elbow Method\n",
        "\n",
        "# eps is the maximum distance between two points. \n",
        "# It is this distance that the algorithm uses to decide on whether to club the two points together\n",
        "# We will make use of the average distances of every point to its k nearest neighbors\n",
        "# These k distances are then plotted in ascending order\n",
        "# The point where you see an elbow like bend corresponds to the optimal *eps* value\n",
        "# At that point, a sharp change in the distance occurs, and thus this point serves as a threshold\n",
        "\n",
        "#! pip install rpy2\n",
        "%reload_ext rpy2.ipython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEbsA1GLUoAI"
      },
      "source": [
        "%%R\n",
        "#install.packages('dbscan')\n",
        "library(dbscan)\n",
        "data(iris)\n",
        "iris <- as.matrix(iris[,1:4])\n",
        "dbscan::kNNdistplot(iris[, -c(5,4,3)], k =  4)\n",
        "\n",
        "# Inference : \n",
        "# It looks like 0.42 is the optimal value for eps\n",
        "abline(h = 0.42, lty = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ3jAcI3VfD6"
      },
      "source": [
        "# Now updating value of eps = 0.42\n",
        "\n",
        "clt_DB = DBSCAN(eps=0.42)\n",
        "model_dbscan = clt_DB.fit(data_normalized)\n",
        "pred_dbscan = model_dbscan.labels_\n",
        "\n",
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(pred_dbscan, return_counts=True)\n",
        "print('DBScan Output Cluster v2')\n",
        "print(dict(zip(unique, counts)))\n",
        "# Silhouette Score\n",
        "print('Silhouette Score for 2 Clusters')\n",
        "print(silhouette_score(data_normalized,pred_dbscan))\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Inference : \n",
        "# In DBScan Cluster output Version 1,one class was missing i.e, no '2' class \n",
        "# But in DBScan New Output Cluster Version 2, distribution is much better\n",
        "# So we should check and set parameters correctly i.e, eps\n",
        "# All which are not classified in any Cluster are classified as OUTLIER\n",
        "# Silhouette Score is 0.35 which is too less"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEbEP5LDVwTz"
      },
      "source": [
        "#### Gaussian Mixture Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsWkbGAQVvja"
      },
      "source": [
        "# Gaussian Mixture Model\n",
        "\n",
        "# Gaussian Mixture Models are probabilistic models\n",
        "# It use the soft clustering approach for distributing the points in different clusters\n",
        "# Gaussian Mixture Models (GMMs) assume that there are a certain number of Gaussian distributions, \n",
        "# and each of these distributions represent a cluster.\n",
        "# Hence, a Gaussian Mixture Model tends to group the data points belonging to a single distribution together.\n",
        "\n",
        "# Silhouette Score\n",
        "# Clusters are well apart from each other as the silhouette score is closer to 1\n",
        "# Silhouette Coefficient score is a metric used to calculate the goodness of a clustering technique \n",
        "# Its value ranges from -1 to 1.\n",
        "# 1: Means clusters are well apart from each other and clearly distinguished.\n",
        "# 0: Means clusters are indifferent, or we can say that the distance between clusters is not significant.\n",
        "# -1: Means clusters are assigned in the wrong way.\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "# Gaussian Mixture Model\n",
        "from sklearn.mixture import GaussianMixture\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6-TKahYWDzz"
      },
      "source": [
        "gmm = GaussianMixture(n_components=2)\n",
        "model_gmm = gmm.fit(data_normalized)\n",
        "model_gmm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO4A2az-WDb9"
      },
      "source": [
        "# GMM Output\n",
        "labels = model_gmm.predict(data_normalized)\n",
        "print(\"Gaussian Output Clusters\")\n",
        "# print(labels)\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "pred_2 = dict(zip(unique, counts))\n",
        "print(\"Frequency of Gaussian Output Cluster\")\n",
        "print(pred_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FplNlKFJWMwP"
      },
      "source": [
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print('GaussianMixture Output Cluster Frequency')\n",
        "print(dict(zip(unique, counts)))\n",
        "# Silhouette Score\n",
        "print('Silhouette Score for 2 Clusters')\n",
        "print(silhouette_score(data_normalized,labels))\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYV-gs5ziCg8"
      },
      "source": [
        "# [Similarity Measures and Dimensionality Reduction Techniques for Time Series Data Mining](https://www.intechopen.com/chapters/39030)\n",
        "\n",
        "Salient points:\n",
        "- Lower bound effect\n",
        "- DTW\n",
        "- SVD\n",
        "- PLA\n",
        "- CHEB\n",
        "- SAX\n",
        "- PAA\n",
        "\n",
        "Other points:  \n",
        "* Motif Discovery\n",
        "* GEMINI (Generic Multimedia Indexity)\n",
        "\n",
        "**Dynamic Time Warping**\n",
        "\n",
        "**Longest Common Subsequence (LCSS) similarity measure**\n",
        "\n",
        "**Dimensionality Reduction Techniques**\n",
        "- DFT\n",
        "- DWT\n",
        "- SVD\n",
        "- PAA\n",
        "- APCA\n",
        "- PLA\n",
        "- Chebyshev Polynomial approximations.\n",
        "- SAX\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_oESKqElmfU"
      },
      "source": [
        "## [Time Series Similarity Using Dynamic Time Warping -Explained](https://medium.com/walmartglobaltech/time-series-similarity-using-dynamic-time-warping-explained-9d09119e48ec) \n",
        "\n",
        "* Cost Matrix, M\n",
        "* Time Normalized Distance, D\n",
        "\n",
        "**Libraries**\n",
        "- [dtw-python](https://pypi.org/project/dtw-python/)\n",
        "- [dtw](https://pypi.org/project/dtw/)\n",
        "\n",
        "**Faster Implementations**\n",
        "- [PrunedDTW](http://sites.labic.icmc.usp.br/prunedDTW/)\n",
        "- [SparseDTW](https://arxiv.org/abs/1201.2969)\n",
        "- [FastDTW](https://cs.fit.edu/~pkc/papers/tdm04.pdf)\n",
        "- [MultiscaleDTW](https://www.researchgate.net/publication/334413562_Iterative_Multiscale_Dynamic_Time_Warping_IMs-DTW_A_tool_for_rainfall_time_series_comparison) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kczoUmWZWSDi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYItHghUnjBn"
      },
      "source": [
        "## [Dynamic Time Warping](https://medium.datadriveninvestor.com/dynamic-time-warping-dtw-d51d1a1e4afc) \n",
        "\n",
        "* Dynamic Time Warping (DTW) is one of the algorithms for measuring the similarity between two temporal time series sequences, which may vary in speed.\n",
        "\n",
        "* DTW gives a non-linear (elastic) alignment between two-time series. Simply, it looks for the best alignment between the two-time series. This produces a more intuitive similarity measure, allowing similar shapes to match even if they are out of phase in the time axis.\n",
        "\n",
        "* The basis of DTW is found on the computations of distance /confusion matrix between two-time series. It can be shown in the below figure (a). \n",
        "\n",
        "![image text](https://miro.medium.com/max/700/1*xC66E1ENK6HO2Z_FRFv25A.png \"Image Title\")\n",
        "\n",
        "* Warping Function: \n",
        "\n",
        "![Warping Function](https://miro.medium.com/max/441/1*2KNMT_Oezrcxbonc_QDrUg.png) \n",
        "\n",
        "```\n",
        "P = p1,…,ps,…,pk\n",
        "ps = (is,js)\n",
        "which minimizes the total distance between them.\n",
        "\n",
        "Here `P` is called a `Warping Function`.\n",
        "```\n",
        "\n",
        "**Time-Normalized Distance Measure**\n",
        "\n",
        "* Time-normalized distance between A and B is given by:\n",
        "\n",
        "![Time-Normalized Distance Measure](https://miro.medium.com/max/377/1*-_s3AWfIf43aFfNGAC7xUQ.png)\n",
        "\n",
        "```\n",
        "where;\n",
        "d(ps): distance between is and js \n",
        "ws > 0:weighting coefficient\n",
        "Best alignment path between A and B: Po=arg(p)min(D(A,B)).\n",
        "```\n",
        "\n",
        "**Boundary Conditions**\n",
        "\n",
        "```\n",
        " i1= 1, ik =n and j1 = 1, jk = m\n",
        " ```\n",
        "\n",
        " **Warping Window**\n",
        "\n",
        " ```\n",
        " |is–js|≤ r, where r > 0 is the window length.\n",
        " ```\n",
        "\n",
        " ![Warping Window](https://miro.medium.com/max/335/1*RCKFYyqNWWxLcDYtg-rjEg.png)\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5H_WDt1rXvQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waPkeyEGnXi2"
      },
      "source": [
        "## [Databricks | Part 1: Understanding Dynamic Time Warping](https://databricks.com/blog/2019/04/30/understanding-dynamic-time-warping.html) \n",
        "\n",
        "* Two-time series (the base time series and new time series) are considered similar when it is possible to map with function f(x) according to the following rules so as to match the magnitudes using an optimal (warping) path.\n",
        "\n",
        "![Two-time series similarity](https://databricks.com/wp-content/uploads/2019/04/dtw-rules-formula.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9HhUZDM3Gmp"
      },
      "source": [
        "## [Databricks | Part 2: Understanding Dynamic Time Warping](https://databricks.com/blog/2019/04/30/using-dynamic-time-warping-and-mlflow-to-detect-sales-trends.html) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeeQzdWuuW7o"
      },
      "source": [
        "## Code references\n",
        "\n",
        "* https://github.com/statefb/dtwalign\n",
        "* https://github.com/PeteWe/ts_similarity_tensorflow/blob/master/SoftDTWTF.py\n",
        "* https://github.com/holgerteichgraeber/TimeSeriesClustering.jl\n",
        "* https://github.com/divyashan/time_series\n",
        "* https://github.com/aditya1709/DTW_kmedoids/blob/master/K_mediods_DTW.py \n",
        "* https://github.com/npschafer/MTS-DA\n",
        "* https://github.com/DynamicTimeWarping/dtw-python\n",
        "* https://github.com/akyadav26/Time-Series-Forecast-with-Deep-Hybrid-Architecture \n",
        "* https://github.com/DavideNardone/MTSS-Multivariate-Time-Series-Software\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc6GzEbrunkF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8uvraNawTTL"
      },
      "source": [
        "## Code for DTW on Stock data\n",
        "\n",
        "[Guide for dtw-python](https://dynamictimewarping.github.io/python/) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOzuYSItyn3J"
      },
      "source": [
        "!pip install dtw-python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDIr1_tK0spy"
      },
      "source": [
        "# !pip uninstall --yes dtw\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQhWJR48wWjP"
      },
      "source": [
        "# 1. Get Open and Close Price of asset (o, c) for each trading day.\n",
        "# libraries\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "print(f\"Get Open and Close Price of Assets\")\n",
        "def download_raw_stock_data(filepath, tickers, start, end, period = '1d'):\n",
        "    \"\"\"\n",
        "    Download Stock tickers\n",
        "    :Parameters:\n",
        "        filepath: str\n",
        "            path to store the raw data\n",
        "        tickers : str, list\n",
        "            List of tickers to download\n",
        "        period: str\n",
        "            the frequency at which to gather the data; common options would include ‘1d’ (daily), ‘1mo’ (monthly), ‘1y’ (yearly)\n",
        "        start: str\n",
        "            the date to start gathering the data. For example ‘2010–1–1’\n",
        "        end: str\n",
        "            the date to end gathering the data. For example ‘2020–1–25’\n",
        "    \n",
        "    \"\"\"\n",
        "    #define the ticker symbol\n",
        "    tickerSymbol = tickers\n",
        "\n",
        "    #get data on this ticker\n",
        "    tickerData = yf.Ticker(tickerSymbol)\n",
        "\n",
        "    #get the historical prices for this ticker\n",
        "    tickerDf = tickerData.history(period=period, start=start, end=end)\n",
        "    tickerDf.to_csv(filepath)\n",
        "\n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "path = f\"raw-stock-data/data-2000-2021\"\n",
        "if not os.path.exists(path):\n",
        "    # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "    # Create a new directory\n",
        "    os.makedirs(path)\n",
        "    print(f\"{path} directory is created\")\n",
        "period = '1d'\n",
        "start='2000-1-1'\n",
        "end='2021-8-31'\n",
        "for tickerName, ticker in dict_tickers.items():\n",
        "    tickerName = tickerName\n",
        "    ticker = ticker\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    download_raw_stock_data(filepath, ticker, start, end, period)\n",
        "\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "print(f\"The size of each asset\")\n",
        "import pandas as pd\n",
        "for tickerName in dict_tickers.keys():\n",
        "    df = pd.read_csv(f\"{path}/{tickerName}.csv\")\n",
        "    print(f\"{tickerName} size: {len(df)}\")\n",
        "\n",
        "# 2. Get weekly data.\n",
        "# 3. Transform $d_{i}$ to sequences of lag * len($d_{i}$) length.\n",
        "\n",
        "def stockDataTransformer(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    df1 = df[['Open', 'Close']].copy()\n",
        "    data = df1.values\n",
        "    n_samples = data.shape[0]//10*10\n",
        "    reshape_number = n_samples*data.shape[1]//10\n",
        "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
        "    return data1\n",
        "\n",
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    return agg\n",
        "\n",
        "from pandas import concat\n",
        "week_sequence = {}\n",
        "lag = 5\n",
        "for tickerName in dict_tickers.keys():\n",
        "    filepath = f\"{path}/{tickerName}.csv\"\n",
        "    # Get the data in the required format\n",
        "    data = stockDataTransformer(filepath)\n",
        "    print(f\"{tickerName} data.shape {data.shape}\")\n",
        "    data_orig = series_to_supervised(data, lag).values\n",
        "    print(f'{tickerName} Data Original after series to supervised on data {data_orig.shape}')\n",
        "    week_sequence[tickerName] = data_orig\n",
        "\n",
        "import numpy as np\n",
        "data = week_sequence['Apple']\n",
        "# 4. Bundle all sequences together\n",
        "for tickerName in week_sequence.keys():\n",
        "    if tickerName != 'Apple':\n",
        "        data1 = week_sequence[tickerName]\n",
        "        data = np.concatenate((data, data1))\n",
        "        print(f\"data.shape {data.shape}\")\n",
        "data_df = pd.DataFrame(data)\n",
        "data_df.to_csv(f\"all_assets_sequences.csv\")\n",
        "\n",
        "# 4. Normalize these sequences to a range (0, 1).\n",
        "from sklearn import preprocessing \n",
        "# https://www.journaldev.com/45109/normalize-data-in-python\n",
        "# Normalizes the sample\n",
        "data_normalized = preprocessing.normalize(data_df)\n",
        "data_normalized_df = pd.DataFrame(data_normalized)\n",
        "data_normalized_df.to_csv(f\"all_assets_sequences_lag{lag+1}_normalized.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POT0R_IGyIul"
      },
      "source": [
        "p = data_normalized[0, :]\n",
        "q = data_normalized[2500, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw4lO08Rztp_"
      },
      "source": [
        "## Find the best match with the canonical recursion formula\n",
        "from dtw import *\n",
        "alignment = dtw(p, q, keep_internals=True)\n",
        "\n",
        "## Display the warping curve, i.e. the alignment curve\n",
        "alignment.plot(type=\"threeway\")\n",
        "\n",
        "## Align and plot with the Rabiner-Juang type VI-c unsmoothed recursion\n",
        "dtw(p, q, keep_internals=True, \n",
        "    step_pattern=rabinerJuangStepPattern(6, \"c\"))\\\n",
        "    .plot(type=\"twoway\",offset=-2)\n",
        "\n",
        "## See the recursion relation, as formula and diagram\n",
        "print(rabinerJuangStepPattern(6,\"c\"))\n",
        "rabinerJuangStepPattern(6,\"c\").plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L71gR5qw0iKa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On3e-7oQ86ON"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsJ3s4fx-UOr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4oQROgX1zqK"
      },
      "source": [
        "# Rough"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSgEZxfp11kl"
      },
      "source": [
        "# https://databricks.com/blog/2019/04/30/understanding-dynamic-time-warping.html\n",
        "# https://pages.databricks.com/rs/094-YMS-629/images/dynamic-time-warping-background.html?_ga=2.108023234.1343331699.1632589542-1320676935.1625150082 \n",
        "!pip install fastdtw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ_bCP7u2Ac3"
      },
      "source": [
        "# Distance between p and q\n",
        "from fastdtw import fastdtw\n",
        "distance = fastdtw(p, q)[0]\n",
        "print(f\"The distance between the two clips is {distance}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwM5SB2R2HvK"
      },
      "source": [
        "sales_dbfspath = \"Sales_Transactions_Dataset_Weekly.csv\"\n",
        "import pandas as pd\n",
        "\n",
        "# Use Pandas to read this data\n",
        "sales_pdf = pd.read_csv(sales_dbfspath)\n",
        "sales_pdf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QosM61vk5UG0"
      },
      "source": [
        "!pip install ucrdtw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcuQaZd44vG1"
      },
      "source": [
        "# Calculate distance via dynamic time warping between product code and optimal time series\n",
        "import numpy as np\n",
        "import _ucrdtw\n",
        "\n",
        "def get_keyed_values(s):\n",
        "    return(s[0], s[1:])\n",
        "\n",
        "def compute_distance(row):\n",
        "    return(row[0], _ucrdtw.ucrdtw(list(row[1][0:52]), list(optimal_pattern), 0.05, True)[1])\n",
        "\n",
        "ts_values = pd.DataFrame(np.apply_along_axis(get_keyed_values, 1, sales_pdf.values))\n",
        "distances = pd.DataFrame(np.apply_along_axis(compute_distance, 1, ts_values.values))\n",
        "distances.columns = ['pcode', 'dtw_dist']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIoDrvYG5R1U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egeUjxrAdguN"
      },
      "source": [
        "# K-means clustering with DTW\n",
        "\n",
        "[Reference](https://towardsdatascience.com/how-to-apply-k-means-clustering-to-time-series-data-28d04a8f7da3) \n",
        "\n",
        "**Dynamic Time Warping Distance Metric for Time Series**\n",
        "\n",
        "- euclidean distance is unsuitable becasue it is invariant to time shifts, ignoring the dimenion of the data.\n",
        "- Instead, it is better to use dynamic time warping (DTW) to compare series. DTW is a technique to measure similarity between two temporal sequences that do not align exactly in time, speed, or length.\n",
        "\n",
        "**DTW Explained**\n",
        "\n",
        "Given series X=(x₀, …, xₙ) and series Y=(y₀, …, yₘ), the DTW distance from X to Y is formulated as the following optimization problem:\n",
        "\n",
        "![image](https://miro.medium.com/max/1178/1*uJIgUXGDH8cYOHkrIda8Ew.png) \n",
        "\n",
        "\n",
        "To summarize the DTW equation: DTW is calculated as the squared root of the sum of squared distances between each element in X and its nearest point in Y. Note that $DTW(X, Y) ≠ DTW(Y, X)$.\n",
        "\n",
        "DTW compares each element in series X with each element in series Y (n x m comparisons). The comparison, d(xᵢ, yⱼ), is just the simple subtraction xᵢ — yⱼ.\n",
        "\n",
        "Then for each xᵢ in X, DTW selects the nearest point in Y for distance calculation.\n",
        "\n",
        "![image](https://miro.medium.com/max/1308/1*m63OOTUf2NQYk5Y_69Y_XQ.png)\n",
        "\n",
        "This creates a warped “path” between X and Y that aligns each point in X to the nearest point in Y. The path is a temporal alignment of time series that minimizes the Euclidean distance between aligned series.\n",
        "\n",
        "\n",
        "![image](https://miro.medium.com/max/458/1*MuDrv0mZWhZiTEajmTMrag.png)\n",
        "\n",
        "\n",
        "Dynamic Time Warping is computed using dynamic programming with complexity O(MN). Click [here](https://en.wikipedia.org/wiki/Dynamic_time_warping) or [here](http://alexminnaar.com/2014/04/16/Time-Series-Classification-and-Clustering-with-Python.html) for more details about the specific algorithm.\n",
        "\n",
        "\n",
        "It is easy to compare two time series with DTW in python: \n",
        "```python\n",
        "from tslearn.metrics import dtw\n",
        "dtw_score = dtw(x, y)\n",
        "\n",
        "```\n",
        "\n",
        "**Variant of DTW: soft-DTW**\n",
        "\n",
        "- soft-DTW is a differentiable variant of DTW that replaces the non-differentiable min operation with a differentiable soft-min operation:\n",
        "\n",
        "![image](https://miro.medium.com/max/644/1*xE3ECmCkB5-CldCodRnbDQ.png)\n",
        "\n",
        "`Footnote: The main advantage of soft-DTW stems from the fact that it is differentiable everywhere. This allows soft-DTW to be used as a neural networks loss function, comparing a ground-truth series and a predicted series.`\n",
        "\n",
        "```python\n",
        "from tslearn.metrics import soft_dtw\n",
        "soft_dtw_score = soft_dtw(x, y, gamma=.1)\n",
        "```\n",
        "\n",
        "**K-means Clustering with Dynamic Time Warping**\n",
        "\n",
        "The k-means clustering algorithm can be applied to time series with dynamic time warping with the following modifications.\n",
        "\n",
        "- Dynamic Time Warping (DTW) is used to collect time series of similar shapes.\n",
        "- Cluster centroids, or barycenters, are computed with respect to DTW. A barycenter is the average sequence from a group of time series in DTW space. The [DTW Barycenter Averaging (DBA)](http://lig-membres.imag.fr/bisson/cours/M2INFO-AIW-ML/papers/PetitJean11.pdf) algorithm minimizes sum of squared DTW distance between the barycenter and the series in the cluster. The [soft-DTW algorithm](https://arxiv.org/abs/1703.01541) minimizes the weighted sum of soft-DTW distances between the barycenter and the series in the cluster. The weights can be tuned but must sum to 1.\n",
        "\n",
        "![image](https://miro.medium.com/max/1334/1*rKV7fP9oGWdaWTRg15ma7A.png)\n",
        "Top row: K-means clustering with DTW (DBA algorithm). Bottom row: K-means clustering with soft-DTW. Each column shows series from different clusters and their centroids, plotted in red. [Source + code](https://tslearn.readthedocs.io/en/stable/auto_examples/clustering/plot_kmeans.html#sphx-glr-auto-examples-clustering-plot-kmeans-py).\n",
        "\n",
        "```python\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "model = TimeSeriesKMeans(n_clusters=3, metric=\"dtw\", max_iter=10)\n",
        "model.fit(data)\n",
        "```\n",
        "\n",
        "`To use soft-DTW instead of DTW, simply set metric=\"softdtw\".`\n",
        "\n",
        "Note that tslearn expects a single time series to be formatted as two-dimensional array. A set of time series should be formatted as a three-dimensional array with shape (num_series, max_length, 1). \n",
        "\n",
        "[utility functions](https://tslearn.readthedocs.io/en/stable/gettingstarted.html#time-series-format) for formatting data and [integrates easily with other time series packages and data formats](https://tslearn.readthedocs.io/en/stable/integration_other_software.html).\n",
        "\n",
        "\n",
        "[tslearn](https://tslearn.readthedocs.io/en/stable/quickstart.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LoDp9h45pyW"
      },
      "source": [
        "### Install tslearn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb65zB6GdhnM"
      },
      "source": [
        "!pip install tslearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga-20Tp-6c45"
      },
      "source": [
        "## [tslearn.clustering.TimeSeriesKMeans](https://tslearn.readthedocs.io/en/stable/gen_modules/clustering/tslearn.clustering.TimeSeriesKMeans.html#tslearn.clustering.TimeSeriesKMeans)\n",
        "\n",
        "[Reference](https://tslearn.readthedocs.io/en/stable/variablelength.html#clustering)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKV0mFM7ozme"
      },
      "source": [
        "### Part 1: Used Data Normalized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb-g5A9k6cOe"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tslearn.clustering import TimeSeriesKMeans, silhouette_score\n",
        "from tslearn.datasets import CachedDatasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, \\\n",
        "    TimeSeriesResampler\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "# X_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n",
        "X_train = data_normalized_df.values\n",
        "print(f\"X_train.shape {X_train.shape}\")\n",
        "# print(f\"y_train.shape {y_train.shape}\")\n",
        "# print(f\"X_test.shape {X_test.shape}\")\n",
        "# print(f\"y_test.shape {y_test.shape}\") \n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "print(f\"X_train.shape {X_train.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRuqY6wqlXME"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXeTmLGLoOwx"
      },
      "source": [
        "# Keep only 50 time series\n",
        "# X_train = TimeSeriesScalerMeanVariance().fit_transform(X_train)\n",
        "print(f\"X_train.shape {X_train.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRh__XOBmEzG"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3fUrI-6muPc"
      },
      "source": [
        "# Make time series shorter\n",
        "# X_train = TimeSeriesResampler(sz=40).fit_transform(X_train)\n",
        "sz = X_train.shape[1]\n",
        "print(f\"X_train.shape {X_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBhg3dmQnab1"
      },
      "source": [
        "# Euclidean k-means\n",
        "print(\"Euclidean k-means\")\n",
        "km = TimeSeriesKMeans(n_clusters=2, verbose=True, random_state=seed)\n",
        "y_pred = km.fit_predict(X_train)\n",
        "print(f\"y_pred.shape {y_pred.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAXsDM0znhah"
      },
      "source": [
        "plt.figure()\n",
        "for yi in range(2):\n",
        "    plt.subplot(2, 2, yi + 1)\n",
        "    for xx in X_train[y_pred == yi][:10]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(0.1, 0.2)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"Euclidean $k$-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wk0Fm8VugIl"
      },
      "source": [
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "print('K Means Output Cluster Frequency')\n",
        "print(dict(zip(unique, counts)))\n",
        "# Silhouette Score\n",
        "print('Silhouette Score')\n",
        "print(silhouette_score(X_train,y_pred))\n",
        "print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHjOzsosCqwz"
      },
      "source": [
        "trial = 1\n",
        "km.to_pickle(f\"trial-{trial}-km.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhbb-oCAnvSd"
      },
      "source": [
        "# DBA-k-means\n",
        "print(\"DBA k-means\")\n",
        "dba_km = TimeSeriesKMeans(n_clusters=2,\n",
        "                          n_init=1,\n",
        "                          max_iter = 10,\n",
        "                          metric=\"dtw\",\n",
        "                          verbose=True,\n",
        "                          max_iter_barycenter=10,\n",
        "                          random_state=seed)\n",
        "y_pred = dba_km.fit_predict(X_train)\n",
        "print(f\"y_pred.shape {y_pred.shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2qatNapquOD"
      },
      "source": [
        "trial = 1\n",
        "dba_km.to_pickle(f\"trial-{trial}-dba-km.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GUIRho0n18R"
      },
      "source": [
        "for yi in range(2):\n",
        "    plt.subplot(2, 2, 1 + yi)\n",
        "    for xx in X_train[y_pred == yi][:10]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(dba_km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(0.1, 0.2)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"DBA $k$-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9A3ihqYEKH_"
      },
      "source": [
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "print('K Means DTW Output Cluster Frequency')\n",
        "print(dict(zip(unique, counts)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlnRqCPJqedc"
      },
      "source": [
        "### Part 2: Use TimeSeriesScalerMeanVariance()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGrqQZr-rY3t"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.datasets import CachedDatasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, \\\n",
        "    TimeSeriesResampler\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "# X_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n",
        "X_train = data_df.values\n",
        "print(f\"X_train.shape {X_train.shape}\")\n",
        "# print(f\"y_train.shape {y_train.shape}\")\n",
        "# print(f\"X_test.shape {X_test.shape}\")\n",
        "# print(f\"y_test.shape {y_test.shape}\") \n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "print(f\"X_train.shape {X_train.shape}\")\n",
        "\n",
        "\n",
        "X_train = TimeSeriesScalerMeanVariance().fit_transform(X_train)\n",
        "print(f\"X_train.shape {X_train.shape}\")\n",
        "n_clusters = 5\n",
        "# Euclidean k-means\n",
        "print(\"Euclidean k-means\")\n",
        "km = TimeSeriesKMeans(n_clusters=n_clusters, verbose=True, random_state=seed)\n",
        "y_pred = km.fit_predict(X_train)\n",
        "print(f\"y_pred.shape {y_pred.shape}\")\n",
        "\n",
        "trial = 2\n",
        "path_km = f\"trial-{trial}-km.pkl\"\n",
        "km.to_pickle(f\"{path_km}.pkl\")\n",
        "plt.figure(figsize=(15,15))\n",
        "for yi in range(n_clusters):\n",
        "    plt.subplot(n_clusters, n_clusters, yi + 1)\n",
        "    for xx in X_train[y_pred == yi][:10]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-2.5, 2.5)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"Euclidean $k$-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZNlZ0spsdVA"
      },
      "source": [
        "n_clusters = 5\n",
        "# Euclidean k-means\n",
        "print(\"Euclidean k-means\")\n",
        "km = TimeSeriesKMeans()\n",
        "trial = 2\n",
        "path_km = f\"trial-{trial}-km.pkl.pkl\"\n",
        "km.from_pickle(path_km)\n",
        "\n",
        "km"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l05Krs_PAPrB"
      },
      "source": [
        "# y_pred = km.predict(X_train)\n",
        "\n",
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "print(f'Trial {trial} Kmeans : Output Cluster Frequency')\n",
        "print(dict(zip(unique, counts)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-4XZSA_sE6y"
      },
      "source": [
        "# DBA-k-means\n",
        "print(\"DBA k-means\")\n",
        "dba_km = TimeSeriesKMeans(n_clusters=5,\n",
        "                          n_init=1,\n",
        "                          max_iter = 10,\n",
        "                          metric=\"dtw\",\n",
        "                          verbose=True,\n",
        "                          max_iter_barycenter=10,\n",
        "                          random_state=seed)\n",
        "y_pred = dba_km.fit_predict(X_train)\n",
        "print(f\"y_pred.shape {y_pred.shape}\")\n",
        "\n",
        "dba_km.to_pickle(f\"trial-{trial}-dba-km.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MArWzAwLtRW-"
      },
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "for yi in range(n_clusters):\n",
        "    plt.subplot(n_clusters, n_clusters, yi + 1)\n",
        "    for xx in X_train[y_pred == yi][:10]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-2.5, 2.5)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"DBA $k$-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef1s9WFl6kAs"
      },
      "source": [
        "from tslearn.clustering import silhouette_score\n",
        "# Frequency count of the Output clusters\n",
        "unique, counts = np.unique(y_pred, return_counts=True)\n",
        "print(f'Trial {trial} Kmeans DTW: Output Cluster Frequency')\n",
        "print(dict(zip(unique, counts)))\n",
        "# # Silhouette Score\n",
        "# print('Silhouette Score')\n",
        "# print(silhouette_score(X_train ,y_pred))\n",
        "# print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHV3J4q47KcD"
      },
      "source": [
        "## Rough of tslearn from this plot_kmeans.py code\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6254Lia7RHa"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tslearn.clustering import TimeSeriesKMeans\n",
        "from tslearn.datasets import CachedDatasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, \\\n",
        "    TimeSeriesResampler\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "X_train, y_train, X_test, y_test = CachedDatasets().load_dataset(\"Trace\")\n",
        "\n",
        "print(f\"X_train.shape {X_train.shape}\")\n",
        "print(f\"y_train.shape {y_train.shape}\")\n",
        "print(f\"X_test.shape {X_test.shape}\")\n",
        "print(f\"y_test.shape {y_test.shape}\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-GChzZU7v-7"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbT8fUBf7iw8"
      },
      "source": [
        "X_train = X_train[y_train < 4]  # Keep first 3 classes\n",
        "print(f\"X_train.shape {X_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FomMhiNK7syk"
      },
      "source": [
        "numpy.random.shuffle(X_train)\n",
        "# Keep only 50 time series\n",
        "X_train = TimeSeriesScalerMeanVariance().fit_transform(X_train[:50])\n",
        "print(f\"X_train.shape {X_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh4Oqed68AD5"
      },
      "source": [
        "# Make time series shorter\n",
        "X_train = TimeSeriesResampler(sz=40).fit_transform(X_train)\n",
        "sz = X_train.shape[1]\n",
        "print(f\"X_train.shape {X_train.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-qP1Qgs8JiX"
      },
      "source": [
        "# Euclidean k-means\n",
        "print(\"Euclidean k-means\")\n",
        "km = TimeSeriesKMeans(n_clusters=3, verbose=True, random_state=seed)\n",
        "y_pred = km.fit_predict(X_train)\n",
        "print(f\"y_pred.shape {y_pred.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR-Fo3Hq8SVY"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyt60Yi28w67"
      },
      "source": [
        "X_train[y_pred==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoJUgEG88qIL"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjixGMXZ8eb6"
      },
      "source": [
        "plt.figure()\n",
        "for yi in range(3):\n",
        "    plt.subplot(3, 3, yi + 1)\n",
        "    for xx in X_train[y_pred == yi]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"Euclidean $k$-means\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKqLI9aL8kQR"
      },
      "source": [
        "# DBA-k-means\n",
        "print(\"DBA k-means\")\n",
        "dba_km = TimeSeriesKMeans(n_clusters=3,\n",
        "                          n_init=2,\n",
        "                          metric=\"dtw\",\n",
        "                          verbose=True,\n",
        "                          max_iter_barycenter=10,\n",
        "                          random_state=seed)\n",
        "y_pred = dba_km.fit_predict(X_train)\n",
        "\n",
        "for yi in range(3):\n",
        "    plt.subplot(3, 3, 4 + yi)\n",
        "    for xx in X_train[y_pred == yi]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(dba_km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"DBA $k$-means\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLGi1npn847B"
      },
      "source": [
        "# Soft-DTW-k-means\n",
        "print(\"Soft-DTW k-means\")\n",
        "sdtw_km = TimeSeriesKMeans(n_clusters=3,\n",
        "                           metric=\"softdtw\",\n",
        "                           metric_params={\"gamma\": .01},\n",
        "                           verbose=True,\n",
        "                           random_state=seed)\n",
        "y_pred = sdtw_km.fit_predict(X_train)\n",
        "\n",
        "for yi in range(3):\n",
        "    plt.subplot(3, 3, 7 + yi)\n",
        "    for xx in X_train[y_pred == yi]:\n",
        "        plt.plot(xx.ravel(), \"k-\", alpha=.2)\n",
        "    plt.plot(sdtw_km.cluster_centers_[yi].ravel(), \"r-\")\n",
        "    plt.xlim(0, sz)\n",
        "    plt.ylim(-4, 4)\n",
        "    plt.text(0.55, 0.85,'Cluster %d' % (yi + 1),\n",
        "             transform=plt.gca().transAxes)\n",
        "    if yi == 1:\n",
        "        plt.title(\"Soft-DTW $k$-means\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHiH4ISQ8955"
      },
      "source": [
        "# Custom Functions for KMeans with DTW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AX8_GNU6zKEw"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuaGxwJKzKEx"
      },
      "source": [
        "## load data_df, data_normalized_df, data_median,\n",
        "\n",
        "## install tslearn_libraries\n",
        "\n",
        "## Use split_data to get X_train, Y_train, X_test, Y_test, reconst, extrapol\n",
        "\n",
        "## [Choose optimal clusters](https://github.com/tslearn-team/tslearn/issues/306) and [this](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py) \n",
        "\n",
        "## Train models on euclidean and dtw\n",
        "\n",
        "## Plot results\n",
        "\n",
        "## [tslearn similarity metrics like dtw_score correlation matrix](https://tslearn.readthedocs.io/en/stable/gen_modules/tslearn.metrics.html)\n",
        "\n",
        "## Given clusters, how many time series samples belong to any of the companies? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdW-EgMczKEx"
      },
      "source": [
        "## Install torchdiffeq, yfinance, tslearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09VZWjVrzKEx",
        "outputId": "d9966934-8d86-4459-c744-3404d5452b3d"
      },
      "source": [
        "def pip_install_required():\n",
        "    # Install the latest version of author's repo neural ode implementation\n",
        "    !git clone https://github.com/rtqichen/torchdiffeq.git\n",
        "    !cd torchdiffeq && pip install -e .\n",
        "    !ls torchdiffeq/torchdiffeq\n",
        "    !pip install yfinance\n",
        "    !pip install tslearn\n",
        "pip_install_required()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torchdiffeq'...\n",
            "remote: Enumerating objects: 1138, done.\u001b[K\n",
            "remote: Counting objects: 100% (434/434), done.\u001b[K\n",
            "remote: Compressing objects: 100% (194/194), done.\u001b[K\n",
            "remote: Total 1138 (delta 256), reused 400 (delta 240), pack-reused 704\u001b[K\n",
            "Receiving objects: 100% (1138/1138), 8.29 MiB | 24.97 MiB/s, done.\n",
            "Resolving deltas: 100% (682/682), done.\n",
            "Obtaining file:///content/torchdiffeq\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.9.0+cu102)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq==0.2.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq==0.2.2) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq==0.2.2) (3.7.4.3)\n",
            "Installing collected packages: torchdiffeq\n",
            "  Running setup.py develop for torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.2\n",
            "_impl  __init__.py\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.63.tar.gz (26 kB)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.9)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.6.3-cp37-cp37m-manylinux2014_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Building wheels for collected packages: yfinance\n",
            "  Building wheel for yfinance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yfinance: filename=yfinance-0.1.63-py2.py3-none-any.whl size=23918 sha256=9a5632bdae29ba9f8cf395825d9073e197d5b771458c0a66b395e66689275910\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/87/8b/7ec24486e001d3926537f5f7801f57a74d181be25b11157983\n",
            "Successfully built yfinance\n",
            "Installing collected packages: lxml, yfinance\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "Successfully installed lxml-4.6.3 yfinance-0.1.63\n",
            "Collecting tslearn\n",
            "  Downloading tslearn-0.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.19.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.51.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.4.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.29.24)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (0.34.0)\n",
            "Installing collected packages: tslearn\n",
            "Successfully installed tslearn-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd2kbqgtzKEx"
      },
      "source": [
        "## Get data in the form of (tickerName, sample_sequence). Store it. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoSWgDF0zKEx",
        "outputId": "304ca3d1-259c-44c5-8702-08cf7662659b"
      },
      "source": [
        "# 1. Get Open and Close Price of asset (o, c) for each trading day.\n",
        "# libraries\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "print(f\"Get Open and Close Price of Assets\")\n",
        "def download_raw_stock_data(filepath, tickers, start, end, period = '1d'):\n",
        "    \"\"\"\n",
        "    Download Stock tickers\n",
        "    :Parameters:\n",
        "        filepath: str\n",
        "            path to store the raw data\n",
        "        tickers : str, list\n",
        "            List of tickers to download\n",
        "        period: str\n",
        "            the frequency at which to gather the data; common options would include ‘1d’ (daily), ‘1mo’ (monthly), ‘1y’ (yearly)\n",
        "        start: str\n",
        "            the date to start gathering the data. For example ‘2010–1–1’\n",
        "        end: str\n",
        "            the date to end gathering the data. For example ‘2020–1–25’\n",
        "    \n",
        "    \"\"\"\n",
        "    #define the ticker symbol\n",
        "    tickerSymbol = tickers\n",
        "\n",
        "    #get data on this ticker\n",
        "    tickerData = yf.Ticker(tickerSymbol)\n",
        "\n",
        "    #get the historical prices for this ticker\n",
        "    tickerDf = tickerData.history(period=period, start=start, end=end)\n",
        "    tickerDf.to_csv(filepath)\n",
        "\n",
        "\n",
        "def download_data(dict_tickers, path, period, start, end):\n",
        "    print(f\"Get Open and Close Price of Assets\")\n",
        "    if not os.path.exists(path):\n",
        "        # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "        # Create a new directory\n",
        "        os.makedirs(path)\n",
        "        print(f\"{path} directory is created\")\n",
        "        \n",
        "    for tickerName, ticker in dict_tickers.items():\n",
        "        tickerName = tickerName\n",
        "        ticker = ticker\n",
        "        filepath = f\"{path}/{tickerName}.csv\"\n",
        "        download_raw_stock_data(filepath, ticker, start, end, period)\n",
        "\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "    print(f\"The size of each asset\")\n",
        "    import pandas as pd\n",
        "    for tickerName in dict_tickers.keys():\n",
        "        df = pd.read_csv(f\"{path}/{tickerName}.csv\")\n",
        "        print(f\"{tickerName} size: {len(df)}\")\n",
        "        \n",
        "dict_tickers = {\n",
        "    'Apple': 'AAPL',\n",
        "    'Microsoft': 'MSFT',\n",
        "    'Google': 'GOOG',\n",
        "    'Bitcoin': 'BTC-USD',\n",
        "    'Facebook': 'FB',\n",
        "    'Walmart': 'WMT',\n",
        "    'Amazon': 'AMZN',\n",
        "    'CVS': 'CVS',\n",
        "    'Berkshire': 'BRK-B',\n",
        "    'ExxonMobil': 'XOM',\n",
        "    'AtandT': 'T',\n",
        "    'Costco': 'COST',\n",
        "    'Walgreens': 'WBA',\n",
        "    'Kroger': 'KR',\n",
        "    'JPMorgan': 'JPM',\n",
        "    'Verizon': 'VZ',\n",
        "    'FordMotor': 'F',\n",
        "    'GeneralMotors': 'GM',\n",
        "    'Dell': 'DELL',\n",
        "    'BankOfAmerica': 'BAC',\n",
        "    'Target': 'TGT',\n",
        "    'GeneralElectric': 'GE',\n",
        "    'JohnsonandJohnson': 'JNJ',\n",
        "    'Nvidia': 'NVDA',\n",
        "    'Intel': 'INTC',\n",
        "}\n",
        "\n",
        "\n",
        "period = '1d'\n",
        "start='2000-1-1'\n",
        "end='2021-8-31'\n",
        "path = f\"raw-stock-data/data-{start.split('-')[0]}-{end.split('-')[0]}\"\n",
        "download_data(dict_tickers, path, period, start, end)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get Open and Close Price of Assets\n",
            "Get Open and Close Price of Assets\n",
            "raw-stock-data/data-2000-2021 directory is created\n",
            "\n",
            "\n",
            "The size of each asset\n",
            "Apple size: 5450\n",
            "Microsoft size: 5450\n",
            "Google size: 4288\n",
            "Bitcoin size: 2537\n",
            "Facebook size: 2336\n",
            "Walmart size: 5450\n",
            "Amazon size: 5450\n",
            "CVS size: 5450\n",
            "Berkshire size: 5450\n",
            "ExxonMobil size: 5450\n",
            "AtandT size: 5450\n",
            "Costco size: 5450\n",
            "Walgreens size: 5450\n",
            "Kroger size: 5450\n",
            "JPMorgan size: 5450\n",
            "Verizon size: 5450\n",
            "FordMotor size: 5450\n",
            "GeneralMotors size: 2713\n",
            "Dell size: 1268\n",
            "BankOfAmerica size: 5450\n",
            "Target size: 5450\n",
            "GeneralElectric size: 5450\n",
            "JohnsonandJohnson size: 5450\n",
            "Nvidia size: 5451\n",
            "Intel size: 5450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TujqBMx-zKEx"
      },
      "source": [
        "# 2. Get weekly data.\n",
        "# 3. Transform $d_{i}$ to sequences of lag * len($d_{i}$) length.\n",
        "\n",
        "def stockDataTransformer(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.set_index('Date', inplace=True)\n",
        "    df1 = df[['Open', 'Close']].copy()\n",
        "    data = df1.values\n",
        "    n_samples = data.shape[0]//10*10\n",
        "    reshape_number = n_samples*data.shape[1]//10\n",
        "    data1 = data[:n_samples].reshape((reshape_number, 10))\n",
        "    return data1\n",
        "\n",
        "def series_to_supervised(data, tickerName, n_in=1, n_out=1, dropnan=True):\n",
        "    \"\"\"\n",
        "    Frame a time series as a supervised learning dataset.\n",
        "    Arguments:\n",
        "        data: Sequence of observations as a list or NumPy array.\n",
        "        n_in: Number of lag observations as input (X).\n",
        "        n_out: Number of observations as output (y).\n",
        "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
        "    Returns:\n",
        "        Pandas DataFrame of series framed for supervised learning.\n",
        "    \"\"\"\n",
        "    n_vars = 1 if type(data) is list else data.shape[1]\n",
        "    df = pd.DataFrame(data)\n",
        "    cols, names = list(), list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        cols.append(df.shift(i))\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        cols.append(df.shift(-i))\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # put it all together\n",
        "    agg = concat(cols, axis=1)\n",
        "    agg.columns = names\n",
        "    # drop rows with NaN values\n",
        "    if dropnan:\n",
        "        agg.dropna(inplace=True)\n",
        "    agg['tickerName'] = tickerName\n",
        "    return agg\n",
        "\n",
        "def get_median(array, data_size, axis = 1):\n",
        "    # https://numpy.org/doc/stable/reference/generated/numpy.median.html\n",
        "    return np.median(array, axis = axis).reshape(data_size, 1)  #, keepdims=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCrqocF2zKEx",
        "outputId": "68665407-1b27-4540-b5f1-998cd3842851"
      },
      "source": [
        "def get_sequence_data(lag, path):\n",
        "    week_sequence = {}\n",
        "    median_data_dict = {}\n",
        "    \n",
        "    for tickerName in dict_tickers.keys():\n",
        "        filepath = f\"{path}/{tickerName}.csv\"\n",
        "        # Get the data in the required format\n",
        "        data = stockDataTransformer(filepath)\n",
        "        # # Total Data Size\n",
        "        data_size = data.shape[0]\n",
        "        print(f\"{tickerName} data.shape {data.shape}\")\n",
        "        data_orig = series_to_supervised(data, tickerName, lag).values\n",
        "        print(f'{tickerName} Data Original after series to supervised on data {data_orig.shape}')\n",
        "        week_sequence[tickerName] = data_orig\n",
        "\n",
        "        median_data = get_median(data, data_size)\n",
        "        print(f'{tickerName} Median data')\n",
        "        # Median data for each week\n",
        "        print(f\"{tickerName} median_data.shape {median_data.shape}\")\n",
        "        # print(pd.DataFrame(median_data, columns = ['median_stockprice_week']).head(10))\n",
        "        print('\\n')\n",
        "\n",
        "        # Convert median_data to (n_samples, 5) matrix\n",
        "        data_m1 = series_to_supervised(median_data, tickerName, lag).values\n",
        "        median_data_dict[tickerName] = data_m1\n",
        "        print(f'{tickerName} Median data after series to supervised')\n",
        "        print(f\"{tickerName} data_m1.shape {data_m1.shape}\")\n",
        "        # print(pd.DataFrame(data_m1, columns = [f\"week i+{i}\" for i in range(1, lag+forecast+1)]))\n",
        "        print('\\n')\n",
        "    return week_sequence, median_data_dict\n",
        "import pandas as pd\n",
        "from pandas import concat\n",
        "import numpy as np\n",
        "lag = 5\n",
        "week_sequence, median_data_dict = get_sequence_data(lag, path)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple data.shape (1090, 10)\n",
            "Apple Data Original after series to supervised on data (1085, 61)\n",
            "Apple Median data\n",
            "Apple median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Apple Median data after series to supervised\n",
            "Apple data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Microsoft data.shape (1090, 10)\n",
            "Microsoft Data Original after series to supervised on data (1085, 61)\n",
            "Microsoft Median data\n",
            "Microsoft median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Microsoft Median data after series to supervised\n",
            "Microsoft data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Google data.shape (856, 10)\n",
            "Google Data Original after series to supervised on data (851, 61)\n",
            "Google Median data\n",
            "Google median_data.shape (856, 1)\n",
            "\n",
            "\n",
            "Google Median data after series to supervised\n",
            "Google data_m1.shape (851, 7)\n",
            "\n",
            "\n",
            "Bitcoin data.shape (506, 10)\n",
            "Bitcoin Data Original after series to supervised on data (501, 61)\n",
            "Bitcoin Median data\n",
            "Bitcoin median_data.shape (506, 1)\n",
            "\n",
            "\n",
            "Bitcoin Median data after series to supervised\n",
            "Bitcoin data_m1.shape (501, 7)\n",
            "\n",
            "\n",
            "Facebook data.shape (466, 10)\n",
            "Facebook Data Original after series to supervised on data (461, 61)\n",
            "Facebook Median data\n",
            "Facebook median_data.shape (466, 1)\n",
            "\n",
            "\n",
            "Facebook Median data after series to supervised\n",
            "Facebook data_m1.shape (461, 7)\n",
            "\n",
            "\n",
            "Walmart data.shape (1090, 10)\n",
            "Walmart Data Original after series to supervised on data (1085, 61)\n",
            "Walmart Median data\n",
            "Walmart median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Walmart Median data after series to supervised\n",
            "Walmart data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Amazon data.shape (1090, 10)\n",
            "Amazon Data Original after series to supervised on data (1085, 61)\n",
            "Amazon Median data\n",
            "Amazon median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Amazon Median data after series to supervised\n",
            "Amazon data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "CVS data.shape (1090, 10)\n",
            "CVS Data Original after series to supervised on data (1085, 61)\n",
            "CVS Median data\n",
            "CVS median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "CVS Median data after series to supervised\n",
            "CVS data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Berkshire data.shape (1090, 10)\n",
            "Berkshire Data Original after series to supervised on data (1085, 61)\n",
            "Berkshire Median data\n",
            "Berkshire median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Berkshire Median data after series to supervised\n",
            "Berkshire data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "ExxonMobil data.shape (1090, 10)\n",
            "ExxonMobil Data Original after series to supervised on data (1085, 61)\n",
            "ExxonMobil Median data\n",
            "ExxonMobil median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "ExxonMobil Median data after series to supervised\n",
            "ExxonMobil data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "AtandT data.shape (1090, 10)\n",
            "AtandT Data Original after series to supervised on data (1085, 61)\n",
            "AtandT Median data\n",
            "AtandT median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "AtandT Median data after series to supervised\n",
            "AtandT data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Costco data.shape (1090, 10)\n",
            "Costco Data Original after series to supervised on data (1085, 61)\n",
            "Costco Median data\n",
            "Costco median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Costco Median data after series to supervised\n",
            "Costco data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Walgreens data.shape (1090, 10)\n",
            "Walgreens Data Original after series to supervised on data (1085, 61)\n",
            "Walgreens Median data\n",
            "Walgreens median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Walgreens Median data after series to supervised\n",
            "Walgreens data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Kroger data.shape (1090, 10)\n",
            "Kroger Data Original after series to supervised on data (1085, 61)\n",
            "Kroger Median data\n",
            "Kroger median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Kroger Median data after series to supervised\n",
            "Kroger data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "JPMorgan data.shape (1090, 10)\n",
            "JPMorgan Data Original after series to supervised on data (1085, 61)\n",
            "JPMorgan Median data\n",
            "JPMorgan median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "JPMorgan Median data after series to supervised\n",
            "JPMorgan data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Verizon data.shape (1090, 10)\n",
            "Verizon Data Original after series to supervised on data (1085, 61)\n",
            "Verizon Median data\n",
            "Verizon median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Verizon Median data after series to supervised\n",
            "Verizon data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "FordMotor data.shape (1090, 10)\n",
            "FordMotor Data Original after series to supervised on data (1085, 61)\n",
            "FordMotor Median data\n",
            "FordMotor median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "FordMotor Median data after series to supervised\n",
            "FordMotor data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "GeneralMotors data.shape (542, 10)\n",
            "GeneralMotors Data Original after series to supervised on data (537, 61)\n",
            "GeneralMotors Median data\n",
            "GeneralMotors median_data.shape (542, 1)\n",
            "\n",
            "\n",
            "GeneralMotors Median data after series to supervised\n",
            "GeneralMotors data_m1.shape (537, 7)\n",
            "\n",
            "\n",
            "Dell data.shape (252, 10)\n",
            "Dell Data Original after series to supervised on data (247, 61)\n",
            "Dell Median data\n",
            "Dell median_data.shape (252, 1)\n",
            "\n",
            "\n",
            "Dell Median data after series to supervised\n",
            "Dell data_m1.shape (247, 7)\n",
            "\n",
            "\n",
            "BankOfAmerica data.shape (1090, 10)\n",
            "BankOfAmerica Data Original after series to supervised on data (1085, 61)\n",
            "BankOfAmerica Median data\n",
            "BankOfAmerica median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "BankOfAmerica Median data after series to supervised\n",
            "BankOfAmerica data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Target data.shape (1090, 10)\n",
            "Target Data Original after series to supervised on data (1085, 61)\n",
            "Target Median data\n",
            "Target median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Target Median data after series to supervised\n",
            "Target data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "GeneralElectric data.shape (1090, 10)\n",
            "GeneralElectric Data Original after series to supervised on data (1085, 61)\n",
            "GeneralElectric Median data\n",
            "GeneralElectric median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "GeneralElectric Median data after series to supervised\n",
            "GeneralElectric data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "JohnsonandJohnson data.shape (1090, 10)\n",
            "JohnsonandJohnson Data Original after series to supervised on data (1085, 61)\n",
            "JohnsonandJohnson Median data\n",
            "JohnsonandJohnson median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "JohnsonandJohnson Median data after series to supervised\n",
            "JohnsonandJohnson data_m1.shape (1085, 7)\n",
            "\n",
            "\n",
            "Nvidia data.shape (1090, 10)\n",
            "Nvidia Data Original after series to supervised on data (1079, 61)\n",
            "Nvidia Median data\n",
            "Nvidia median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Nvidia Median data after series to supervised\n",
            "Nvidia data_m1.shape (1079, 7)\n",
            "\n",
            "\n",
            "Intel data.shape (1090, 10)\n",
            "Intel Data Original after series to supervised on data (1085, 61)\n",
            "Intel Median data\n",
            "Intel median_data.shape (1090, 1)\n",
            "\n",
            "\n",
            "Intel Median data after series to supervised\n",
            "Intel data_m1.shape (1085, 7)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzAUAE7r1TiD",
        "outputId": "f7c13484-8b18-45a1-e66b-4a4d9467fbbe"
      },
      "source": [
        "def get_bundle(week_sequence, median_data_dict):\n",
        "    data = week_sequence['Apple']\n",
        "    median_data = median_data_dict['Apple']\n",
        "    # 4. Bundle all sequences together\n",
        "    for tickerName in week_sequence.keys():\n",
        "        if tickerName != 'Apple':\n",
        "            data1 = week_sequence[tickerName]\n",
        "            median_data1 = median_data_dict[tickerName]\n",
        "            data = np.concatenate((data, data1))\n",
        "            median_data = np.concatenate((median_data, median_data1))\n",
        "            print(f\"data.shape {data.shape}\")\n",
        "    return data, median_data\n",
        "import numpy as np\n",
        "data, median_data = get_bundle(week_sequence, median_data_dict)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.shape (2170, 61)\n",
            "data.shape (3021, 61)\n",
            "data.shape (3522, 61)\n",
            "data.shape (3983, 61)\n",
            "data.shape (5068, 61)\n",
            "data.shape (6153, 61)\n",
            "data.shape (7238, 61)\n",
            "data.shape (8323, 61)\n",
            "data.shape (9408, 61)\n",
            "data.shape (10493, 61)\n",
            "data.shape (11578, 61)\n",
            "data.shape (12663, 61)\n",
            "data.shape (13748, 61)\n",
            "data.shape (14833, 61)\n",
            "data.shape (15918, 61)\n",
            "data.shape (17003, 61)\n",
            "data.shape (17540, 61)\n",
            "data.shape (17787, 61)\n",
            "data.shape (18872, 61)\n",
            "data.shape (19957, 61)\n",
            "data.shape (21042, 61)\n",
            "data.shape (22127, 61)\n",
            "data.shape (23206, 61)\n",
            "data.shape (24291, 61)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID25H7Kh4Qyb"
      },
      "source": [
        "def save_bundle(data, median_data, path_data, path_median_data, file_data, file_median_data, n_in=1, n_out=1, n_vars=10):\n",
        "    names = list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        if i == 0:\n",
        "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "        else:\n",
        "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "    names+=['tickerName']\n",
        "    data_df = pd.DataFrame(data)\n",
        "    data_df.columns = names\n",
        "    if not os.path.exists(path_data):\n",
        "        # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "        # Create a new directory\n",
        "        os.makedirs(path_data)\n",
        "        print(f\"{path_data} directory is created\")\n",
        "    data_df.to_csv(f\"{path_data}/{file_data}.csv\")\n",
        "    if not os.path.exists(path_median_data):\n",
        "        # https://appdividend.com/2021/07/03/how-to-create-directory-if-not-exist-in-python/\n",
        "        # Create a new directory\n",
        "        os.makedirs(path_median_data)\n",
        "        print(f\"{path_median_data} directory is created\")\n",
        "    \n",
        "    names = list()\n",
        "    # input sequence (t-n, ... t-1)\n",
        "    for i in range(n_in, 0, -1):\n",
        "        names += [f\"var_{i}\"]\n",
        "    # forecast sequence (t, t+1, ... t+n)\n",
        "    for i in range(0, n_out):\n",
        "        if i == 0:\n",
        "            names += [f\"var_{i}\"]\n",
        "        else:\n",
        "            names += [f\"var_{i}\"]\n",
        "    names+=['tickerName']\n",
        "    median_data_df = pd.DataFrame(median_data)\n",
        "    median_data_df.columns = names\n",
        "    median_data_df.to_csv(f\"{path_median_data}/{file_median_data}.csv\")\n",
        "    \n",
        "    return data_df, median_data_df\n",
        "path_data = \"bundle_data\"\n",
        "path_median_data = \"bundle_median_data\"\n",
        "file_data = 'data'\n",
        "file_median_data = 'median_data'\n",
        "lag = 5\n",
        "forecast = 1\n",
        "n_vars = 10\n",
        "data_df, median_data_df = save_bundle(data, median_data, path_data, path_median_data, file_data, file_median_data, lag, forecast, n_vars)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "1J5kaNnqHeuP",
        "outputId": "52be443d-7b9e-4bf2-d16c-2f21d3465fe5"
      },
      "source": [
        "data_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var1(t-5)</th>\n",
              "      <th>var2(t-5)</th>\n",
              "      <th>var3(t-5)</th>\n",
              "      <th>var4(t-5)</th>\n",
              "      <th>var5(t-5)</th>\n",
              "      <th>var6(t-5)</th>\n",
              "      <th>var7(t-5)</th>\n",
              "      <th>var8(t-5)</th>\n",
              "      <th>var9(t-5)</th>\n",
              "      <th>var10(t-5)</th>\n",
              "      <th>var1(t-4)</th>\n",
              "      <th>var2(t-4)</th>\n",
              "      <th>var3(t-4)</th>\n",
              "      <th>var4(t-4)</th>\n",
              "      <th>var5(t-4)</th>\n",
              "      <th>var6(t-4)</th>\n",
              "      <th>var7(t-4)</th>\n",
              "      <th>var8(t-4)</th>\n",
              "      <th>var9(t-4)</th>\n",
              "      <th>var10(t-4)</th>\n",
              "      <th>var1(t-3)</th>\n",
              "      <th>var2(t-3)</th>\n",
              "      <th>var3(t-3)</th>\n",
              "      <th>var4(t-3)</th>\n",
              "      <th>var5(t-3)</th>\n",
              "      <th>var6(t-3)</th>\n",
              "      <th>var7(t-3)</th>\n",
              "      <th>var8(t-3)</th>\n",
              "      <th>var9(t-3)</th>\n",
              "      <th>var10(t-3)</th>\n",
              "      <th>var1(t-2)</th>\n",
              "      <th>var2(t-2)</th>\n",
              "      <th>var3(t-2)</th>\n",
              "      <th>var4(t-2)</th>\n",
              "      <th>var5(t-2)</th>\n",
              "      <th>var6(t-2)</th>\n",
              "      <th>var7(t-2)</th>\n",
              "      <th>var8(t-2)</th>\n",
              "      <th>var9(t-2)</th>\n",
              "      <th>var10(t-2)</th>\n",
              "      <th>var1(t-1)</th>\n",
              "      <th>var2(t-1)</th>\n",
              "      <th>var3(t-1)</th>\n",
              "      <th>var4(t-1)</th>\n",
              "      <th>var5(t-1)</th>\n",
              "      <th>var6(t-1)</th>\n",
              "      <th>var7(t-1)</th>\n",
              "      <th>var8(t-1)</th>\n",
              "      <th>var9(t-1)</th>\n",
              "      <th>var10(t-1)</th>\n",
              "      <th>var1(t)</th>\n",
              "      <th>var2(t)</th>\n",
              "      <th>var3(t)</th>\n",
              "      <th>var4(t)</th>\n",
              "      <th>var5(t)</th>\n",
              "      <th>var6(t)</th>\n",
              "      <th>var7(t)</th>\n",
              "      <th>var8(t)</th>\n",
              "      <th>var9(t)</th>\n",
              "      <th>var10(t)</th>\n",
              "      <th>tickerName</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.803995</td>\n",
              "      <td>0.858137</td>\n",
              "      <td>0.829868</td>\n",
              "      <td>0.785788</td>\n",
              "      <td>0.79537</td>\n",
              "      <td>0.797286</td>\n",
              "      <td>0.813578</td>\n",
              "      <td>0.72829</td>\n",
              "      <td>0.73979</td>\n",
              "      <td>0.762789</td>\n",
              "      <td>0.781954</td>\n",
              "      <td>0.749373</td>\n",
              "      <td>0.735478</td>\n",
              "      <td>0.711042</td>\n",
              "      <td>0.728291</td>\n",
              "      <td>0.668399</td>\n",
              "      <td>0.724337</td>\n",
              "      <td>0.741706</td>\n",
              "      <td>0.766622</td>\n",
              "      <td>0.769975</td>\n",
              "      <td>0.774288</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.809744</td>\n",
              "      <td>0.816931</td>\n",
              "      <td>0.885448</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.875865</td>\n",
              "      <td>0.853346</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>0.814536</td>\n",
              "      <td>0.804953</td>\n",
              "      <td>0.860533</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.844721</td>\n",
              "      <td>0.83418</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.829389</td>\n",
              "      <td>0.779079</td>\n",
              "      <td>0.774288</td>\n",
              "      <td>0.79537</td>\n",
              "      <td>0.797286</td>\n",
              "      <td>0.768538</td>\n",
              "      <td>0.772372</td>\n",
              "      <td>0.757518</td>\n",
              "      <td>0.769017</td>\n",
              "      <td>0.792016</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.874428</td>\n",
              "      <td>0.873949</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.863408</td>\n",
              "      <td>0.865325</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.871074</td>\n",
              "      <td>0.833701</td>\n",
              "      <td>0.838013</td>\n",
              "      <td>0.887844</td>\n",
              "      <td>Apple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.781954</td>\n",
              "      <td>0.749373</td>\n",
              "      <td>0.735478</td>\n",
              "      <td>0.711042</td>\n",
              "      <td>0.728291</td>\n",
              "      <td>0.668399</td>\n",
              "      <td>0.724337</td>\n",
              "      <td>0.741706</td>\n",
              "      <td>0.766622</td>\n",
              "      <td>0.769975</td>\n",
              "      <td>0.774288</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.809744</td>\n",
              "      <td>0.816931</td>\n",
              "      <td>0.885448</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.875865</td>\n",
              "      <td>0.853346</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>0.814536</td>\n",
              "      <td>0.804953</td>\n",
              "      <td>0.860533</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.844721</td>\n",
              "      <td>0.83418</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.829389</td>\n",
              "      <td>0.779079</td>\n",
              "      <td>0.774288</td>\n",
              "      <td>0.79537</td>\n",
              "      <td>0.797286</td>\n",
              "      <td>0.768538</td>\n",
              "      <td>0.772372</td>\n",
              "      <td>0.757518</td>\n",
              "      <td>0.769017</td>\n",
              "      <td>0.792016</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.874428</td>\n",
              "      <td>0.873949</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.863408</td>\n",
              "      <td>0.865325</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.871074</td>\n",
              "      <td>0.833701</td>\n",
              "      <td>0.838013</td>\n",
              "      <td>0.887844</td>\n",
              "      <td>0.883532</td>\n",
              "      <td>0.91228</td>\n",
              "      <td>0.902697</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.883053</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.878741</td>\n",
              "      <td>0.852867</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.872511</td>\n",
              "      <td>Apple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.774288</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.809744</td>\n",
              "      <td>0.816931</td>\n",
              "      <td>0.885448</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.875865</td>\n",
              "      <td>0.853346</td>\n",
              "      <td>0.831306</td>\n",
              "      <td>0.814536</td>\n",
              "      <td>0.804953</td>\n",
              "      <td>0.860533</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.844721</td>\n",
              "      <td>0.83418</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.829389</td>\n",
              "      <td>0.779079</td>\n",
              "      <td>0.774288</td>\n",
              "      <td>0.79537</td>\n",
              "      <td>0.797286</td>\n",
              "      <td>0.768538</td>\n",
              "      <td>0.772372</td>\n",
              "      <td>0.757518</td>\n",
              "      <td>0.769017</td>\n",
              "      <td>0.792016</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.874428</td>\n",
              "      <td>0.873949</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.863408</td>\n",
              "      <td>0.865325</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.871074</td>\n",
              "      <td>0.833701</td>\n",
              "      <td>0.838013</td>\n",
              "      <td>0.887844</td>\n",
              "      <td>0.883532</td>\n",
              "      <td>0.91228</td>\n",
              "      <td>0.902697</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.883053</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.878741</td>\n",
              "      <td>0.852867</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.872511</td>\n",
              "      <td>0.868079</td>\n",
              "      <td>0.891197</td>\n",
              "      <td>0.899343</td>\n",
              "      <td>0.883172</td>\n",
              "      <td>0.880178</td>\n",
              "      <td>0.846159</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.8682</td>\n",
              "      <td>0.870595</td>\n",
              "      <td>0.878741</td>\n",
              "      <td>Apple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.804953</td>\n",
              "      <td>0.860533</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.844721</td>\n",
              "      <td>0.83418</td>\n",
              "      <td>0.843284</td>\n",
              "      <td>0.829389</td>\n",
              "      <td>0.779079</td>\n",
              "      <td>0.774288</td>\n",
              "      <td>0.79537</td>\n",
              "      <td>0.797286</td>\n",
              "      <td>0.768538</td>\n",
              "      <td>0.772372</td>\n",
              "      <td>0.757518</td>\n",
              "      <td>0.769017</td>\n",
              "      <td>0.792016</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.874428</td>\n",
              "      <td>0.873949</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.863408</td>\n",
              "      <td>0.865325</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.871074</td>\n",
              "      <td>0.833701</td>\n",
              "      <td>0.838013</td>\n",
              "      <td>0.887844</td>\n",
              "      <td>0.883532</td>\n",
              "      <td>0.91228</td>\n",
              "      <td>0.902697</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.883053</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.878741</td>\n",
              "      <td>0.852867</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.872511</td>\n",
              "      <td>0.868079</td>\n",
              "      <td>0.891197</td>\n",
              "      <td>0.899343</td>\n",
              "      <td>0.883172</td>\n",
              "      <td>0.880178</td>\n",
              "      <td>0.846159</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.8682</td>\n",
              "      <td>0.870595</td>\n",
              "      <td>0.878741</td>\n",
              "      <td>0.908926</td>\n",
              "      <td>0.999004</td>\n",
              "      <td>0.97361</td>\n",
              "      <td>0.935279</td>\n",
              "      <td>0.957318</td>\n",
              "      <td>0.981275</td>\n",
              "      <td>0.965944</td>\n",
              "      <td>0.963548</td>\n",
              "      <td>0.969297</td>\n",
              "      <td>0.941986</td>\n",
              "      <td>Apple</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.797286</td>\n",
              "      <td>0.768538</td>\n",
              "      <td>0.772372</td>\n",
              "      <td>0.757518</td>\n",
              "      <td>0.769017</td>\n",
              "      <td>0.792016</td>\n",
              "      <td>0.796807</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.827952</td>\n",
              "      <td>0.874428</td>\n",
              "      <td>0.873949</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.863408</td>\n",
              "      <td>0.865325</td>\n",
              "      <td>0.870116</td>\n",
              "      <td>0.871074</td>\n",
              "      <td>0.833701</td>\n",
              "      <td>0.838013</td>\n",
              "      <td>0.887844</td>\n",
              "      <td>0.883532</td>\n",
              "      <td>0.91228</td>\n",
              "      <td>0.902697</td>\n",
              "      <td>0.874907</td>\n",
              "      <td>0.883053</td>\n",
              "      <td>0.880657</td>\n",
              "      <td>0.878741</td>\n",
              "      <td>0.852867</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.872511</td>\n",
              "      <td>0.868079</td>\n",
              "      <td>0.891197</td>\n",
              "      <td>0.899343</td>\n",
              "      <td>0.883172</td>\n",
              "      <td>0.880178</td>\n",
              "      <td>0.846159</td>\n",
              "      <td>0.844242</td>\n",
              "      <td>0.8682</td>\n",
              "      <td>0.870595</td>\n",
              "      <td>0.878741</td>\n",
              "      <td>0.908926</td>\n",
              "      <td>0.999004</td>\n",
              "      <td>0.97361</td>\n",
              "      <td>0.935279</td>\n",
              "      <td>0.957318</td>\n",
              "      <td>0.981275</td>\n",
              "      <td>0.965944</td>\n",
              "      <td>0.963548</td>\n",
              "      <td>0.969297</td>\n",
              "      <td>0.941986</td>\n",
              "      <td>0.941987</td>\n",
              "      <td>0.935279</td>\n",
              "      <td>0.926654</td>\n",
              "      <td>0.937195</td>\n",
              "      <td>0.932883</td>\n",
              "      <td>0.964027</td>\n",
              "      <td>0.936237</td>\n",
              "      <td>0.930008</td>\n",
              "      <td>0.929289</td>\n",
              "      <td>0.875865</td>\n",
              "      <td>Apple</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  var1(t-5) var2(t-5) var3(t-5)  ...   var9(t)  var10(t) tickerName\n",
              "0  0.803995  0.858137  0.829868  ...  0.838013  0.887844      Apple\n",
              "1  0.781954  0.749373  0.735478  ...  0.844242  0.872511      Apple\n",
              "2  0.774288  0.796807  0.809744  ...  0.870595  0.878741      Apple\n",
              "3  0.804953  0.860533  0.843284  ...  0.969297  0.941986      Apple\n",
              "4  0.797286  0.768538  0.772372  ...  0.929289  0.875865      Apple\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qukgTb6M59q9"
      },
      "source": [
        "def normalize_data_bundle(data_df, median_data_df):\n",
        "    # https://www.journaldev.com/45109/normalize-data-in-python\n",
        "    # Normalizes the sample\n",
        "    data_normalized = preprocessing.normalize(data_df)\n",
        "    data_normalized_df = pd.DataFrame(data_normalized)\n",
        "    # data_normalized_df.to_csv(f\"all_assets_sequences_lag{lag+1}_normalized.csv\")\n",
        "    return data_normalized_df\n",
        "\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPS37DG65wCS"
      },
      "source": [
        "## load data_df, data_normalized_df, data_median,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9bWH-24485J"
      },
      "source": [
        "def load_bundle(path_data, path_median_data, file_data, file_median_data, device):\n",
        "    data_df = pd.read_csv(f\"{path_data}/{file_data}.csv\", index_col='tickerName')\n",
        "    # delete one by one like column is 'Unnamed: 0' so use it's name\n",
        "    data_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "    median_data_df = pd.read_csv(f\"{path_median_data}/{file_median_data}.csv\", index_col='tickerName')\n",
        "    # delete one by one like column is 'Unnamed: 0' so use it's name\n",
        "    median_data_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "    return data_df, median_data_df"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz0j4nLeEFtg"
      },
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "data_df, median_data_df = load_bundle(path_data, path_median_data, file_data, file_median_data, device)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_iz9I0eHp8F",
        "outputId": "d6e6d414-e83b-4846-fb40-b98507f87d59"
      },
      "source": [
        "data_df.info()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 24291 entries, Apple to Intel\n",
            "Data columns (total 60 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   var1(t-5)   24291 non-null  float64\n",
            " 1   var2(t-5)   24291 non-null  float64\n",
            " 2   var3(t-5)   24291 non-null  float64\n",
            " 3   var4(t-5)   24291 non-null  float64\n",
            " 4   var5(t-5)   24291 non-null  float64\n",
            " 5   var6(t-5)   24291 non-null  float64\n",
            " 6   var7(t-5)   24291 non-null  float64\n",
            " 7   var8(t-5)   24291 non-null  float64\n",
            " 8   var9(t-5)   24291 non-null  float64\n",
            " 9   var10(t-5)  24291 non-null  float64\n",
            " 10  var1(t-4)   24291 non-null  float64\n",
            " 11  var2(t-4)   24291 non-null  float64\n",
            " 12  var3(t-4)   24291 non-null  float64\n",
            " 13  var4(t-4)   24291 non-null  float64\n",
            " 14  var5(t-4)   24291 non-null  float64\n",
            " 15  var6(t-4)   24291 non-null  float64\n",
            " 16  var7(t-4)   24291 non-null  float64\n",
            " 17  var8(t-4)   24291 non-null  float64\n",
            " 18  var9(t-4)   24291 non-null  float64\n",
            " 19  var10(t-4)  24291 non-null  float64\n",
            " 20  var1(t-3)   24291 non-null  float64\n",
            " 21  var2(t-3)   24291 non-null  float64\n",
            " 22  var3(t-3)   24291 non-null  float64\n",
            " 23  var4(t-3)   24291 non-null  float64\n",
            " 24  var5(t-3)   24291 non-null  float64\n",
            " 25  var6(t-3)   24291 non-null  float64\n",
            " 26  var7(t-3)   24291 non-null  float64\n",
            " 27  var8(t-3)   24291 non-null  float64\n",
            " 28  var9(t-3)   24291 non-null  float64\n",
            " 29  var10(t-3)  24291 non-null  float64\n",
            " 30  var1(t-2)   24291 non-null  float64\n",
            " 31  var2(t-2)   24291 non-null  float64\n",
            " 32  var3(t-2)   24291 non-null  float64\n",
            " 33  var4(t-2)   24291 non-null  float64\n",
            " 34  var5(t-2)   24291 non-null  float64\n",
            " 35  var6(t-2)   24291 non-null  float64\n",
            " 36  var7(t-2)   24291 non-null  float64\n",
            " 37  var8(t-2)   24291 non-null  float64\n",
            " 38  var9(t-2)   24291 non-null  float64\n",
            " 39  var10(t-2)  24291 non-null  float64\n",
            " 40  var1(t-1)   24291 non-null  float64\n",
            " 41  var2(t-1)   24291 non-null  float64\n",
            " 42  var3(t-1)   24291 non-null  float64\n",
            " 43  var4(t-1)   24291 non-null  float64\n",
            " 44  var5(t-1)   24291 non-null  float64\n",
            " 45  var6(t-1)   24291 non-null  float64\n",
            " 46  var7(t-1)   24291 non-null  float64\n",
            " 47  var8(t-1)   24291 non-null  float64\n",
            " 48  var9(t-1)   24291 non-null  float64\n",
            " 49  var10(t-1)  24291 non-null  float64\n",
            " 50  var1(t)     24291 non-null  float64\n",
            " 51  var2(t)     24291 non-null  float64\n",
            " 52  var3(t)     24291 non-null  float64\n",
            " 53  var4(t)     24291 non-null  float64\n",
            " 54  var5(t)     24291 non-null  float64\n",
            " 55  var6(t)     24291 non-null  float64\n",
            " 56  var7(t)     24291 non-null  float64\n",
            " 57  var8(t)     24291 non-null  float64\n",
            " 58  var9(t)     24291 non-null  float64\n",
            " 59  var10(t)    24291 non-null  float64\n",
            "dtypes: float64(60)\n",
            "memory usage: 11.3+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuQxa2rL9aBJ"
      },
      "source": [
        "# 4. Normalize these sequences to a range (0, 1).\n",
        "from sklearn import preprocessing \n",
        "data_normalized_df = normalize_data_bundle(data_df, median_data_df)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVjswH1H-jQq"
      },
      "source": [
        "\n",
        "## install tslearn_libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKJvYhYv-irM"
      },
      "source": [
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tslearn.clustering import TimeSeriesKMeans, silhouette_score\n",
        "from tslearn.datasets import CachedDatasets\n",
        "from tslearn.preprocessing import TimeSeriesScalerMeanVariance, TimeSeriesResampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "def get_data_in_tslearn_format(df):\n",
        "    X_train = df.values\n",
        "    print(f\"X_train.shape {X_train.shape}\")\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "    print(f\"X_train.shape {X_train.shape}\")\n",
        "    return X_train\n",
        "\n",
        "def train_kmeans(X_train, n_clusters, seed=0):\n",
        "    # Euclidean k-means\n",
        "    print(\"Euclidean k-means\")\n",
        "    km = TimeSeriesKMeans(n_clusters=2, verbose=True, random_state=seed)\n",
        "    km.fit(X_train)\n",
        "    y_pred = km.predict(X_train)\n",
        "    print(f\"y_pred.shape {y_pred.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTBdapK5LVCj"
      },
      "source": [
        "## [Choose optimal clusters](https://github.com/tslearn-team/tslearn/issues/306) and [this](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Enh0J1IyLQbE"
      },
      "source": [
        "## Plot results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GITARjd7LMXi"
      },
      "source": [
        "def plot_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpvgGaOpLZvP"
      },
      "source": [
        "## Train models on euclidean and dtw\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTtLZ42pLamf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUiEHe94La1N"
      },
      "source": [
        "## [tslearn similarity metrics like dtw_score correlation matrix](https://tslearn.readthedocs.io/en/stable/gen_modules/tslearn.metrics.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qS7rZw2LgXs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUS2j-6pLfvT"
      },
      "source": [
        "## Given clusters, how many time series samples belong to any of the companies? \n"
      ]
    }
  ]
}